<!DOCTYPE html><html class="theme-next muse use-motion" lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><script src="https://cdn.staticfile.org/pace/1.0.2/pace.min.js"></script><link href="https://cdn.staticfile.org/pace/1.0.2/themes/blue/pace-theme-big-counter.min.css" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="//cdn.staticfile.org/fancybox/2.1.5/jquery.fancybox.min.js" rel="stylesheet" type="text/css"><link href="//cdn.staticfile.org/font-awesome/4.6.2/css/font-awesome.min.css" rel="stylesheet" type="text/css"><link href="/css/main.css?v=6.4.1" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.4.1"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.4.1"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.4.1"><link rel="mask-icon" href="/images/logo.svg?v=6.4.1" color="#222"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Muse",version:"6.4.1",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!0,fastclick:!1,lazyload:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="description" content="VGGNet论文翻译，小卷积核、更深网络的引领者VGGNet: Very Deep Convolutional Networks for Large-Scale Image RecognitionKaren Simonyan, Andrew Zisserman, 2014Visual Geometry Group, Department of Engineering Science, Univer"><meta name="keywords" content="Computer Vision,Deep Learning,Paper,Image Classification,VGGNet"><meta property="og:type" content="article"><meta property="og:title" content="VGGNet"><meta property="og:url" content="http://muyaan.com/2018/08/09/VGGNet/index.html"><meta property="og:site_name" content="慕湮"><meta property="og:description" content="VGGNet论文翻译，小卷积核、更深网络的引领者VGGNet: Very Deep Convolutional Networks for Large-Scale Image RecognitionKaren Simonyan, Andrew Zisserman, 2014Visual Geometry Group, Department of Engineering Science, Univer"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="http://muyaan.com/2018/08/09/VGGNet/1533629012794.png"><meta property="og:image" content="http://muyaan.com/2018/08/09/VGGNet/1533629031721.png"><meta property="og:image" content="http://muyaan.com/2018/08/09/VGGNet/1533731489008.png"><meta property="og:image" content="http://muyaan.com/2018/08/09/VGGNet/1533732504470.png"><meta property="og:image" content="http://muyaan.com/2018/08/09/VGGNet/1533732952144.png"><meta property="og:image" content="http://muyaan.com/2018/08/09/VGGNet/1533785670219.png"><meta property="og:image" content="http://muyaan.com/2018/08/09/VGGNet/1533786020407.png"><meta property="og:updated_time" content="2018-09-12T03:22:25.028Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="VGGNet"><meta name="twitter:description" content="VGGNet论文翻译，小卷积核、更深网络的引领者VGGNet: Very Deep Convolutional Networks for Large-Scale Image RecognitionKaren Simonyan, Andrew Zisserman, 2014Visual Geometry Group, Department of Engineering Science, Univer"><meta name="twitter:image" content="http://muyaan.com/2018/08/09/VGGNet/1533629012794.png"><link rel="canonical" href="http://muyaan.com/2018/08/09/VGGNet/"><script type="text/javascript" id="page.configurations">CONFIG.page={sidebar:""}</script><title>VGGNet | 慕湮</title><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?ca5844321cfb80fdf6f12b4dcc326991";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style type="text/css">.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.logo-line-after i{right:initial}</style></noscript></head><body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">慕湮</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">白日放歌须纵酒</h1></div><div class="site-nav-toggle"><button aria-label="切换导航栏"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://muyaan.com/2018/08/09/VGGNet/"><span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="慕湮"><meta itemprop="description" content=""><meta itemprop="image" content="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4"></span><span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="慕湮"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">VGGNet</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2018-08-09 15:33:48" itemprop="dateCreated datePublished" datetime="2018-08-09T15:33:48+08:00">2018-08-09</time> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2018-09-12 11:22:25" itemprop="dateModified" datetime="2018-09-12T11:22:25+08:00">2018-09-12</time> </span><span id="/2018/08/09/VGGNet/" class="leancloud_visitors" data-flag-title="VGGNet"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数：</span> <span title="本文字数">11k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">20 分钟</span></div></div></header><div class="post-body" itemprop="articleBody"><p>VGGNet论文翻译，小卷积核、更深网络的引领者</p><blockquote><p><a href="https://arxiv.org/abs/1409.1556" rel="external nofollow noopener noreferrer" target="_blank">VGGNet: Very Deep Convolutional Networks for Large-Scale Image Recognition</a><br>Karen Simonyan, Andrew Zisserman, 2014<br>Visual Geometry Group, Department of Engineering Science, University of Oxford</p></blockquote><a id="more"></a><hr><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>卷积网络（ConvNets）近期在大规模图像和视频识别中大获成功，这得归功于大量公开图片集和高性能计算系统。特别的是，深度视觉识别架构在ILSVRC获得的重要角色，成为了大规模图片分类系统的试验场。</p><p>随着卷积网络在计算机视觉领域越来越常用，业界对AlexNet原始架构做出了多种改进尝试以获得更高的准确度。比如ILSVRC 2013的最佳成绩在第一个卷积层使用了更小的感受野大小和更短的步长。另一条提升途径是用整个图片的多个尺度来密集地训练测试网络。在本论文中，我们着重于卷积网络架构设计的重要方向——深度。为了这一目的，我们固定了架构中的其他参数，不断地通过增加卷积层来增加网络深度。由于在所有层使用非常小的卷积核，这一方式变得可行。</p><p>我们得到了精确得多的卷积网络架构，它不仅在ILSVRC分类和定位任务获得前沿的准确率，还能应用于其余图片识别数据集。甚至只是用作简单流程的一部分时，也取得了非常好的性能。我们公布了两个最佳模型以助于进一步的研究。</p><p>本论文以如下形式组织。在章节2，我们描述了我们的卷积网络配置。图片分类的训练和测试细节在章节3，在章节4中比较了这些配置。章节5总结了本论文。为了完备性，我们还在附录A中描述并评估了我们的ILSVRC 2014物体定位系统，并在附录B中讨论了将极深特征泛化到其余数据集。</p><h2 id="2-ConvNet-Configurations"><a href="#2-ConvNet-Configurations" class="headerlink" title="2. ConvNet Configurations"></a>2. ConvNet Configurations</h2><p>为了在公平的设定中评价增加卷积网络深度带来的提升，我们所有卷积网络配置都设计为使用同样的准则，这是被Ciresan等人和Krizhevsky等人激发的。在本节，我们首先描述我们卷积网络的通用设计（2.1），并描述用于测试的特殊配置细节（2.2）。2.3中讨论了我们的设计细节，并与先驱进行了比较。</p><h3 id="2-1-Architecture"><a href="#2-1-Architecture" class="headerlink" title="2.1 Architecture"></a>2.1 Architecture</h3><p>训练时网络使用的输入图片固定为$224\times 224$的RGB。我们使用的唯一预处理是为每张图的每个像素减去训练集的平均RGB值。图片穿越堆叠的卷积层，我们使用非常小的卷积核，感受野大小为$3\times3$(是带有左右、上下、中心的最小尺寸)。在有的配置中我们也使用$1\times1$的卷积核，可被看做输入通道的线性变换（后跟非线性化）。卷积步长固定为1像素；卷积层输入时采用空间填充，使卷积计算后空间分辨率得以保持，如$3\times3$的卷积层需填充1像素。空间池化通过5个最大池化层进行，它们跟在某些卷积层后。最大池化窗口为$2\times2$，步长为2.</p><p>一个卷积层栈（它们有不同的深度和结构）后跟一个全连接层FC：前两个每个都有4096个通道，最后一个需要进行1000路ILSVRC分类，因此有1000个通道。最后一层是soft-max层。所有网络中，FC配置都一样。</p><p>所有中间层都配备了非线性整流函数ReLU。值得说明的是我们的网络除了一个之外，均不含Local Response Normalisation LRN：如章节4中展示的那样，这一归一化并不能提升在ILSVRC上的性能，而且会增大内存占用和时间消耗。在使用了的地方，LRN层参数都是来自Krzhevsky等人的。</p><h3 id="2-2-Configurations"><a href="#2-2-Configurations" class="headerlink" title="2.2 Configurations"></a>2.2 Configurations</h3><p>本论文的卷积网络配置都在表1中列出。在论文后续部分我们只会使用它们的名字（A-E）。所有的配置都遵从章节2.1的通用规则，仅仅在深度上有不同：从网络A中的11层（8卷积3全连接）到E的19层（16卷积3全连接）。卷积层的宽度（通道数）都非常小，从第一层的64开始，每个最大池化层都乘以2，直到达到512.</p><p><img src="/2018/08/09/VGGNet/./1533629012794.png" alt="表1"></p><p>我们在表2中列出了每种配置的参数量。尽管有更大的深度，我们的网络并不比那些更窄有着更大卷积核尺寸和卷积通道的网络参数量大。</p><p><img src="/2018/08/09/VGGNet/./1533629031721.png" alt="表2"></p><h3 id="2-3-Discussion"><a href="#2-3-Discussion" class="headerlink" title="2.3 Discussion"></a>2.3 Discussion</h3><p>我们的配置与过去顶级成绩所使用的有较大不同。与其在第一个卷积层使用相对大的感受野（AlexNet $11\times 11$ 步长为4，ZFnet $7 \times7$ 步长为2），我们在整个网络都使用了非常小的卷积核$3\times3$，能在输入中的每个像素进行卷积（步长为1）。堆叠两个$3\times3$的卷积层（之间没有空间池化）有与$5\times5$等效的感受野；堆叠3个就等效$7\times7$。那么我们这样做，如堆叠3个$3\times3$卷积层而不是一个$7\times7$卷积层，的好处是什么呢。首先，我们包含了3个非线性激活层，而不是1个，这让决策函数更有辨识力。其次，我们缩减了参数量：假如3层$3\times3$卷积栈的输入输出都是C个通道的话，该栈可通过$3(3^2C^2)=27C^2$个权重参数化；而一个$7\times7$卷积层将需要$7^2C^2=49C^2$个参数，增多了81%。这可以看做为$7 \times7$卷积核引入了一个正则化，强制它们分解为$3\times3$卷积核（其中没有加入非线性化）。</p><p>并入$1\times1$卷积层（表1，配置C）是一种为决策函数增加非线性的办法，同时还不影响卷积层的感受野。尽管在我们这里$1\times1$卷积层实质上是在同样维度空间上的一个线性投影（因为输入输出通道数相等），我们使用整流函数引入了额外的非线性。值得一提的是近期的Network in Network也使用了$1\times1$卷积层。</p><p>CiresanNet曾用过小卷积核，但它们的网络深度远远低于我们，且未在大规模ILSVRC数据集上评估。Goodfellow等人将深度卷积网络（11层）应用于街牌号识别，并展示了提升深度能获得更好的性能。GoogLeNet，一个ILSVRC 2014最佳记录，与我们的研究独立进行，也使用了极深的卷积网络(22层)和小卷积核（不光是$3\times3$，他们还使用$1\times1$和$5\times5$）。但它们的网络拓扑要比我们的复杂得多，为了减少计算量，空间分辨率也在首层减少很多。在章节4.5也会显示，我们的模型在单网络准确度上超越了GoogLeNet。</p><h2 id="3-Classification-Framework"><a href="#3-Classification-Framework" class="headerlink" title="3. Classification Framework"></a>3. Classification Framework</h2><p>在上一节我们介绍了网络配置细节。本节我们将描述训练和测试中的细节。</p><h3 id="3-1-Training"><a href="#3-1-Training" class="headerlink" title="3.1 Training"></a>3.1 Training</h3><p>卷积网络训练流程大体按照AlexNet的方法（除了从多尺度的训练图片采样输入图片块，这将在后面解释）。即，训练是通过带动量的mini-batch SDG（基于LeNet的反向传播）优化多项式逻辑回归目标来完成的。batch size为256，动量为0.9。训练通过权重衰减（$L_2$惩罚乘数设为0.0005）来正则化，并在前两个FC应用dropout正则化（dropout比例设为0.5）。学习率初始化为0.01，并在验证集准确度不再提高时除以10。总得来说，学习率下降了3次，训练在370k次迭代（74个epoch）后停止。尽管我们比AlexNet参数多、深度大，却只需更少的epoch训练来收敛的原因是有：(a)由深度和更小的卷积核隐式引入的正则化；(b)相关层的预初始化。</p><p>网络权重的初始化很重要，因为在深度网络中，不好的初始值会因梯度的不稳定停滞训练。为了绕过这一问题，我们从训练配置A开始，足够窄以致随机地初始化后也能训练。接着在训练更深的网络结构时，我们用来自网络A的权重初始化前4个卷积层和后3个全连接层（所有中间层随机初始化）。我们并未降低预初始化层的学习率，让它们在训练中可以改变。在随机初始化时，我们使用了均值为0方差为0.01的正态分布。bias被初始化为0。值得一提的是，论文提交后我们发现可以通过Glorot和Bengio的随机初始化流程，而不用预训练初始化。</p><p>为了获得固定大小的$224\times 224$卷积网络输入图片，过去的做法是随机从缩放后的训练图片截取下来（每次SGD每张图片截一块）。为了进一步增广训练集，被截取的图片块将随机水平翻转，随机RGB色差。训练图片的缩放将在随后解释。</p><p><strong>Training image size.</strong> 设S为一个等轴缩放过(isotropically-rescaled)的训练图片的短边，也是卷积网络输入裁剪的来源（我们同样使用S指代训练尺度）。因为截取块大小为$224\times224$，故S不能小于224：如果S=224，截取块会得到整张图片的信息，完全跨越一张训练图片的短边。如果S远大于224，则截图会是图片的一小块，包含一个小物体或物体的一部分。</p><p>我们考虑了两种设置训练尺度S的方法。第一种是固定S，即单尺度训练（需要注意抽样块中的图像内容仍能表达多尺度）。在我们的实验中，我们评估了用两个固定尺度训练的模型：S=256（在过去的研究中广泛应用）和S=384。给定一个卷积网络配置，我们首先用S=256训练。为了加速S=384网络的训练，它会被S=256预训练的权重初始化，而且我们会使用更小的学习率，0.001。</p><p>第二个方法是多尺度训练，每个训练图片都独立地用一个来自$[S_{min},S_{max}]$(我们使用256和512)的随机采样S缩放。因为图片中物体可能有不同的大小，在训练中考虑这一点会有用。这也可看做训练集尺度扰动的数据增广，让单模型在各种尺度物体上训练。出于速度的原因，我们预训练一个固定尺度S=384的模型，再用同样配置，基于单尺度模型权重调优多尺度模型。</p><h3 id="3-2-Testing"><a href="#3-2-Testing" class="headerlink" title="3.2 Testing"></a>3.2 Testing</h3><p>在测试时，给定一个训练好的卷积网络和一个输入图像，它通过如下方式分类。首先等轴缩放到预定义的最小图像边，记做Q（也用它指代测试尺度）。Q不需要与S相等（我们会在章节4展示，使用多个尺度的Q能提升性能）。接着网络以一种类似Sermanet的方式密集地应用在缩放后的测试图片上。 也就是，首先将FC层转换成卷积层（第一个转换为$7\times7$，最后两个转换为$1\times1$卷积层）。得到的全卷积网络应用于整个图片上（未剪切）。结果是一个类得分map，其通道数等于类数量，以及一个可变的空间分辨率，它依赖于输入图片尺寸。最终，为了得到该图的固定大小类得分向量，对类得分map进行空间平均（sum-pooled）。我们也对测试集使用水平翻转进行增广；最终使用soft-max来获得图片的最终得分。</p><p>因为全卷积网络应用于整张图片，因此测试时不再需要采样多个截图块，因为需要网络重计算每个截图块，那样效率更低。然而，像GoogLeNet那样使用一大堆截图块能提升准确率，因为它相比FCN得到了输入图片的更精细的抽样。同样，因其不同的卷积边界条件，多截图块检测也是密集检测的一个补足：将卷积网络应用于一个截图块时，多截图块方法是用0填充卷积feature map的，而在密集检测中是来自图片邻近部分，这能潜在地提升网络整体感受野，捕获更多的上下文。同时我们相信实际运用中多截图块带来的精确度提升不值得其计算时间消耗。我们也用每个尺度50个截图块（$5\times5$常规截图再加翻转）来测试了我们的网络以供参考，共3个尺度下的150个截图块。</p><h3 id="3-3-Implementation-Details"><a href="#3-3-Implementation-Details" class="headerlink" title="3.3 Implementation Details"></a>3.3 Implementation Details</h3><p>我们使用Caffe实现，但进行了不少修改来让我们在单系统的多GPU上训练并测试，以及在多尺度的原图上训练和测试。多GPU训练需要数据并行，将每个batch训练图片分割到GPU各自batch中，在各GPU种并行处理。在GPU batch的梯度计算出后，进行平均以获得整个batch的梯度。梯度计算在各GPU中是同步的，因此结果与单GPU训练一致。</p><h2 id="4-Classification-Experiments"><a href="#4-Classification-Experiments" class="headerlink" title="4 Classification Experiments"></a>4 Classification Experiments</h2><p><strong>Dataset.</strong> 在本章，我们展示了本论文所述结构的卷积网络在ILSVRC 2012数据集上的分类结果。该数据集有着1000类图片，并分作3个子集：训练（130万张），验证（5万张）和测试（10万张）。分类性能用两个指标评估：top-1和top-5误差。前者是多类分类错误，如错误分类的图片占比；后者是ILSVRC的主要指标，通过gt类不属于前五预测类的图片的占比计算。</p><h3 id="4-1-Single-Scale-Evaluation"><a href="#4-1-Single-Scale-Evaluation" class="headerlink" title="4.1 Single Scale Evaluation"></a>4.1 Single Scale Evaluation</h3><p>我们从对不同的模型以章节2.2的层设置，进行单尺度测试。测试图片设置如下：对于固定的S，令$Q=S$；对于扰动的$S \in [S_{min},S_{max}]$，$Q=0.5(S_{min}+S_{max})$，结果见表3。</p><p>首先，我们注意到在没有任何归一化层的模型A使用local response normalisation(A-LRN网络)没有提升。因此在更深的结构（B-E）中不再使用归一化。</p><p>其次，我们观察到随着卷积网络深度增加，分类错误会减少：从11层的A到19层的E。需要注意的是，虽然有同样的深度，但配置C（包含3个$1\times1$卷积层）不如D（使用了$3\times3$卷积）。这说明虽然增加非线性的确有用（C比B好），不重要的感受野卷积到的空间上下文是很重要的（D比C好）。错误率在深度达到19后稳定，但更深的模型也许能从更大的数据集中获利。我们也把B与更窄的五层$5\times5$卷积比较，它是通过替换B中每一对$3\times3$卷积层（如章节2.3所讲，它们感受野相同）来得到的。更窄的网络的top-1 error要比B高7%，这也确认了更深网络更小卷积优于更窄网络更大卷积。</p><p>最终，在训练时的尺度抖动（$S\in [256;512]$）得到了比在固定小边尺寸($S=256$，$S=384$)训练好得多的结果，尽管测试时使用的单尺度。这证明了训练时使用尺度抖动进行增广确实有助于捕捉多尺度的图片信息。</p><p><img src="/2018/08/09/VGGNet/./1533731489008.png" alt="表3"></p><h3 id="4-2-Multi-Scale-Evaluation"><a href="#4-2-Multi-Scale-Evaluation" class="headerlink" title="4.2 Multi-Scale Evaluation"></a>4.2 Multi-Scale Evaluation</h3><p>在单尺度上测试这些模型之后，我们开始评估测试时尺度抖动的作用。它在一个模型上运行一张图片的多个缩放后版本（对应Q不同的值），并对结果值取平均。考虑到测试和训练尺度不一会带来的性能下降，模型以与训练尺度相似的尺度进行测试：$Q = \{ S-32,S,S+32\}$。同时在训练时的尺度扰动允许模型测试时的可用尺度更广，故以$S\in [S_[min],S_{max}]$训练的模型通过大范围的$Q=\{S_{min},0.5(S_{min}+S_{max}),S_{max}\}$测试。</p><p>结果如表4，说明测试时的尺度扰动能提升性能（与表3的以单尺度测试的同模型相比）。与之前相同，最深的网络结果最好，训练时的尺度扰动也比单尺度好。我们在验证集上的最佳单网性能为24.8%/7.5的top-1/top-5 error（表4中加粗）。在测试集上，配置E达到7.3%的top-5 error。</p><p><img src="/2018/08/09/VGGNet/./1533732504470.png" alt="表4"></p><h3 id="4-3-Multi-Crop-Evaluation"><a href="#4-3-Multi-Crop-Evaluation" class="headerlink" title="4.3 Multi-Crop Evaluation"></a>4.3 Multi-Crop Evaluation</h3><p>在表5中我们比较了多截图块测试和密集检测测试（细节见3.2）。我们还用它们各自softmax平均值评估了两个方法的互补性。可以看出多截图块方法性能稍好，而它们确实是互补的，因为它们的组合性能最好。如之前提到的那样，我们猜测这是因为它们卷积边界条件的不同。</p><p><img src="/2018/08/09/VGGNet/./1533732952144.png" alt="表5"></p><h3 id="4-4-ConvNet-Fusion"><a href="#4-4-ConvNet-Fusion" class="headerlink" title="4.4 ConvNet Fusion"></a>4.4 ConvNet Fusion</h3><p>直到现在为止，我们都在测试单个模型性能。在这一部分，我们将通过对其soft-max输出进行平均来组合多个模型。因模型完备性的增加，这能提升性能。</p><p>结果见表6。在提交ILSVRC时我们仅仅训练了单尺度网络和一个多尺度模型D（通过仅调优全连接层）。组合7个网络得到了7.3%的test error。在提交后我们考虑只组合两个性能最佳多尺度模型（D，E），使用密集检测错误率为7%，使用密集加多尺度测试得到6.8%。</p><p><img src="/2018/08/09/VGGNet/./1533785670219.png" alt="表6"></p><h3 id="4-5-Comparison-with-the-State-of-the-Art"><a href="#4-5-Comparison-with-the-State-of-the-Art" class="headerlink" title="4.5 Comparison with the State of the Art"></a>4.5 Comparison with the State of the Art</h3><p>表7是我们结果与前沿结果的比较。在ILSVRC 2014 分类竞赛中，我们的VGG使用7个模型的组合，以7.3%的错误率得到了第二名。在提交后，我们通过两个模型的组合进一步将错误率降低到6.8%。</p><p>如表7所示，我们的极深卷积网络显著地超越了上一代模型，与本次竞赛的冠军GoogLeNet也相差无几（它们是6.7%）。考虑到我们的最佳结果是只组合了两个模型获得的，比大部分提交的方案都少。在单模型中我们的性能最佳，高过GoogLeNet 0.9%。</p><p><img src="/2018/08/09/VGGNet/./1533786020407.png" alt="表7"></p><h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5 Conclusion"></a>5 Conclusion</h2><p>在本论文中，我们使用大规模图片分类任务测试了极深卷积网络（19层）。证明了深度表达对于分类准确度有帮助，而在ImageNet上传统卷积网络结构（LeNet，AlexNet）增大深度后能取得前沿性能。在附录中，我们还展示了本模型能泛化到许多任务和数据集上，并取得与构建于更浅的图片表达深度上的更复杂的识别流程相当甚至更好的结果。我们的结果再一次证明了视觉表达上深度的重要性。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>Bell, S., Upchurch, P., Snavely, N., and Bala, K. Material recognition in the wild with the materials in context<br>database. CoRR, abs/1412.0623, 2014.<br>Chatfield, K., Simonyan, K., Vedaldi, A., and Zisserman, A. Return of the devil in the details: Delving deep<br>into convolutional nets. In Proc. BMVC., 2014.<br>Cimpoi, M., Maji, S., and Vedaldi, A. Deep convolutional filter banks for texture recognition and segmentation.<br>CoRR, abs/1411.6836, 2014.<br>Ciresan, D. C., Meier, U., Masci, J., Gambardella, L. M., and Schmidhuber, J. Flexible, high performance<br>convolutional neural networks for image classification. In IJCAI, pp. 1237–1242, 2011.<br>Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M., Ranzato, M., Senior, A., Tucker, P., Yang,<br>K., Le, Q. V., and Ng, A. Y. Large scale distributed deep networks. In NIPS, pp. 1232–1240, 2012.<br>Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image<br>database. In Proc. CVPR, 2009.<br>Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. Decaf: A deep convolutional<br>activation feature for generic visual recognition. CoRR, abs/1310.1531, 2013.<br>Everingham, M., Eslami, S. M. A., Van Gool, L., Williams, C., Winn, J., and Zisserman, A. The Pascal visual<br>object classes challenge: A retrospective. IJCV, 111(1):98–136, 2015.<br>Fei-Fei, L., Fergus, R., and Perona, P. Learning generative visual models from few training examples: An<br>incremental bayesian approach tested on 101 object categories. In IEEE CVPR Workshop of Generative<br>Model Based Vision, 2004.<br>Girshick, R. B., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object detection<br>and semantic segmentation. CoRR, abs/1311.2524v5, 2014. Published in Proc. CVPR, 2014.<br>Gkioxari, G., Girshick, R., and Malik, J. Actions and attributes from wholes and parts. CoRR, abs/1412.2604,<br>2014.<br>Glorot, X. and Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In Proc.<br>AISTATS, volume 9, pp. 249–256, 2010.<br>Goodfellow, I. J., Bulatov, Y., Ibarz, J., Arnoud, S., and Shet, V. Multi-digit number recognition from street<br>view imagery using deep convolutional neural networks. In Proc. ICLR, 2014.<br>Griffin, G., Holub, A., and Perona, P. Caltech-256 object category dataset. Technical Report 7694, California<br>Institute of Technology, 2007.<br>He, K., Zhang, X., Ren, S., and Sun, J. Spatial pyramid pooling in deep convolutional networks for visual<br>recognition. CoRR, abs/1406.4729v2, 2014.<br>Hoai, M. Regularized max pooling for image categorization. In Proc. BMVC., 2014.<br>Howard, A. G. Some improvements on deep convolutional neural network based image classification. In Proc.<br>ICLR, 2014.<br>Jia, Y. Caffe: An open source convolutional architecture for fast feature embedding.<br><a href="http://caffe.berkeleyvision.org/" rel="external nofollow noopener noreferrer" target="_blank">http://caffe.berkeleyvision.org/</a>, 2013.<br>Karpathy, A. and Fei-Fei, L. Deep visual-semantic alignments for generating image descriptions. CoRR,<br>abs/1412.2306, 2014.<br>Kiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying visual-semantic embeddings with multimodal neural<br>language models. CoRR, abs/1411.2539, 2014.<br>Krizhevsky, A. One weird trick for parallelizing convolutional neural networks. CoRR, abs/1404.5997, 2014.<br>Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet classification with deep convolutional neural networks.<br>In NIPS, pp. 1106–1114, 2012.<br>LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. Backpropagation<br>applied to handwritten zip code recognition. Neural Computation, 1(4):541–551, 1989.<br>Lin, M., Chen, Q., and Yan, S. Network in network. In Proc. ICLR, 2014.<br>Long, J., Shelhamer, E., and Darrell, T. Fully convolutional networks for semantic segmentation. CoRR,<br>abs/1411.4038, 2014.<br>Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning and Transferring Mid-Level Image Representations<br>using Convolutional Neural Networks. In Proc. CVPR, 2014.<br>Perronnin, F., S´anchez, J., and Mensink, T. Improving the Fisher kernel for large-scale image classification. In<br>Proc. ECCV, 2010.<br>Razavian, A., Azizpour, H., Sullivan, J., and Carlsson, S. CNN Features off-the-shelf: an Astounding Baseline<br>for Recognition. CoRR, abs/1403.6382, 2014.</p></div><div class="popular-posts-header">相关文章</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="/2018/08/16/GoogLeNet-v1/" rel="bookmark">GoogLeNet v1</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/2018/08/12/YOLOv2-and-YOLO9000/" rel="bookmark">YOLOv2 and YOLO9000</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/2018/08/13/YOLOv3/" rel="bookmark">YOLOv3</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/2018/09/06/Aligned-ReID/" rel="bookmark">Aligned ReID</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/2018/07/22/ResNet/" rel="bookmark">ResNet</a></div></li></ul><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>慕湮</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="http://muyaan.com/2018/08/09/VGGNet/" title="VGGNet">http://muyaan.com/2018/08/09/VGGNet/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noopener noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Computer-Vision/" rel="tag"># Computer Vision</a> <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a> <a href="/tags/Paper/" rel="tag"># Paper</a> <a href="/tags/Image-Classification/" rel="tag"># Image Classification</a> <a href="/tags/VGGNet/" rel="tag"># VGGNet</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2018/08/07/SSD/" rel="next" title="SSD"><i class="fa fa-chevron-left"></i> SSD</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2018/08/12/YOLOv2-and-YOLO9000/" rel="prev" title="YOLOv2 and YOLO9000">YOLOv2 and YOLO9000 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4" alt="慕湮"><p class="site-author-name" itemprop="name">慕湮</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">23</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">31</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/tycallen" target="_blank" title="GitHub" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:tyc.allen@gmail.com" target="_blank" title="E-Mail" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-envelope"></i>E-Mail</a> </span><span class="links-of-author-item"><a href="http://weibo.com/pojunallen" target="_blank" title="微博" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-weibo"></i>微博</a> </span><span class="links-of-author-item"><a href="https://tuchong.com/1070837" target="_blank" title="图虫" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-globe"></i>图虫</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-block"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> 友情链接</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="http://dotrabbit.tk" title="dotrabbit" target="_blank" rel="external nofollow noopener noreferrer">dotrabbit</a></li></ul></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Introduction"><span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-ConvNet-Configurations"><span class="nav-text">2. ConvNet Configurations</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-Architecture"><span class="nav-text">2.1 Architecture</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Configurations"><span class="nav-text">2.2 Configurations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-Discussion"><span class="nav-text">2.3 Discussion</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Classification-Framework"><span class="nav-text">3. Classification Framework</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Training"><span class="nav-text">3.1 Training</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-Testing"><span class="nav-text">3.2 Testing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-Implementation-Details"><span class="nav-text">3.3 Implementation Details</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Classification-Experiments"><span class="nav-text">4 Classification Experiments</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-Single-Scale-Evaluation"><span class="nav-text">4.1 Single Scale Evaluation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-Multi-Scale-Evaluation"><span class="nav-text">4.2 Multi-Scale Evaluation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-Multi-Crop-Evaluation"><span class="nav-text">4.3 Multi-Crop Evaluation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-ConvNet-Fusion"><span class="nav-text">4.4 ConvNet Fusion</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-5-Comparison-with-the-State-of-the-Art"><span class="nav-text">4.5 Comparison with the State of the Art</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Conclusion"><span class="nav-text">5 Conclusion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-text">References</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; 2015 – <span itemprop="copyrightYear">2018</span> <span class="with-love" id="animate"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">慕湮</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="站点总字数">320k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">9:42</span></div><div class="powered-by">由 <a class="theme-link" target="_blank" rel="external nofollow noopener noreferrer" href="https://hexo.io">Hexo</a> 强力驱动</div><span class="post-meta-divider">|</span><div class="theme-info">主题 – <a class="theme-link" target="_blank" rel="external nofollow noopener noreferrer" href="https://theme-next.org">NexT.Muse</a></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="//cdn.staticfile.org/jquery/2.1.3/jquery.min.js"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="//cdn.staticfile.org/velocity/1.2.1/velocity.min.js"></script><script type="text/javascript" src="//cdn.staticfile.org/velocity/1.2.1/velocity.ui.min.js"></script><script type="text/javascript" src="//cdn.staticfile.org/fancybox/2.1.5/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=6.4.1"></script><script type="text/javascript" src="/js/src/motion.js?v=6.4.1"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=6.4.1"></script><script type="text/javascript" src="/js/src/post-details.js?v=6.4.1"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.1"></script><script>function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();

      Counter('get', '/classes/Counter', { where: JSON.stringify({ url }) })
        .done(function ({ results }) {
          if (results.length > 0) {
            var counter = results[0];
            
              var $element = $(document.getElementById(url));
              $element.find('.leancloud-visitors-count').text(counter.time + 1);
            
            Counter('put', `/classes/Counter/${counter.objectId}`, JSON.stringify({ time: { "__op":"Increment", "amount":1 } }))
            
            .fail(function ({ responseJSON }) {
                console.log('Failed to save Visitor num, with error message: ' + responseJSON.error);
            })
          } else {
            
              Counter('post', '/classes/Counter', JSON.stringify({ title: title, url: url, time: 1}))
                .done(function () {
                  var $element = $(document.getElementById(url));
                  $element.find('.leancloud-visitors-count').text(1);
                })
                .fail(function () {
                  console.log('Failed to create');
                });
            
          }
        })
      .fail(function ({ responseJSON }) {
        console.log('LeanCloud Counter Error:' + responseJSON.code + " " + responseJSON.error);
      });
    }
    

    $(function() {
      $.get('https://app-router.leancloud.cn/2/route?appId=' + "UWGXBMGbguCDhSCbC3NrUQtL-gzGzoHsz")
        .done(function ({ api_server }) {
          var Counter = function (method, url, data) {
            return $.ajax({
              method: method,
              url: `https://${api_server}/1.1${url}`,
              headers: {
                'X-LC-Id': "UWGXBMGbguCDhSCbC3NrUQtL-gzGzoHsz",
                'X-LC-Key': "ke1jrA5b6VyR89Kqqqwf2kPP",
                'Content-Type': 'application/json',
              },
              data: data,
            });
          };
          
          addCount(Counter);
          
        })
    });</script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="http://muyaan.com/js/src/async.js"></script></body></html>