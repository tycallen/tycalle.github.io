<!DOCTYPE html><html class="theme-next muse use-motion" lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><script src="https://cdn.staticfile.org/pace/1.0.2/pace.min.js"></script><link href="https://cdn.staticfile.org/pace/1.0.2/themes/blue/pace-theme-big-counter.min.css" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="//cdn.staticfile.org/fancybox/2.1.5/jquery.fancybox.min.js" rel="stylesheet" type="text/css"><link href="//cdn.staticfile.org/font-awesome/4.6.2/css/font-awesome.min.css" rel="stylesheet" type="text/css"><link href="/css/main.css?v=6.4.1" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.4.1"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.4.1"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.4.1"><link rel="mask-icon" href="/images/logo.svg?v=6.4.1" color="#222"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Muse",version:"6.4.1",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!0,fastclick:!1,lazyload:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="description" content="YOLOv3论文翻译，一个YOLOv2的升级版。作者还表达了对计算机视觉技术应用到不好的地方的担忧，并吐槽了论文评审和mAP评价方法。感觉是个超酷的人~YOLOv3: An Incremental ImprovementJoseph Redmon, Ali Farhadi, 2018.4University of Washington"><meta name="keywords" content="Computer Vision,Deep Learning,Paper,Object Detection,YOLO,YOLOv3"><meta property="og:type" content="article"><meta property="og:title" content="YOLOv3"><meta property="og:url" content="http://muyaan.com/2018/08/13/YOLOv3/index.html"><meta property="og:site_name" content="慕湮"><meta property="og:description" content="YOLOv3论文翻译，一个YOLOv2的升级版。作者还表达了对计算机视觉技术应用到不好的地方的担忧，并吐槽了论文评审和mAP评价方法。感觉是个超酷的人~YOLOv3: An Incremental ImprovementJoseph Redmon, Ali Farhadi, 2018.4University of Washington"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="http://muyaan.com/2018/08/13/YOLOv3/1534134422688.png"><meta property="og:image" content="http://muyaan.com/2018/08/13/YOLOv3/1534147274150.png"><meta property="og:image" content="http://muyaan.com/2018/08/13/YOLOv3/1534150142987.png"><meta property="og:image" content="http://muyaan.com/2018/08/13/YOLOv3/1534150396426.png"><meta property="og:image" content="http://muyaan.com/2018/08/13/YOLOv3/1534151673939.png"><meta property="og:image" content="http://muyaan.com/2018/08/13/YOLOv3/1534152814487.png"><meta property="og:image" content="http://muyaan.com/2018/08/13/YOLOv3/1534159729671.png"><meta property="og:updated_time" content="2018-09-12T03:22:25.022Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="YOLOv3"><meta name="twitter:description" content="YOLOv3论文翻译，一个YOLOv2的升级版。作者还表达了对计算机视觉技术应用到不好的地方的担忧，并吐槽了论文评审和mAP评价方法。感觉是个超酷的人~YOLOv3: An Incremental ImprovementJoseph Redmon, Ali Farhadi, 2018.4University of Washington"><meta name="twitter:image" content="http://muyaan.com/2018/08/13/YOLOv3/1534134422688.png"><link rel="canonical" href="http://muyaan.com/2018/08/13/YOLOv3/"><script type="text/javascript" id="page.configurations">CONFIG.page={sidebar:""}</script><title>YOLOv3 | 慕湮</title><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?ca5844321cfb80fdf6f12b4dcc326991";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style type="text/css">.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.logo-line-after i{right:initial}</style></noscript></head><body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">慕湮</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">白日放歌须纵酒</h1></div><div class="site-nav-toggle"><button aria-label="切换导航栏"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://muyaan.com/2018/08/13/YOLOv3/"><span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="慕湮"><meta itemprop="description" content=""><meta itemprop="image" content="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4"></span><span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="慕湮"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">YOLOv3</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2018-08-13 19:38:32" itemprop="dateCreated datePublished" datetime="2018-08-13T19:38:32+08:00">2018-08-13</time> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2018-09-12 11:22:25" itemprop="dateModified" datetime="2018-09-12T11:22:25+08:00">2018-09-12</time> </span><span id="/2018/08/13/YOLOv3/" class="leancloud_visitors" data-flag-title="YOLOv3"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数：</span> <span title="本文字数">7.6k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">14 分钟</span></div></div></header><div class="post-body" itemprop="articleBody"><p>YOLOv3论文翻译，一个YOLOv2的升级版。作者还表达了对计算机视觉技术应用到不好的地方的担忧，并吐槽了论文评审和mAP评价方法。感觉是个超酷的人~</p><blockquote><p><a href="https://arxiv.org/abs/1804.02767" rel="external nofollow noopener noreferrer" target="_blank">YOLOv3: An Incremental Improvement</a><br>Joseph Redmon, Ali Farhadi, 2018.4<br>University of Washington</p></blockquote><h2><a href="#" class="headerlink"></a><a id="more"></a></h2><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>有时候人就是会漫不经心地过完一年。今年我并没有做特别多研究。花了很多时间在推特上。稍微玩耍了一下GAN。去年还剩余了一点动力，我成功的提升了一些YOLO。但实话说，没有什么超级有趣的，仅仅是一系列让其更好的小改变。我也参与了一些其他人的研究。</p><p>这就是如今情况的原因。我们有一个camera-ready截止日，我们需要引用一些我在YOLO中实现的更新，但我们没有来源。因此就有了这篇技术报告。</p><p>技术报告最棒的地方就是它们不需要介绍，大家都知道目前情况。首先我们会告诉你YOLOv3到底啥情况。接下来我们会告诉你我们怎么做的。我们还会告诉你我们试过但无效的方法。最后我们会思考这一切的意义。</p><h2 id="2-The-Deal"><a href="#2-The-Deal" class="headerlink" title="2. The Deal"></a>2. The Deal</h2><p>我们主要采用了其他人的主意。我们还训练了一个新的比其余好的分类网络。我们会带领你从草稿到整个系统，以便理解。</p><p><img src="/2018/08/13/YOLOv3/./1534134422688.png" alt="图1"></p><h3 id="2-1-Bounding-Box-Prediction"><a href="#2-1-Bounding-Box-Prediction" class="headerlink" title="2.1 Bounding Box Prediction"></a>2.1 Bounding Box Prediction</h3><p>我们按YOLO9000的方法，使用尺寸聚类作为锚点框来预测限位框。网络为每个限位框预测4个坐标，$t_x,t_y,t_w,t_h$。如果该格偏离图片左上角$(c_x,c_y)$，限位框prior宽高为$p_w,p_h$的话，预测就为：</p><script type="math/tex;mode=display">\begin{align}
b_x &= \sigma(t_x) + c_x   \notag \\
b_y &= \sigma(t_y) + c_y \notag\\
b_w &= p_w e ^{t_w} \notag\\
b_h &= p_h e ^{t_h} \notag\\
\end{align}</script><p>我们在训练中使用平方和误差loss。如果一些坐标预测的gt值为$\hat t_{\star}$，我们的梯度就是gt值减去预测值：$\hat t_{\star}-t_{\star}$。这一gt值通过上述公式变化能简单算出。</p><p>YOLOv3为每个限位框使用逻辑回归预测一个objectness分。如果一个prior比其余prior覆盖gt物体都多，该值为1。如果它不是最多的，但与gt物体的重叠超过了一个阈值（0.5），我们按[17]的做法忽略该预测。不像[17]，我们系统仅为每个gt物体赋予一个限位框prior。如果一个prior没有关联的gt物体，它将不参与坐标或分类预测loss计算，仅有objectness。</p><p><img src="/2018/08/13/YOLOv3/./1534147274150.png" alt="图2"></p><h3 id="2-2-Class-Prediction"><a href="#2-2-Class-Prediction" class="headerlink" title="2.2 Class Prediction"></a>2.2 Class Prediction</h3><p>每个限位框使用多标签分类预测该框可能包含的物体类别。我们没有使用softmax，因为发现它不是好的性能所必需的，使用了独立逻辑回归器。在训练中对于类预测我们使用了二分交叉熵loss。</p><p>这一方法有助于我们迁移到更复杂的领域，如Open Image Dataset$^{[7]}$。该数据集有许多重叠标签（如“女人”和“人”）。使用softmax需要假定每个框仅有一个类，然而事实常常不是这样。多标签方法更适合为该数据集建模。</p><h3 id="2-3-Predictions-Across-Scales"><a href="#2-3-Predictions-Across-Scales" class="headerlink" title="2.3 Predictions Across Scales"></a>2.3 Predictions Across Scales</h3><p>YOLOv3在3个不同尺度预测限位框。我们的系统用一个类似FPN$^{[8]}$的概念从那些尺度提取特征。我们为基础特征提取器增加了多个卷积层。最终预测一个编码了限位框、objectness和类预测的3维张量。在COCO实验中，我们在每个尺度预测3个限位框，故张量为：$N\times N\times [3*(4+1+80)]$，4个限位框偏移，1个objectness预测和80个类预测。</p><p>接着我们采用两层之前的特征图，并进行2倍的下采样。我们还采用了更早的特征图，并用串联法与未采样的特征合并。这一方法让我们能从未采样的特征得到更有意义的语义semantic信息，从更早的特征图得到细纹理信息。接下来我们又增加了几个卷积层来处理合并了的特征图，并最终预测一个类似的张量，但大小变为两倍。</p><p>我们再一次实现了同样的设计以为最终尺度预测限位框。因此为第三个尺度的预测收益与所有之前的计算，包括网络早期的细纹理特征。</p><p>我们还是使用k-means聚类来决定限位框prior。我们随机地选择了9个簇和3个尺度，并将簇均匀分到各scale中。在COCO上的9个簇是：$(10\times13)$，$(16\times30)$，$(33\times23)$，$(30\times61)$，$(62\times45)$，$(59\times119)$，$(116\times90)$，$(156\times198)$，$(373\times326)$。</p><h3 id="2-4-Feature-Extractor"><a href="#2-4-Feature-Extractor" class="headerlink" title="2.4 Feature Extractor"></a>2.4 Feature Extractor</h3><p>我们使用了新的网络来进行特征抽取。我们的新网络是YOLOv2使用的Darknet-19和新流行的残差网络的混合方法。我们的网络使用连续的$3\times3$和$1\times1$卷积层，但现在也有一些短路连接，而且深得多了。因为它有53个卷积层，因此称之为Darknet-53。<br><img src="/2018/08/13/YOLOv3/./1534150142987.png" alt="表1"></p><p>这一网络比Darknet-19强力得多，仍比ResNet-101或ResNet-152高效。下表是ImageNet结果：<br><img src="/2018/08/13/YOLOv3/./1534150396426.png" alt="表2"></p><p>每个网络都是用同样设定训练，并在$256\times256$上测试单切片准确度。运行时间在一块Titan X上测量。Darknet-53与前沿分类器有着近似的性能，但浮点运算数更少，速度更快。它比ResNet-101性能好，同时快1.5倍，跟ResNet-152性能相似，但快2倍。</p><p>Darknet-53同样获得了测得最高的每秒浮点数运算数。这意味着网络结构更好地利用了GPU。这主要因为ResNet有着过多的层，因此不够有效率。</p><h3 id="2-5-Training"><a href="#2-5-Training" class="headerlink" title="2.5 Training"></a>2.5 Training</h3><p>我们仍在完整图片上训练，未使用HNM或任何类似的东西。我们使用多尺度训练，许多数据增广方法，BN，所有标准的东西。我们使用Darknet网络进行训练和测试。</p><h2 id="3-How-We-Do"><a href="#3-How-We-Do" class="headerlink" title="3. How We Do"></a>3. How We Do</h2><p>YOLOv3相当棒！见表3。在COCO的奇怪的mAP评价方法下，它的性能类似SSD变种，但要快3倍。它仍在这一评判中落后于其它模型，如RetinaNet。</p><p><img src="/2018/08/13/YOLOv3/./1534151673939.png" alt="表3"></p><p>但假如我们查看旧的评价方法，mAP@.5，YOLOv3非常强大。它接近RetinaNet的性能，远超SSD的变种。这说明YOLOv3是强力的检测器，擅长为物体产出相当好的限位框。但当IoU阈值上升时性能的大幅下降，说明它很难将框完美对齐物体。</p><p>在过去YOLO在小物体上非常挣扎。但现在我们发现这一趋势开始逆转了。使用新的多尺度预测，我们看到YOLOv3有着相对高的AP性能。但相比起中大型物体，性能仍稍差。还需更多探索搞清楚这一问题。</p><p>当我们在AP50上画出精确度/速度曲线时（图3），我们发现YOLOv3明显好过其它系统。更快、更好。<br><img src="/2018/08/13/YOLOv3/./1534152814487.png" alt="图3"></p><h2 id="4-Things-We-Tried-That-Didn’t-Work"><a href="#4-Things-We-Tried-That-Didn’t-Work" class="headerlink" title="4. Things We Tried That Didn’t Work"></a>4. Things We Tried That Didn’t Work</h2><p>我们研究YOLOv3时尝试过许多方法。很多都不管用，以下是我们能回忆起的：</p><p><strong>Anchor box $x,y$ offset predictions.</strong> 我们尝试过常见锚点框预测机制，预测限位框宽高的倍数作为$x,y$偏移，并使用线性激活。我们发现这一方法会降低模型稳定性，并不特别有效。</p><p><strong>Linear $x,y$ predictions instead of logistic.</strong> 我们尝试过使用线性激活函数来直接预测$x,y$偏移，而不是逻辑激活。这导致mAP降低了几个点。</p><p><strong>Focal loss.</strong> 我们尝试过focal loss，它降低了我们2个点的mAP。也许对于focal loss尝试解决的问题，YOLOv3已经足够健壮。因为它将objectness预测和条件类概率预测分开了。因此大部分样本没有类预测的loss？或是其他原因？我们也不是完全确定。</p><p><strong>Dual IoU threshold and truth assignment.</strong> Faster R-CNN在训练时使用了两个IoU阈值。如果预测与gt的重叠超过0.7，它就是正样本，[.3-.7]之间忽略，对于所有gt物体都低于.3的作为负样本。我们使用了类似策略但结果不佳。</p><p>我们对当前的方法比较满意，它似乎至少是一个局部最优解。也许这些方法最终会得到好的结果，也许它们仅需要一些调优来稳定训练。</p><h2 id="5-What-This-All-Means"><a href="#5-What-This-All-Means" class="headerlink" title="5. What This All Means"></a>5. What This All Means</h2><p>YOLOv3是一个好的检测器，它快速而精确。它在COCO评价方法上没那么强大，但在老的mAP@.5上非常好。</p><p>为何我们要切换评价方法呢。原始COCO论文有如下神秘句子：“一旦评估服务器完成，会添加完整的评估方法讨论”。Russakovsky等人报告人类区分.3到.5的IoU非常困难。“训练人类使其区别IoU0.3和0.5超乎想象地困难”$^{[18]}$。如果人类都难以区分，那意义何在？</p><p>但或许更好的问题是：“我们现在有了这些检测器，我们用来做什么呢？”。许多相关研究人员在Google和Facebook工作。我猜想这一技术不会用于挖掘个人信息并卖给。。。啥，你说这正是它要使用的地方？哦。。。</p><p>那么其他重金资助视觉研究的都是军队，他们不会做任何恐怖的事，比如用新技术杀掉许多人。。。哦等一下（作者由海军研究办公室和Google资助）。</p><p>我希望大部分人使用计算机视觉来做一些快乐、好的事情，如计算国家公园中的斑马数量$^{[13]}$，或追踪家里乱晃的猫$^{[19]}$。但计算机视觉已用于可疑的地方，作为研究者我们应由责任，至少考虑我们的工作可能的危害，并想办法缓解它。我们欠这个世界那么多。</p><p>最后，别再@我了（因为我终于退出推特了）。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] Analogy. Wikipedia, Mar 2018. 1<br>[2] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and<br>A. Zisserman. The pascal visual object classes (voc) challenge.<br>International journal of computer vision, 88(2):303–<br>338, 2010. 6<br>[3] C.-Y. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg.<br>Dssd: Deconvolutional single shot detector. arXiv preprint<br>arXiv:1701.06659, 2017. 3<br>[4] D. Gordon, A. Kembhavi, M. Rastegari, J. Redmon, D. Fox,<br>and A. Farhadi. Iqa: Visual question answering in interactive<br>environments. arXiv preprint arXiv:1712.03316, 2017. 1<br>[5] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning<br>for image recognition. In Proceedings of the IEEE conference<br>on computer vision and pattern recognition, pages<br>770–778, 2016. 3<br>[6] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara,<br>A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, et al.<br>Speed/accuracy trade-offs for modern convolutional object<br>detectors. 3<br>[7] I. Krasin, T. Duerig, N. Alldrin, V. Ferrari, S. Abu-El-Haija,<br>A. Kuznetsova, H. Rom, J. Uijlings, S. Popov, A. Veit,<br>S. Belongie, V. Gomes, A. Gupta, C. Sun, G. Chechik,<br>D. Cai, Z. Feng, D. Narayanan, and K. Murphy. Openimages:<br>A public dataset for large-scale multi-label and<br>multi-class image classification. Dataset available from<br><a href="https://github.com/openimages" rel="external nofollow noopener noreferrer" target="_blank">https://github.com/openimages</a>, 2017. 2<br>[8] T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, and<br>S. Belongie. Feature pyramid networks for object detection.<br>In Proceedings of the IEEE Conference on Computer Vision<br>and Pattern Recognition, pages 2117–2125, 2017. 2, 3<br>[9] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar. ´<br>Focal loss for dense object detection. arXiv preprint<br>arXiv:1708.02002, 2017. 1, 3, 4<br>[10] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,<br>P. Dollar, and C. L. Zitnick. Microsoft coco: Com- ´<br>mon objects in context. In European conference on computer<br>vision, pages 740–755. Springer, 2014. 2<br>[11] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-<br>Y. Fu, and A. C. Berg. Ssd: Single shot multibox detector.<br>In European conference on computer vision, pages 21–37.<br>Springer, 2016. 3<br>[12] I. Newton. Philosophiae naturalis principia mathematica.<br>William Dawson &amp; Sons Ltd., London, 1687. 1<br>[13] J. Parham, J. Crall, C. Stewart, T. Berger-Wolf, and<br>D. Rubenstein. Animal population censusing at scale with<br>citizen science and photographic identification. 2017. 4<br>[14] J. Redmon. Darknet: Open source neural networks in c.<br><a href="http://pjreddie.com/darknet/" rel="external nofollow noopener noreferrer" target="_blank">http://pjreddie.com/darknet/</a>, 2013–2016. 3<br>[15] J. Redmon and A. Farhadi. Yolo9000: Better, faster, stronger.<br>In Computer Vision and Pattern Recognition (CVPR), 2017<br>IEEE Conference on, pages 6517–6525. IEEE, 2017. 1, 2, 3<br>[16] J. Redmon and A. Farhadi. Yolov3: An incremental improvement.<br>arXiv, 2018. 4<br>[17] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards<br>real-time object detection with region proposal networks.<br>arXiv preprint arXiv:1506.01497, 2015. 2<br>[18] O. Russakovsky, L.-J. Li, and L. Fei-Fei. Best of both<br>worlds: human-machine collaboration for object annotation.<br>In Proceedings of the IEEE Conference on Computer Vision<br>and Pattern Recognition, pages 2121–2131, 2015. 4<br>[19] M. Scott. Smart camera gimbal bot scanlime:027, Dec 2017.<br>4<br>[20] A. Shrivastava, R. Sukthankar, J. Malik, and A. Gupta. Beyond<br>skip connections: Top-down modulation for object detection.<br>arXiv preprint arXiv:1612.06851, 2016. 3<br>[21] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi.<br>Inception-v4, inception-resnet and the impact of residual<br>connections on learning. 2017. 3</p><h2 id="Rebuttal"><a href="#Rebuttal" class="headerlink" title="Rebuttal"></a>Rebuttal</h2><p>吐槽ICCV的论文审阅，反驳了评委的审阅意见。吐槽了目前的评价方式mAP。</p><p><img src="/2018/08/13/YOLOv3/./1534159729671.png" alt="图4"></p></div><div class="popular-posts-header">相关文章</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="/2018/08/03/YOLO/" rel="bookmark">YOLO</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/2018/08/12/YOLOv2-and-YOLO9000/" rel="bookmark">YOLOv2 and YOLO9000</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/2018/08/07/SSD/" rel="bookmark">SSD</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/2018/08/01/Faster-R-CNN/" rel="bookmark">Faster R-CNN</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/2018/07/28/Fast-R-CNN/" rel="bookmark">Fast R-CNN</a></div></li></ul><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>慕湮</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="http://muyaan.com/2018/08/13/YOLOv3/" title="YOLOv3">http://muyaan.com/2018/08/13/YOLOv3/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noopener noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Computer-Vision/" rel="tag"># Computer Vision</a> <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a> <a href="/tags/Paper/" rel="tag"># Paper</a> <a href="/tags/Object-Detection/" rel="tag"># Object Detection</a> <a href="/tags/YOLO/" rel="tag"># YOLO</a> <a href="/tags/YOLOv3/" rel="tag"># YOLOv3</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2018/08/12/YOLOv2-and-YOLO9000/" rel="next" title="YOLOv2 and YOLO9000"><i class="fa fa-chevron-left"></i> YOLOv2 and YOLO9000</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2018/08/16/GoogLeNet-v1/" rel="prev" title="GoogLeNet v1">GoogLeNet v1 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4" alt="慕湮"><p class="site-author-name" itemprop="name">慕湮</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">22</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">31</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/tycallen" target="_blank" title="GitHub" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:tyc.allen@gmail.com" target="_blank" title="E-Mail" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-envelope"></i>E-Mail</a> </span><span class="links-of-author-item"><a href="http://weibo.com/pojunallen" target="_blank" title="微博" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-weibo"></i>微博</a> </span><span class="links-of-author-item"><a href="https://tuchong.com/1070837" target="_blank" title="图虫" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-globe"></i>图虫</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-block"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> 友情链接</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="http://dotrabbit.tk" title="dotrabbit" target="_blank" rel="external nofollow noopener noreferrer">dotrabbit</a></li></ul></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text"></span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Introduction"><span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-The-Deal"><span class="nav-text">2. The Deal</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-Bounding-Box-Prediction"><span class="nav-text">2.1 Bounding Box Prediction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Class-Prediction"><span class="nav-text">2.2 Class Prediction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-Predictions-Across-Scales"><span class="nav-text">2.3 Predictions Across Scales</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-Feature-Extractor"><span class="nav-text">2.4 Feature Extractor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-Training"><span class="nav-text">2.5 Training</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-How-We-Do"><span class="nav-text">3. How We Do</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Things-We-Tried-That-Didn’t-Work"><span class="nav-text">4. Things We Tried That Didn’t Work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-What-This-All-Means"><span class="nav-text">5. What This All Means</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-text">References</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Rebuttal"><span class="nav-text">Rebuttal</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; 2015 – <span itemprop="copyrightYear">2018</span> <span class="with-love" id="animate"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">慕湮</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="站点总字数">320k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">9:42</span></div><div class="powered-by">由 <a class="theme-link" target="_blank" rel="external nofollow noopener noreferrer" href="https://hexo.io">Hexo</a> 强力驱动</div><span class="post-meta-divider">|</span><div class="theme-info">主题 – <a class="theme-link" target="_blank" rel="external nofollow noopener noreferrer" href="https://theme-next.org">NexT.Muse</a></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="//cdn.staticfile.org/jquery/2.1.3/jquery.min.js"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="//cdn.staticfile.org/velocity/1.2.1/velocity.min.js"></script><script type="text/javascript" src="//cdn.staticfile.org/velocity/1.2.1/velocity.ui.min.js"></script><script type="text/javascript" src="//cdn.staticfile.org/fancybox/2.1.5/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=6.4.1"></script><script type="text/javascript" src="/js/src/motion.js?v=6.4.1"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=6.4.1"></script><script type="text/javascript" src="/js/src/post-details.js?v=6.4.1"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.1"></script><script>function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();

      Counter('get', '/classes/Counter', { where: JSON.stringify({ url }) })
        .done(function ({ results }) {
          if (results.length > 0) {
            var counter = results[0];
            
              var $element = $(document.getElementById(url));
              $element.find('.leancloud-visitors-count').text(counter.time + 1);
            
            Counter('put', `/classes/Counter/${counter.objectId}`, JSON.stringify({ time: { "__op":"Increment", "amount":1 } }))
            
            .fail(function ({ responseJSON }) {
                console.log('Failed to save Visitor num, with error message: ' + responseJSON.error);
            })
          } else {
            
              Counter('post', '/classes/Counter', JSON.stringify({ title: title, url: url, time: 1}))
                .done(function () {
                  var $element = $(document.getElementById(url));
                  $element.find('.leancloud-visitors-count').text(1);
                })
                .fail(function () {
                  console.log('Failed to create');
                });
            
          }
        })
      .fail(function ({ responseJSON }) {
        console.log('LeanCloud Counter Error:' + responseJSON.code + " " + responseJSON.error);
      });
    }
    

    $(function() {
      $.get('https://app-router.leancloud.cn/2/route?appId=' + "UWGXBMGbguCDhSCbC3NrUQtL-gzGzoHsz")
        .done(function ({ api_server }) {
          var Counter = function (method, url, data) {
            return $.ajax({
              method: method,
              url: `https://${api_server}/1.1${url}`,
              headers: {
                'X-LC-Id': "UWGXBMGbguCDhSCbC3NrUQtL-gzGzoHsz",
                'X-LC-Key': "ke1jrA5b6VyR89Kqqqwf2kPP",
                'Content-Type': 'application/json',
              },
              data: data,
            });
          };
          
          addCount(Counter);
          
        })
    });</script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="http://muyaan.com/js/src/async.js"></script></body></html>