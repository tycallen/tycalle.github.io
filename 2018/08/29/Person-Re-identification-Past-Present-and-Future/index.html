<!DOCTYPE html><html class="theme-next muse use-motion" lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><script src="https://cdn.staticfile.org/pace/1.0.2/pace.min.js"></script><link href="https://cdn.staticfile.org/pace/1.0.2/themes/blue/pace-theme-big-counter.min.css" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="//cdn.staticfile.org/fancybox/2.1.5/jquery.fancybox.min.js" rel="stylesheet" type="text/css"><link href="//cdn.staticfile.org/font-awesome/4.6.2/css/font-awesome.min.css" rel="stylesheet" type="text/css"><link href="/css/main.css?v=6.4.1" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.4.1"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.4.1"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.4.1"><link rel="mask-icon" href="/images/logo.svg?v=6.4.1" color="#222"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Muse",version:"6.4.1",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!0,fastclick:!1,lazyload:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="description" content="一篇Person re-ID的调研型论文。由于对re-ID一无所知，就选择了它作为切入点。不得不说翻译本论文实在不是个好主意，20页真的有点多。前期也翻译了不少其它论文，翻译相比直接阅读有利有弊吧，不过以后会有选择性的翻译，注重总结了。Person Re-identification: Past, Present and FutureLiang Zheng, Yi Yang, Alexander"><meta name="keywords" content="Deep Learning,Paper,re-ID"><meta property="og:type" content="article"><meta property="og:title" content="Person Re-identification: Past, Present and Future"><meta property="og:url" content="http://muyaan.com/2018/08/29/Person-Re-identification-Past-Present-and-Future/index.html"><meta property="og:site_name" content="慕湮"><meta property="og:description" content="一篇Person re-ID的调研型论文。由于对re-ID一无所知，就选择了它作为切入点。不得不说翻译本论文实在不是个好主意，20页真的有点多。前期也翻译了不少其它论文，翻译相比直接阅读有利有弊吧，不过以后会有选择性的翻译，注重总结了。Person Re-identification: Past, Present and FutureLiang Zheng, Yi Yang, Alexander"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="http://muyaan.com/2018/08/29/Person-Re-identification-Past-Present-and-Future/1534841523660.png"><meta property="og:image" content="http://muyaan.com/2018/08/29/Person-Re-identification-Past-Present-and-Future/1534841558725.png"><meta property="og:image" content="http://muyaan.com/2018/08/29/Person-Re-identification-Past-Present-and-Future/1534846564392.png"><meta property="og:image" content="http://muyaan.com/2018/08/29/Person-Re-identification-Past-Present-and-Future/1534987637891.png"><meta property="og:image" content="http://muyaan.com/2018/08/29/Person-Re-identification-Past-Present-and-Future/1534992372656.png"><meta property="og:image" content="http://muyaan.com/2018/08/29/Person-Re-identification-Past-Present-and-Future/1534995644290.png"><meta property="og:image" content="http://muyaan.com/2018/08/29/Person-Re-identification-Past-Present-and-Future/1535007776910.png"><meta property="og:image" content="http://muyaan.com/2018/08/29/Person-Re-identification-Past-Present-and-Future/1535011091444.png"><meta property="og:image" content="http://muyaan.com/2018/08/29/Person-Re-identification-Past-Present-and-Future/1535177028113.png"><meta property="og:image" content="http://muyaan.com/2018/08/29/Person-Re-identification-Past-Present-and-Future/1535082836117.png"><meta property="og:image" content="http://muyaan.com/2018/08/29/Person-Re-identification-Past-Present-and-Future/1535106560787.png"><meta property="og:image" content="http://muyaan.com/2018/08/29/Person-Re-identification-Past-Present-and-Future/1535177004460.png"><meta property="og:image" content="http://muyaan.com/2018/08/29/Person-Re-identification-Past-Present-and-Future/1535355590351.png"><meta property="og:image" content="http://muyaan.com/2018/08/29/Person-Re-identification-Past-Present-and-Future/1535513409052.png"><meta property="og:updated_time" content="2018-09-12T03:22:25.049Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Person Re-identification: Past, Present and Future"><meta name="twitter:description" content="一篇Person re-ID的调研型论文。由于对re-ID一无所知，就选择了它作为切入点。不得不说翻译本论文实在不是个好主意，20页真的有点多。前期也翻译了不少其它论文，翻译相比直接阅读有利有弊吧，不过以后会有选择性的翻译，注重总结了。Person Re-identification: Past, Present and FutureLiang Zheng, Yi Yang, Alexander"><meta name="twitter:image" content="http://muyaan.com/2018/08/29/Person-Re-identification-Past-Present-and-Future/1534841523660.png"><link rel="canonical" href="http://muyaan.com/2018/08/29/Person-Re-identification-Past-Present-and-Future/"><script type="text/javascript" id="page.configurations">CONFIG.page={sidebar:""}</script><title>Person Re-identification: Past, Present and Future | 慕湮</title><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?ca5844321cfb80fdf6f12b4dcc326991";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style type="text/css">.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.logo-line-after i{right:initial}</style></noscript></head><body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">慕湮</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">白日放歌须纵酒</h1></div><div class="site-nav-toggle"><button aria-label="切换导航栏"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://muyaan.com/2018/08/29/Person-Re-identification-Past-Present-and-Future/"><span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="慕湮"><meta itemprop="description" content=""><meta itemprop="image" content="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4"></span><span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="慕湮"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">Person Re-identification: Past, Present and Future</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2018-08-29 20:36:11" itemprop="dateCreated datePublished" datetime="2018-08-29T20:36:11+08:00">2018-08-29</time> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2018-09-12 11:22:25" itemprop="dateModified" datetime="2018-09-12T11:22:25+08:00">2018-09-12</time> </span><span id="/2018/08/29/Person-Re-identification-Past-Present-and-Future/" class="leancloud_visitors" data-flag-title="Person Re-identification: Past, Present and Future"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数：</span> <span title="本文字数">62k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">1:52</span></div></div></header><div class="post-body" itemprop="articleBody"><p>一篇Person re-ID的调研型论文。由于对re-ID一无所知，就选择了它作为切入点。不得不说翻译本论文实在不是个好主意，20页真的有点多。前期也翻译了不少其它论文，翻译相比直接阅读有利有弊吧，不过以后会有选择性的翻译，注重总结了。</p><blockquote><h1 id="Person-Re-identification-Past-Present-and-Future"><a href="#Person-Re-identification-Past-Present-and-Future" class="headerlink" title="Person Re-identification: Past, Present and Future"></a><a href="https://arxiv.org/abs/1610.02984" rel="external nofollow noopener noreferrer" target="_blank">Person Re-identification: Past, Present and Future</a></h1><p>Liang Zheng, Yi Yang, Alexander G. Hauptmann, 2016.10</p><p>University of Technology at Sydney, Carnegie Mellon University</p></blockquote><h2><a href="#" class="headerlink"></a><a id="more"></a></h2><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>据荷马所说（荷马史诗 iv:412），斯巴达王墨涅拉奥斯Mennelaus在特洛伊战争后回家的旅程非常平静；他希望与神和解，平安返家。他被告知他必须俘获普罗透斯（希腊海神）Proteus，逼迫他揭晓答案。尽管普罗透斯变形为狮子，大蛇，豹，水甚至一棵树，墨涅拉奥斯成功地在他离开海洋与一群海豹睡觉时捉住了他。普罗透斯最终告诉了他答案。</p><p>也许这是关于在剧烈的外表改变下重识别一个人的最古老的故事。在1961年，当讨论到精神状态与行为的关系时，Alvin Plantinga[1]提供了re-identification的第一个定义：</p><blockquote><p><em>“ To re-identify a particular, then, is to identify it as (numerically) the same particular as one encountered on a previous occasion “.</em></p></blockquote><p>因此，Person re-ID被各种研究领域学习，如玄学metaphysics[1]，心理学[2]和逻辑学[3]。所有研究都基于莱布尼兹法则Leibniz’s Law，“如果它们所有属性都相同，则它们不可能是不同的实体”。</p><p>在现代计算机视觉领域，person re-ID任务与过去有着相似的思考。在视频监控领域，person re-ID通常以person-of-interest (query)的形式告知某人是否被另一个摄像头在另一个地点/时间锁观察到。这一任务的出现可以归功于1) 公共安全需求的增长和2) 遍布主题公园、大学、街道的大量摄像头网络。两个原因都使得单纯依赖人力暴力地来高效而精确地识别person-of-interest或跨摄像头跟踪一个人过于昂贵。</p><p>技术上讲，实用的视频监控person re-ID系统可以分解为3个模块，如，行人检测detection，行人追踪tracking和行人检索retrieval。通常认为前两个模块是独立的计算机视觉任务，故大部分研究集中于最后一个模块，行人检索。在本调查中如无特指，re-ID就是指行人检索。从计算机视觉的观点来看，re-ID最有挑战的问题是如何在剧烈的外观变化下正确地匹配同一个人的两张图片，如光照，姿势和视角，它有着重要的科学价值。拥有了研究和应用意义后，re-ID社区迅速增长，不断增长的顶级会刊发表量就是证据（图1）。<br><img src="/2018/08/29/Person-Re-identification-Past-Present-and-Future/./1534841523660.png" alt="图1"></p><h3 id="1-1-Organization-of-This-Survey"><a href="#1-1-Organization-of-This-Survey" class="headerlink" title="1.1 Organization of This Survey"></a>1.1 Organization of This Survey</h3><p>已经有了一些re-ID的调查：[4, 5, 6, 7]。在本调查中，我们主要讨论re-ID的视觉部分，也是社区的焦点，给与读者相机标定camera calibration和[5]中拓扑学方法的参考。另一个与之前调查的不同之处在于我们关注已有的或未来可行的不同re-ID子任务，而不是技术或架构的细节。其中深度学习方法、端到端的re-ID和大规模re-ID是重点，都是当下流行主题或未来趋势。这一报告首先在1.2简短介绍了re-ID的历史，在1.3介绍了与分类和检索的关系。我们在章节2、3分别描述了基于图片的和基于视频的文献。两个章节都分类为手工制作或深度学习系统。在章节4，因为检测、追踪和re-ID的关系尚未被广泛研究，我们会讨论一些研究并指出未来研究重点。在章节5介绍了采用了前沿的检索模型的大规模re-ID系统，同样是未来重要方向。章节6为未决问题的总结，章节7是论文总结。</p><h3 id="1-2-A-Brief-History-of-Person-Re-ID"><a href="#1-2-A-Brief-History-of-Person-Re-ID" class="headerlink" title="1.2 A Brief History of Person Re-ID"></a>1.2 A Brief History of Person Re-ID</h3><p>Person re-ID的研究始于多摄像头追踪[8]。自那时起已发展处多个重要方向。在本调查中，我们简短介绍了person re-ID的一些里程碑（图2）。</p><p><img src="/2018/08/29/Person-Re-identification-Past-Present-and-Future/./1534841558725.png" alt="图2"></p><p><strong>Multi-camera tracking.</strong> 在早年，person re-ID这个词还未被正式提出的时候，紧紧地跟多摄像头跟踪一起出现，它将外观模型与不相交的摄像头的几何校正geometry calibration集成。1997年，Huang和Russel[9]提出了一个用于在给与其它摄像头观察证据后，估计它在一个摄像头的后续出现预测的贝叶斯公式。其外观模型包括多个时空spartial-temporal特征如颜色，车长vehicle length，宽高，速度和观测时间。关于这一主题的综合调查见[8]。</p><p><strong>Multi-camera tracking with explicit “re-identification”.</strong> 据我们所知，最早提出“person re-identification”的关于多摄像头追踪的论文是2005年来自阿姆斯特丹大小的Wojciech Zajdel, Zoran Zivkovic和Ben J.A Krose[10]提出的。在他们的ICRA’05年的“Keeping track of humans: Have I seen this person before?”中，他们的目标是“在一个人离开视野又回来后重识别他”。在他们的方法中，为每个人赋予了一个独特，潜在latent的标签，并定义了一个动态的贝叶斯网络来编码标签和来自踪片（追踪小片段tracklets）的特征（颜色和时空线索）的概率性关系。一个进入的人的ID通过用近似贝叶斯推理算法Bayesian inference algorithm计算的后续标签分布决定。</p><p><strong>The independence of re-ID (image-based). </strong>2006年，Gheissari等人[11]在一个用于前景检测的时空分割算法后，仅用了人的视觉线索。基于颜色和显著边缘直方图salient edgel histograms的视觉匹配是通过一个清晰的行人模型或Hessian-Affine interest point operator完成的。实验在一个由3个有适度视野重叠的摄像头捕获的44个人的数据集上进行。值得一提的是尽管它们的时空分段方法使用了视频帧，特征设计和匹配过程都没有使用视频信息，故我们将其分类为基于图片的re-ID。这一论文标志着person re-ID与多摄像头追踪的分离，以及作为独立计算机视觉任务的开始。</p><p><strong>Video-based re-ID. </strong>最初用于在视频进行追踪，但大部分re-ID研究着重于图片匹配。在2010年，两个论文提出了multishot re-ID[12, 13]，它们随机选取帧。颜色是两个论文都使用了的特征，而Farenzena等人的[13]额外使用了一个分割模型来检测前景。对于距离测量，两个论文都是计算两个图片集的限位框间最小距离，Bazzani等人[12]进一步对颜色和一般缩影特征generic epitome features使用了巴氏Bhattacharyya距离。证明了对每个人使用多帧比单帧有效，而随着选择的帧数量增加，re-ID准确度会饱和。</p><p><strong>Deep learing for re-ID. </strong>2014年，深度学习在图片分类[14]上的成功散布到了re-ID，Yi等人[15]和Li等人[16]都使用一个孪生siamese卷积网络[17]来决定是否一对输入图片属于同一个ID。选择孪生模型的原因也许是各个身份的训练样本数量有限（通常是两个）。除开一些参数设定上的变种，主要区别在于[15]为网络增加了一个额外的cost函数，而[16]使用了更好的身体划分。它们使用的数据集不同，因此两个方法没法直接比较。尽管在小数据集上的性能还未稳定，深度学习方法已成为re-ID的流行选项。</p><p><strong>End-to-end image-based re-ID. </strong>尽管大部分研究实验中都使用手工切分的限位框或由固定检测器输出的限位框，研究行人检测器对于re-ID准确度的影响仍有必要。2014年，Xu等人[18]通过结合检测(commonness)和re-ID(uniqueness)得分解决了这一题目。在CAMPUS数据集上发现，联合考虑检测和re-ID得分能比分开使用得到更高的行人检索准确度。</p><h3 id="1-3-Relationship-with-Classification-and-Retrieval"><a href="#1-3-Relationship-with-Classification-and-Retrieval" class="headerlink" title="1.3 Relationship with Classification and Retrieval"></a>1.3 Relationship with Classification and Retrieval</h3><p><img src="/2018/08/29/Person-Re-identification-Past-Present-and-Future/./1534846564392.png" alt="表1"><br>在训练和测试类的关系形式上，Person re-ID落在图片分类[14]和实体检索[19]之间（表1）。对于图片分类，各类都有训练图片，而测试图片都可以归到预定义的类中，记做表1的”seen”。对于实体检索，通常没有训练数据，因为没人能预知查询内容而且数据集可能有多重物体。故训练类是”not available”而测试类（查询）是”unseen”。</p><p>与图片分类相比，person re-ID相似点是都有训练类，包括不同身份的图片。re-ID与实体检索相似点是测试身份是未见过的：他们与训练身份没有交集，除了他们都是行人图片以外。</p><p>因此re-ID能利用分类和检索。一方面使用训练类，区别discriminative距离度量[20]或特征嵌入embedding[16, 21]能从行人空间person space习得。另一方面，当需要检索时，re-ID能受益于高效的索引结构[22]和哈希技术[23]。在本调查中，高效的训练和检索方法都会加以介绍，以作为未来重要方向。</p><h2 id="2-Image-Based-Person-Re-ID"><a href="#2-Image-Based-Person-Re-ID" class="headerlink" title="2. Image-Based Person Re-ID"></a>2. Image-Based Person Re-ID</h2><p>自2006年Gheissari等人[11]的研究起，person re-ID主要以使用单张图片的方式进行。假设一个封闭世界玩具模型，其中$\mathbb{G}$是由N个图片组成的数据集，记做$\{g_i\}_{i=1}^N$。它们分别属于N个不同身份$1, 2, …, N$。给定查询图片q，其身份通过如下方程确定：</p><script type="math/tex;mode=display">i^{\star}= {\arg \max} _{i\in 1,2,...,N} sim (q,g_i) \tag 1</script><p>其中$i^{\star}$是查询q的身份，sim(.,.)是一种相似函数。</p><h3 id="2-1-Hand-crafted-Systems"><a href="#2-1-Hand-crafted-Systems" class="headerlink" title="2.1 Hand-crafted Systems"></a>2.1 Hand-crafted Systems</h3><p>从等式1能看出玩具re-ID系统所需的两个组件为图片描述和距离度量。</p><h4 id="2-1-1-Pedestrian-Description"><a href="#2-1-1-Pedestrian-Description" class="headerlink" title="2.1.1 Pedestrian Description"></a>2.1.1 Pedestrian Description</h4><p>行人描述中最常用的特征是颜色，纹理特征用得较少。在[13]中，行人前景从背景中分割出来，并为每个身体部分计算对称轴。基于身体配置，加权颜色直方图WH，最大稳定色区maximally stable color regions MSCR，recurrent high-structured patches RHSP 均被算出。WH为靠近对称轴的像素赋予更大的权重，并为每个部分形成颜色直方图。MSCR检测稳定色区并提取如颜色、区域和图心等特征。RHSP是捕捉周期纹理块的纹理特征。Gheissari等人[11]提出了一个检测稳定前景区域的时空分割方法。对于一个局部区域，会计算HS和edgel直方图。接着会编码主要局部边界方向dominant local boundary orientation及edgel两边的RGB比值。Gray和Tao[24]使用8色通道（RGB，HS和YCbCr）和亮度luminance通道上的21个特征过滤器，行人被水平带状分割。许多近期研究[25, 26, 27]使用了与[24]同样的特征集。类似的，Mignon等人[28]建立了基于RGB，YUV和HSV通道的特征向量和水平条状LBP纹理直方图。</p><p>与前文描述的早期研究相比，近年的手动构造的特征或多或少有些一致[20, 29, 30, 31, 32]。在Zhao等人一系列研究中[30, 33, 34]，通过$10\times10$步长为5的密集采样抽取了128维SIFT和32维LAB颜色直方图；这一特征在[35]也用到了。Adjacency constrained search用于所查询的块找到图片集中的有着相似纬度水平条的最佳匹配。Das等人将HSV直方图应用到来自[12]提出的轮廓silhouette的头部、躯干torso和四肢。Li等人[31]同样从小块中提取local color descriptor，但用了层级高斯化hierarchical Gaussianization聚合它们以捕获空间信息，这是学习[38]的步骤。Pedagadi等人[39]在使用PAC缩减维度前从HSV和YUV空间提取颜色直方图和moments。Liu等人[40]为每个local patch提取了HSV直方图，梯度直方图和LBP直方图。为了提升RGB值对光度变化的健壮性，Yang等人[41]提出了用于全局行人颜色描述的salient color names based color descriptor(SCNCD)。背景和不同色彩空间的影响也被分析了。在[20]，Liao等人提出了local maximal occurrence(LOMO) descriptor，包含了颜色和SILTP直方图。同一水平条里的二进制bins经历一个max pooling和一个log变换前的3尺度金字塔模型。LOMO后来也被[42, 43]使用，Chen等人[32]也使用了类似的特征集。在[44]中，Zheng等人提出了为每个local patch提取11维的color name descriptor，并用一个Bag-of-Words（BoW）模型将它们聚集到一个全局向量。在[46]中，提出了hierarchical Gaussian特征来描述颜色和纹理线索，通过多个高斯分布为每个区域建模。每个分布都代表区域中的一个patch。</p><p>除了直接使用低维的颜色和纹理特征，另一个好的选择是基于属性attribute-based的特征，可以看做中维度的表达。通常认为与低维的描述符descriptor比起来，属性对于图像平移image translation更健壮。在[47]中，Layne等人在VIPeR数据集标注了15个关于服装attire和软生物识别soft biometrics的二元属性。低维的颜色与纹理特征被用于训练属性分类器。在属性加权后，结果向量集成到SDALF[13]框架，与其余视觉特征混合。Liu等人[48]使用标注的属性过滤噪音LDA topics，提升了latent Dirichlet allocation(LDA)模型。Liu等人[49]提出了以无监督的形式通过共同属性发现一些行人原型prototype，并依据这些原型自适应地为不同的查询人决定特征权重。一些近期研究使用外部数据来训练属性。在[50]中，Su等人把同一个人在不同的摄像头下的二元语义属性binary semantic attributes嵌入到连续low-rank属性空间，使属性向量对匹配更具辨识力。Shi等人[51]提出了从现有流行摄影数据集中学习一系列属性，包括颜色、纹理和类别标签。这些属性可以直接转移到监控视频的re-ID并获得有竞争力的结果。近期Li等人[52]采集了有富标注的行人属性的大规模数据集，来帮助基于属性的re-ID方法。</p><h4 id="2-1-2-Distance-Metric-Learning"><a href="#2-1-2-Distance-Metric-Learning" class="headerlink" title="2.1.2 Distance Metric Learning"></a>2.1.2 Distance Metric Learning</h4><p>在手工构建的re-ID系统中，好的距离度量对于其成功是决定性的，因为高维视觉特征通常不会捕捉样本偏移下的不变性因素。关于距离度量训练方法的完整调查可以看[53]。这些度量训练方法可被分类为监督学习 vs 无监督学习，global learning vs local learning。在person re-ID，主要研究是supervised global distance metric learning。</p><p>global metric learing的大体思想是让相同类的向量更近，不同类的向量更远。最常用的方程基于马氏Mahalanobis距离，通过用特征空间用线性缩放和旋转将欧氏距离泛化而来。两个向量$x_i, x_j$的平方距离可以写作：</p><script type="math/tex;mode=display">d(x_i,x_j) = (x_i,x_j)^T M(x_i,x_j)  \tag 2</script><p>其中M是正半定semidefinite矩阵。按Xing等人[54]的启发，等式2可以转化为凸编程问题。</p><p>在person re-ID中，当前最流行的独立训练方法KISSME[55]基于等式2的。该方法中，一对(i, j)是否类似的决策表达为似然率likelihood ratio测试。使用了逐对差异$x_{i,j} = x_i - x_j$，差异空间假定为一个0均值的高斯分布。在[55]中说明，马氏距离度量可以自然地源于对数似然率测试，而且实践中，为数据点使用了主元分析PCA来消除维度相关性。</p><p>基于等式2，有许多度量训练方法被提出。在早期，一些经典度量训练方法以最近邻分类为目标。Weinberger等人[56]提出了大边缘large margin最近邻训练(LMNN)方法，它为目标邻居（匹配对）设置了周界perimeter，并惩罚入侵了周界的（冒名顶替着imposters）。这一方法属于supervised local distance metric learning类别[53]。为了避免LMNN遇到的过拟合问题，Davis等人[57]提出了information-theoretic metric learning(ITML)作为满足给定相似点的约束和保证习得的度量与初始距离函数足够近的权衡。</p><p>近几年，Hirzer等人[58]提出了放松正向positivity约束，它为矩阵M提供了充分的近似，但计算消耗低很多。Chen等人[38]为马氏距离额外增加了一个双线性bilinear相似点，使得跨patch相似点能被建模。在[31]中，global distance metric与包含了$(x_i,x_j)$的正交orthogonal信息的local adaptive threshold rule结合。在[59]中，Liao等人建议坚持perserving 正半定semidefinite约束，并提出为正负样本赋予不同的权值。Yang等人[60]考虑了图片对的差别和共同点，发现不相似对的协方差矩阵可从相似对的推测出来，使训练过程能扩展到大数据集。</p><p>除了训练距离度量，一些研究关注于训练有识别力的子空间。Liao等人[20]提出训练投影w到低维子空间，以一种类似linear discriminant analysis（LDA）[61]的方式解决cross-view数据：</p><script type="math/tex;mode=display">\mathfrak J(w) = \frac {w^T S_b w} {W^T S_w w} \tag 3</script><p>其中$S_b, S_w$分别是between-class和within-class分别是散布矩阵。接着在得到的子空间使用KISSME训练一个距离函数。为了训练w，Zhang等人[42]进一步运用了Foley-Sammon变换来训练一个有辨识力的零空间，满足类内within-class分散scatter为0且类间between-class分散为正值。关于维度缩减，Pedagadi等人[39]顺序结合了无监督PCA和保留了局部邻居结构local neighborhood structure的有监督local Fisher discriminative analysis。在[28]中，逐对约束元分析pairwise constrained component analysis(PCCA)被提出，它训练了一个可以直接在高维数据上工作的线性映射函数，而ITML和KISSME需要一步维度缩减。在[62]中，Xiong等人进一步提出了两个已有子空间投影方法的加强版，分别是regularized PCCA[28]和kernel LFDA[39]。</p><p>除了使用马氏距离（等式2）的方法，有人还使用了其它训练工具如SVM或boosting。Prosser等人[25]提出训练一个weak RankSVMs的集合，并随后与一个强力ranker组合。在[63]中，一个structural SVM用于在决策层面合并不同的颜色描述符。在[43]中，Zhang等人为每个训练身份训练了一个特别的SVM，并将每个测试图片映射到一个从其视觉特征推理出的权重向量。Gray和Tao[24]提出了使用AdaBoost算法来选择许多不同类的简单特征，合并为一个相似性函数。</p><h3 id="2-2-Deeply-learned-Systems"><a href="#2-2-Deeply-learned-Systems" class="headerlink" title="2.2 Deeply-learned Systems"></a>2.2 Deeply-learned Systems</h3><p>自从Krizhevsky等人的Alexnet[14]在ILSVRC’12上取得了压倒性胜利后，基于CNN的深度学习模型就非常流行。前两个使用了深度学习的re-ID论文是[15, 16]。通常来说，社区常用的CNN模型有两类。第一类是用于图片分类[14]和物体检测[64]的分类模型。另一种是使用两张[65]或三张[66]的孪生siamese模型。re-ID中的深度学习主要瓶颈是缺乏训练数据。大部分re-ID数据集仅为每个身份提供两张图片，如VIPeR[24]，故当前大部分基于CNN的re-ID方法集中于孪生模型。在[15]中，一个输入图片被分割为3个有重叠的水平部分，各部分会穿过两个卷积层和一个全连接层，将其混合并为该图片输出一个向量。两个输出向量的相似度通过余弦距离计算。由Li等人[16]设计的架构不同之处在于增加了一个patch匹配层，将两个图片不同水平条的卷积结果相乘，类似ACS[30]的思想。后来，Ahmed等人[69]通过计算cross-input neighborhood difference特征提升了孪生模型，将一张输入图片的特征与其余图片的邻居位置特征做比较。[16]使用乘积来计算相似维度的patch相似度，而Ahmad的[69]使用减法。Wu等人[70]使用更小的卷积核加深了网络，叫做”PersonNet”。在[71]中，Varior等人将长短期记忆网络LSTM吸收进了孪生网络。LSTM有序地处理图片块，从而记忆下空间联系以增强深度特征的辨识力。Varior等人[72]提出在每个卷积层后插入一个门函数gating function，在一对测试图片输入网络时捕捉有效的微妙的subtle模式。这一方法在多个benchmark获得了前沿准确率，但有多个缺点。查询在输入到网络前需要与图片集中所有图片组队——在大数据集就非常低效。类似[72]，Liu等人[73]提出在孪生模型中集成一个基于弱注意力soft attention的模型，来适应性地关注输入图片对的重要部分；但这一方法仍受限于计算的低效。这些网络都使用两张图片作为输入，Cheng等人[74]设计了一个三元loss函数，接收三张图片作为输入。在首个卷积层后，每张图片划分出4个有重叠的身体部分，并在FC层合并为一个。Su等人[75]提出了一个3步训练流程，包含使用独立数据集进行属性预测和一个在有ID标签的数据集训练的三元loss函数。</p><p>孪生模型的一个缺点是它没有完全利用re-ID标注。事实上它仅需要成对（成三）的标签。在re-ID中，仅用来判断一对图片是否类似的标签是弱标签。另一个潜在的有效策略包括使用一个分类/identification模式，能完全利用re-ID标签。在[76]中，来自多个数据集的训练身份合并为训练集，并在分类网络使用一个softmax loss。为每个FC单元提出的影响impact分与基于影响分的领域指导domain guided的dropout一起，训练得到了有竞争力的re-ID准确率。在更大的数据集上，如PRW和MARS，分类模型在没有精心选择训练样本的情况下就获得了不错的性能[21, 77]。但identification loss的应用仍需要每个身份更多的样本来使模型收敛。作为比较，本调查为每类模型提供了一些基线结果。在表2中，我们在Market-1501数据集[44]上实现了identification和验证模型。所有网络都使用了默认参数设定，并从ImageNet预训练的模型进行调优。图片在输入到网络前缩放到了$224\times224$。初始学习率设置为0.001，并在每个epoch后降低10倍。训练在36个epoch后完成。我们能清晰地观察到identification超越了验证模型，ResNet-50模型[68]在数据集上与近期结果[71, 72, 75]相比获得了前沿re-ID精确度。</p><p><img src="/2018/08/29/Person-Re-identification-Past-Present-and-Future/./1534987637891.png" alt="表2"></p><p>上述论文以端到端的形式训练深度特征，有些可以选择低级特征作为输入。在[79]中，为每个图片使用低维描述符，包括SIFT和颜色直方图，集成为一个single Fisher Vector[80]。混合网络在Fisher向量上建立了全连接层，并以linear discriminative analysis（LDA）用作目标函数，来产生有低类内（intra-class）方差和高inter-class方差的组合。Wu等人[81]提出将FC特征与低维特征向量连接，后跟一个在softmax loss层前的另一个FC层。这一方法强迫FC特征使用手工特征。</p><h3 id="2-3-Datasets-and-Evaluation"><a href="#2-3-Datasets-and-Evaluation" class="headerlink" title="2.3 Datasets and Evaluation"></a>2.3 Datasets and Evaluation</h3><h4 id="2-3-1-Datasets"><a href="#2-3-1-Datasets" class="headerlink" title="2.3.1 Datasets"></a>2.3.1 Datasets</h4><p>许多基于图片的re-ID数据集已面世，一些常用数据集在表3中进行了总结。最常测试的benchmark是VIPeR。它包含了632个身份，每个身份有两张图片。10个随机train/test划分用于获得稳定性能，每个划分的训练和测试集中都有316个不同身份。这些数据集反应了不同的场景。比如，GRID[84]在地下车站收集的，iLIDS[83]在机场到达大厅捕捉的，CUHK01[88]，CUHK02[89]，CUHK03[16]和Market-1501[44]在大学校园中收集的。近年来多个方面都取得了进展。</p><p><img src="/2018/08/29/Person-Re-identification-Past-Present-and-Future/./1534992372656.png" alt="表3"></p><p>首先数据集规模在增长。非常多数据集的大小都相对小，特别是早期的那些，但近期的数据集，如CUHK03和Market-1501要大一些。都有超过1000个身份和超过10000个限位框，且他们都为训练深度学习模型提供了足够的数据。这即是说，我们需要承认当前数据量仍愿意不够。社区非常需要更大的数据集。</p><p>其次，限位框倾向于使用行人检测器来生成限位框（如DPM[91]和ACF[92]），而不是手绘。实际应用中，使用人力来为图集绘制限位框是不可行的，因此必须使用检测器。这也许会使限位框偏离理想的那个。据[16]显示使用检测出的限位框相比手绘的，由于检测器的误差如未对准，通常会降低re-ID准确率。在[44]，许多错误检测结果（在背景上）被引入到图集中，这是使用检测器不可避免的。[44]的实验显示随着图集中引入的错误选择distracotrs更多，re-ID准确率持续下降。因此，社区去研究有实际缺点的数据集，如错误检测和未对准，会有好处。</p><p>第三，在收集中使用了更多的摄像头。比如，Market-1501的每个身份能被最多6个摄像头捕捉到。这一设计需要度量训练方法有较好的泛化能力，而不是小心的在确定的两摄像头对间调优。实际上，在城市级的有n个摄像头网络的网络，摄像头对的数量为$C_n^2$，因此为每个摄像头收集标记数据并训练$C_n^2$个距离度量是不可能的。</p><p>如需更多相关数据集的描述，我们推荐[5]和<a href="http://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/projectpages/reiddataset.html" rel="external nofollow noopener noreferrer" target="_blank">网站</a></p><h4 id="2-3-2-Evaluation-Metrics"><a href="#2-3-2-Evaluation-Metrics" class="headerlink" title="2.3.2 Evaluation Metrics"></a>2.3.2 Evaluation Metrics</h4><p>在评估re-ID算法时，通常使用累积匹配特征cumulative matching characteristics (CMC)曲线。CMC表达了一个查询身份在不同大小的候选人列表中出现的概率。不管图集中有多少gt匹配，CMC计算中只计第一个匹配。故基本上当每个查询仅有一个gt时，CMC与评估方法一样精确。这一度量是可以接受的，实际中人们更关心返回的rank list的最佳匹配gt。</p><p>但为了研究完整性，对于图集中存在多个gt时。Zheng等人[44]提出使用mAP来评估。动机是完美的re-ID系统应该为用户返回所有真匹配。有一个情况是两个系统在找出第一个gt上平分秋色，但在检索召回率上不同。这一场景下CMC的辨识力不够但mAP可以。因此在Market-1501上mAP与CMC一起使用，该数据集每个查询在多个摄像头有多个gt。后来在[71, 72, 93]中也报告在有多个gt的数据集使用了mAP。</p><h4 id="2-3-3-Re-ID-Accuracy-Over-the-Years"><a href="#2-3-3-Re-ID-Accuracy-Over-the-Years" class="headerlink" title="2.3.3 Re-ID Accuracy Over the Years"></a>2.3.3 Re-ID Accuracy Over the Years</h4><p>在本节，我们总结了近年来在多个有代表性的数据集上的re-ID准确率，见图3。我们将当期的方法大致分为手工和深度学习的。对于每个数据集，会选择各年有最高re-ID准确率的方法进行展现。基于这些结果，可以获得三个主要见解。</p><p><img src="/2018/08/29/Person-Re-identification-Past-Present-and-Future/./1534995644290.png" alt="图3"></p><p>首先，可以观察到这些年6个数据集上的性能提升趋势。在VIPeR，CUHK01，i-LIDS，PRID 450，CUHK03和Market-1501分别观测到了51.9%，56.7%，35.0%，42.6%，57.2%和31.62%的性能提升。比如，在被研究得最多的数据集VIPeR[82]，从2008到2016，有代表性的论文[13, 24, 30, 32, 85, 94, 95]见证了rank-1准确率从2008年[24]的12.0%到2015年[94]的63.9%，高达51.9%的提升。对于2015年公布的Market-1501数据集，前沿结果从44.42%[44]提升到了76.04%[72]，达31.62%的提升。</p><p>其次，除了VIPeR的例外，深度学习在其余5个数据集获得了前沿的结果，均超越了手工系统的性能。在CUHK03和Market-1501上，迄今最大的两个数据集，我们观察到了深度学习[72, 76]压倒性的优势。因为VIPeR相对小，深度学习的优势无法完全发挥；在这设置下，手工度量训练更有优势。考虑到在图片分类和物体检测的例子，很有可能深度学习系统会在接下来几年继续统治re-ID社区。</p><p>第三，我们推测现在还有提升空间，特别是当更大的数据集公布后。比如在Market-1501数据集，尽管没有使用多个查询的最佳rank-1准确度达到了65.88%[72]，mAP却很低（39.55%）。这说明尽管从6个摄像头池中找到第一个真匹配相对简单（rank-1准确率），找出困难正样本仍很重要，它能获得更高的召回率（mAP）。另一方面，尽管看起来我们已能在这些数据集获得60-70%的rank-1准确率，我们仍需记住这些数据集只能覆盖实际应用的一小部分。实际上，除了[44]以外，[96]也报道了10倍的图集大小增长导致了10倍的rank-1准确度下降，就算最佳的模型也只有个位数得分。因此，考虑到低mAP（re-ID召回率）和现有数据集的小尺寸，我们对基于图片的re-ID的重要突破非常乐观。</p><h2 id="3-Video-Based-Person-Re-ID"><a href="#3-Video-Based-Person-Re-ID" class="headerlink" title="3. Video-Based Person Re-ID"></a>3. Video-Based Person Re-ID</h2><p>在文献中，person re-ID主要以单图片的方式进行探索。近年来，因为包含了更多研究可能的数据丰富程度增长，基于视频的re-ID开始流行起来。它与基于图片的re-ID的等式1有相似的方程。基于视频的re-ID用两个限位框集$\{q_i\}_{i=1} ^{n_q}$和$\{g_i\}_{j=1} ^{n_g}$替换了图片q和g，其中$n_q$和$n_g$是各视频序列的限位框数量。与限位框特征一样重要，基于视频的方法注意力放在多帧匹配方案和时间信息的集成。</p><h3 id="3-1-Hand-crafted-Systems"><a href="#3-1-Hand-crafted-Systems" class="headerlink" title="3.1 Hand-crafted Systems"></a>3.1 Hand-crafted Systems</h3><p>2010的前两个尝试[12, 13]都是手工系统。它们主要使用了基于颜色的描述符并有选择的使用前景分割来检测行人。他们使用了与基于图片的re-ID方法的类似图片特征，主要差别是匹配函数。如章节1.2所提到的，两个方法都计算两个限位框集的特征的最小欧氏距离作为集的相似度。本质上这种方法应被分类到多帧“multi-shot” person re-ID中，其两个帧集合的相似性扮演了关键角色。这一多帧匹配策略被后来的研究[97, 98]所采用。在[86]中，多帧被用于训练一个基于协方差特征集的有辨识力的boosting模型。在[99]中，SURF局部特征被用于检测和描述兴趣点，接着在KD树中建立索引以加速匹配。在[11]中生成了一个时空图来为前景分割辨识时空稳定区域。接着用聚类方法对时间段内计算出局部描述，以提升匹配性能。Cong等人[100]使用来自视频序列的复写地理结构manifold geometric structure来构建基于颜色特征的更紧凑的的空间描述符。Karaman等人[101]提出使用条件随机野conditional random field（CRF）来体现时空领域的约束。在[102]中使用了颜色和选出的图片来构建了对帧的模型，它捕捉其有特点的外表以及随时间变化的变化。Karanam等人[103]使用了一个人的多帧并提出probe特征，表现为同一个人在图集中的线性组合。一个身份的多帧也能被用来增强身体部分的校准。在[85]中寻找部分与部分的精确一致的成果，Cheng等人提出了一个迭代算法，由于身体部分检测器的提升，每次迭代后绘画结构拟合得就越精确。在[104]中，行人姿态被加入估计，有着相同姿态的帧会匹配出更高的信心分。</p><p>上述方法通常基于多帧建立外观模型，而近来的趋势是模型包含时间线索。Wang等人[105]提出使用时空描述符来re-identify行人。它的特征包括HOG3D[106]和不太能量图gait energy image (GEI)[107]。通过设计一个流能量侧写flow energy profile (FEP)可以检测出步行周期waling cycles，因此在局部最小/最大附近的帧就被用于提取动作特征。最终，可靠的时空特征被选出并通过一个有辨识力的视频排名模型进行匹配。在[108]，Liu等人提出将一个视频序列分解为一系列表达与必然动作相等的身体动作单元，并从中提取Fisher向量作为该人的最终表达。Gao等人[109]使用行人的周期periodicity属性并将步行周期分割为由时间对齐池化temporally aligned pooling描述的多段。在[110]中提出了一个新的基于密集计算的多向梯度的时空描述符和抛弃短期的噪音动作。</p><p>在进行视频匹配时，距离度量训练同样重要。在[111]中提出了一个验证方法集，采用了transfer ranking来确定查询是否匹配属于同一个人的图片中的一张。在[89]所提出的局部匹配模型的多帧扩展，最小化最佳匹配的距离并缩减跨视野变换数量。在[112]中，Zhu等人提出同时训练intra和inter-video距离度量，使视频表达更紧凑，更能辨识不同身份的视频。You等人[113]提出了top-push距离训练方法，通过选择有辨识力的特征优化了视频re-ID的top-rank匹配。</p><h3 id="3-2-Deeply-learned-Systems"><a href="#3-2-Deeply-learned-Systems" class="headerlink" title="3.2 Deeply-learned Systems"></a>3.2 Deeply-learned Systems</h3><p>在基于视频的re-ID，数据量通常比基于图片的数据集大，因为每个tracklet包含许多帧（表4）</p><p><img src="/2018/08/29/Person-Re-identification-Past-Present-and-Future/./1535007776910.png" alt="表4"></p><p>两种方法的基本区别是基于视频的每个匹配单元有多帧图片，多匹配策略或视频池化后的单匹配策略都是可行的。多匹配策略用于较早的研究[12, 13]，有着较大的计算消耗且在大数据集可能有问题。另一方面，基于池化的方法将帧级别的特征聚集到一个global向量，有着更好的伸缩性。因此当前基于视频的re-ID方法通常采用池化步骤。该步可以是[21, 114]那样的极大/平均池化，或[115]那样通过FC层进行训练。在Zheng等人的系统[21]并没有显式地捕捉时间信息；一个身份的帧集被看做训练一个有softmax loss的分类CNN模型的样本。通过极大值池化聚集的帧特征能在三个数据集上产生不错的准确率。这些方法被证明是有效的，不过仍有许多提升空间。re-ID社区可以从动作/事件识别社区借鉴点子。比如，Xu等人[116]提出将CaffeNet的第5个卷积层列特征聚集到FIsher向量[80]或VLAD[117]，通过直接的CNN特征转换。Fernando等人[118]提出了一个learning-to-rank模型，来捕捉一个视频中帧特征是如何随时间演化的，得到了视频级的时间动态temporal dynamics视频描述符。Wang等人[119]将一个多级编码层multi-level encoding layer嵌入了CNN模型，得到了视频长度范围极大的视频描述符。</p><p>另一个好方法是在最终表达中注入时间信息。在手工系统中，Wang等人[105]和Liu等人[108]在iLIDS-VID和PRID-2011数据集上使用纯时空信息，并得到了有竞争力的准确率。但[21]显示在MARS数据集上时空特征不够有辨识力，因为许多行人在同一摄像头下有相似的行走waling姿态，而且同一行人在不同摄像头下的动作特征会有很大区别。他们的观点是在大规模视频re-ID系统中，外观特征是关键。即便如此，本调查还是呼吁关注近期研究[114, 115, 120]，其中外观特征（如CNN，颜色和LBP）被用作输入RNN的起点以捕获帧之间的时间流动。在[114]，用CNN模型从连续视频帧提取特征，接着输入到一个循环recurrent最终层，使时间步间的信息流被允许。接下来特征通过极大或平均池化层合并，得到视频的外观特征。所有这些结构都合并到一个孪生网络中。[120]也使用了类似的架构。它们之间有两重区别，首先[120]使用了一种特殊的RNN，Gated Recurrent Unit (GRU)；其次，[114]使用了一个身份loss，有助于loss收敛和性能提升。尽管[114, 120]使用孪生网络来计算loss，Yan等人[115]和Zheng等人[21]使用identification模型来将每个输入视频分类到对应身份。在[115]，如颜色、LBP等手工低级特征输入到多个LSTM，LSTM的输出连接到softmax层。在动作识别中，Wu等人[121]提出从一个视频同时提取外观和时空特征，并构建一个混合网络来融合两类特征。在本调查中，我们注意到外观和时空模型的有辨识力地合并是未来视频re-ID研究的一个有效解决方案。</p><h3 id="3-3-Datasets-and-Evaluation"><a href="#3-3-Datasets-and-Evaluation" class="headerlink" title="3.3 Datasets and Evaluation"></a>3.3 Datasets and Evaluation</h3><p>现存的一些多帧re-ID数据集，如ETH[122]，3DPES[123]，PRID-2011[86]，iLIDS-VID[105]和MARS[21]，其统计见前文的表4。ETH数据集使用单个可动摄像头。它包含三个序列，每个序列提供多张图片。该数据集较简单，多帧场景下的re-ID准确率接近100%。3DPeS数据集使用8个无重叠户外摄像头采集。尽管公开的视频，但该数据集多用于单帧re-ID。PRID-2011和iLIDS-VID比较相似，它们都通过两个摄像头采集，每个身份有两个视频序列。iLIDS-VID有300个室内场景的身份。PRID-2011的两个户外摄像头分别有385和749个身份，其中有200个身份在两个摄像头下都观察到了。[105]提议在测试中使用其中178个身份。通常认为iLIDS-VID因其室内环境比PRID-2011更具挑战。MARS数据集是近期公开的大规模视频re-ID数据集，在超过20000个视频序列中有1261个身份。它通过DPM检测器和GMMCP跟踪器[125]生成的。因为它发布不久，我们无法提供在其上的结果的完整总结。图4展现了在三个代表性视频（多帧）re-ID数据集上的前沿结果。可以得到三个主要结论：</p><p><img src="/2018/08/29/Person-Re-identification-Past-Present-and-Future/./1535011091444.png" alt="图4"></p><p>首先，ETHZ数据集已达到性能饱和。在2015年，Lisanti等人[124]和Martinel等人[126]报告了接近100%的rank-1准确率。在[124]，每个序列使用5张图片，ETHZ序列1，2，3的rank-1准确率分别为99.8%, 99.7%和99.9%。使用10张图片的结果更高，得到了100%。主要原因是该数据集的身份相对少，而因为只使用了一个可移动摄像头，图片方差小。这是第一个完成了其初始目标的re-ID数据集。</p><p>其次，在iLIDS-VID和PRID-2011数据集上仍在进行活跃的视频re-ID研究。我们清晰地观察到了它们的rank-1准确率提升。</p><p>第三，深度学习方法在视频re-ID取得了统治级的领先。各个数据集的领先成绩都来自深度网络。与基于图片的re-ID相比，基于视频的数据集明显大得多。MARS提供了超过500k训练帧，而Market-1501只有13k。有了这些训练数据，不光是基于视频的，基于图片的网络也能训练。不过虽然MARS的rank-1准确率达到68.3%，但mAP仍相对低（49.3%），而在评估每个摄像机对时，性能进一步下降。因此我们相信基于视频的re-ID仍有较大提升空间。</p><h2 id="4-Future-Detection-Tracking-and-Person-Re-ID"><a href="#4-Future-Detection-Tracking-and-Person-Re-ID" class="headerlink" title="4. Future: Detection, Tracking and Person Re-ID"></a>4. Future: Detection, Tracking and Person Re-ID</h2><h3 id="4-1-Previous-Works"><a href="#4-1-Previous-Works" class="headerlink" title="4.1 Previous Works"></a>4.1 Previous Works</h3><p>尽管person re-ID始自多摄像头最终，但它现在已是一个独立的研究主题。在本调查中，我们将re-ID看做未来重要方向，会与行人检测和追踪合并为一个场景，但是一个更独立的角色。特别地，我们考虑一个端到端（在这里指从原始视频查询一个人）的系统，以原始视频作为输入，并集成行人检测与跟踪，以及re-id。</p><p>直到最近，大部分re-ID研究都基于两个假设：首先，需要行人限位框图集；其次，限位框都是手工完美画出的。但实际应用中这两个假设不能成立。一方面，图集大小因检测器阈值不同而变化很大。较低的阈值会产生更多检测框（更大的图集，更高的召回率，较低的精确度）。在检测的召回/精确度随着阈值变化的情况下，re-ID准确率不能保持稳定。另一方面，当使用行人检测器时，限位框常常存在检测误差，例如没对齐，漏检和误报。此外，当使用行人追踪时，追踪误差也许会导致一个tracklet中的异常outlier帧，例如，背景和行人有不同的身份。因此行人检测与追踪对re-ID的准确率有直接影响，这在社区中很少被讨论。接下来我们会回顾这个方向的几个研究。</p><p>最初的解决第二个问题的尝试中，多个数据集被引入，如CUHK03，Market-1501和MARS。这些数据集不保证完美检测/追踪输出，与实际应用更近了一步。例如，Li等人[16]在CUHK03上发现使用检测出的限位框的re-ID准确率比使用手工的低。后续研究也报告了这一观察[42, 127]。这些发现与实际应用非常相关。在MARS，追踪误差（图8）与检测误差都呈现出来了，但追踪误差对re-ID准确率的影响仍未知。</p><p><img src="/2018/08/29/Person-Re-identification-Past-Present-and-Future/./1535177028113.png" alt="图8"></p><p>除开数据集在引入检测/追踪误差的进展，它们并没有精确评估对re-ID的影响，这对于在端到端re-ID系统中从非常多现存检测/追踪方法选择非常重要。据我们所知，第一个端到端person re-ID研究是由Xu等人[18]2014年提出的。他们使用名词”commonness”来描述一个图片限位框与行人相似程度，“uniqueness”来指代限位框图集与查询间的相似度。他们用一个指数函数融合了commonness与uniqueness。这一方法消除了误背景检测。尽管它们考虑了检测对re-ID的影响，但缺乏完整的跑分和对图集的动态问题的考虑。</p><p><img src="/2018/08/29/Person-Re-identification-Past-Present-and-Future/./1535082836117.png" alt="图5"></p><p>在2016，Xiao等人[128]和Zheng等人[77]同时提出了基于大规模数据集的端到端re-ID系统。这两个研究以原始视频帧和一个限位框查询作为输入（见图5）。行人检测器在原始帧上进行检测，输出的限位框构成了re-ID图集，接着使用经典的re-ID。这一流程在[18, 128]中称为“行人搜索 person search”，不再限制于re-ID（图5b）：它对检测模块（图5a）给与了同样的注意力。这一流程非常重要的一方面是，给定同样的re-ID特征集，更好的检测器能产出更高的re-ID准确率。在[77, 128]对野外行人re-ID（PRW）和大规模行人搜索（LSPS）数据集实现了许多基线。另一个有趣主题是行人检测能否帮助re-ID。在[18, 77]中，检测信心集成到了最终re-ID得分中。在[128]，在一个Faster R-CNN模型中同时考虑了行人检测和re-ID，而[77]中在调优一个预训练的用于行人检测的R-CNN模型时，ID-discriminative embedding IDE显得更优秀。这些方法提供了一点弱标注检测数据对于提升re-ID准确率的帮助的洞察力。</p><p>然而，在“端到端”系统[18, 77, 128]中，没有提到行人追踪，也没有任何已知研究、数据集解决re-ID中的追踪的影响。我们认为这是集成检测、追踪和检索到一个框架的终极目标，并度量各模块对整体re-ID性能的影响。我们呼吁大规模数据集提供用于这三个任务的限位框标注。</p><h3 id="4-2-Future-Issues"><a href="#4-2-Future-Issues" class="headerlink" title="4.2 Future Issues"></a>4.2 Future Issues</h3><h4 id="4-2-1-System-Performance-Evaluation"><a href="#4-2-1-System-Performance-Evaluation" class="headerlink" title="4.2.1 System Performance Evaluation"></a>4.2.1 System Performance Evaluation</h4><p>合适的评价方法论是关键，有时还是棘手的问题。通常来说不存在一个“正确”协议，特别是对于正在开发的端到端re-ID任务。一个端到端的re-ID系统从当前对基于特定检测器/追踪器和它们参数的动态图集的研究开始。此外，对于person re-ID场景评估检查/追踪性能的研究近乎空白。因此本调查提出了两个方面的问题。</p><p>首先，为re-ID中的行人检测和追踪使用有效评估是很关键的。评估方法应以现实且无偏差的形式，利用re-ID准确率量化并排名检测器和追踪器性能。比如，行人检测主要使用log-average miss rate (MR)，对精度范围为$[10^{-2},10^0]$的FPPI (false positive per image)进行平均。也有使用平均精度（AP）的。Dollar等人[135]认为在特定任务使用MR比FPPI好（miss rate against FPPI is preferred），如自动驾驶中其可接受的FPPI有上限。与自动驾驶应用相反，person re-ID目标是找到一个人，无需关心false positive率。故我们能同时使用MR和AP来度量用于person re-ID的行人检测。</p><p>AP/MR计算中的重要参数是IoU。如果与gt框的IoU大于阈值，被检查的限位框就被认为是正确的。通常阈值为0.5。KITTI benchmark对于车辆检测要求0.7的IoU，而对行人是0.5。需要注意使用更大的阈值比更小的性能会好。图6提供了在PRW数据集上检测准确率AP与re-ID准确率（rank-1或mAP）之间的关系。显然它们在IoU阈值为0.7时是线性关系。而0.5的IoU下比较离散。这一相关性说明应使用更大的IoU。</p><p><img src="/2018/08/29/Person-Re-identification-Past-Present-and-Future/./1535106560787.png" alt="图6"></p><p>考虑到限位框定位质量对re-ID准确率的重要性，在评估检测器质量时研究IoU阈值，并查看是否与re-ID准确率相关是一个好主意。我们的直觉是更大的IoU限制能强迫得到更好的定位结果，但这有一些限制，因为在IoU变大时，检测器性能差距趋向于消失[136]。[138]提出使用AR（average recall），并为不同的检测器阈值画出AR。这一评价方式同时考虑了召回率和定位度。</p><p>尽管有一些指导行人检测评估的方法，但对于追踪时完全未知的。在多物体追踪multiple object tracking MOT benchmark[139]上有许多评估方法，包括多物体追踪精度MOTP[140]，mostly track(MT) 目标（gt行人轨迹被追踪结果覆盖80%以上的占比），false positive FP的总数量，ID switch IDS的总数量，踪迹断开Frag的次数，每秒处理的帧数 Hz等。也许有些指标指示能力有限，如处理速度，因为追踪是离线步骤。对于re-ID，因为猜想追踪精度也是关键，因为它不希望在tracklet中有异常值，那会让池化效果打折扣。我们也猜想在re-ID下MT的80%不是最优阈值。如[105]建议的，在一个行走周期中提取特征是一个好的实践，因此生成长的追踪序列可能不会带来大幅re-ID提升。在未来，一旦有用于评估追踪和re-ID的数据集，刻不容缓的问题就是为不同追踪器设计合适的度量。</p><p>第二个问题是评价方法对整个系统的re-ID的关系。与传统的，图集是固定的re-ID不同，在端到端的re-ID系统中，图集随检测/追踪阈值变换而差异很大。更严的阈值象征着更高的检测、追踪信心，故图集会更小，背景检测会更少。不仅如此，图集大小对re-ID准确率有直接影响。我们考虑一个极端情况。当检测、追踪阈值非常严格，图集会非常小，甚至有可能gt匹配都没包括进去。另一个极端下，当阈值非常松散，图集会非常大，包含许多背景检测，会对re-ID有负面影响。故可以预计的是过严过松的阈值都不好，需要re-ID评价协议反应出re-ID随图集变化的情况。在[77]，Zheng等人画出了rank-1准确率和mAP对每张图的检测数量曲线。可以观察到曲线先上升，达到极值后下降。在PRW数据集上，极值出现在每张图4-5个检测时，可以看做每张图片行人数量的均值。在[128]使用了类似的流程rank-1匹配率与检测召回率的曲线被画出，在召回率为70%时达到最大值。当召回率进一步增大时，过多的错误检测会降低re-ID准确率。还有其他的想法，如画出re-ID准确率对FPPI的曲线。需要注意图集大小与检测器阈值相关。</p><p>我们也指出了另一个端到端系统中的re-ID评估方法。实际上在查询一个限位框/视频序列时，虽然检测、追踪系统直接定位到特定帧并告知坐标最好，但系统只知道哪些帧这个身份出现过也是可以的。其具体位置可以通过人力高效得出。所以，找到查询人物在哪些帧出现过比起“检测/追踪+re-ID”任务要相对简单，因为检测追踪误差影响不会太大。在这一场景下re-ID的准确率应比标准re-ID高。同样可以使用mAP对检索出的视频帧使用。因为这一任务不要求精确定位人，我们可以放松检测器或追踪器，目标放在提升帧级别的召回率上。它们可以被训练为使用宽松IoU阈值，粗略定位一个行人区域，将重心放在匹配上。</p><h4 id="4-2-2-The-Influence-of-Detector-Tracker-on-Re-ID"><a href="#4-2-2-The-Influence-of-Detector-Tracker-on-Re-ID" class="headerlink" title="4.2.2 The Influence of Detector/Tracker on Re-ID"></a>4.2.2 The Influence of Detector/Tracker on Re-ID</h4><p>Person re-ID始自行人追踪[9]，如果认为它们是一个身份，来自多个摄像头的tracklet就会关联起来。这项研究将re-ID当做追踪系统的一部分，并没有评估定位/追踪准确率对re-ID准确率的影响。但是，就算re-ID独立以来，许多研究在手绘图片限位框上进行，而这是离现实很远的理想情况。因此，在端到端re-ID系统中，理解检测/追踪对re-id的影响很关键。</p><p>首先，行人检测追踪误差确实会影响re-ID准确率，但其本质和可行解决方案仍未决。检测误差（图7）会导致行人未对齐，大小变化，部分缺失和最重要的FP和楼检测，都影响了re-ID性能，为社区提出了新的挑战[16, 44, 96]。</p><p><img src="/2018/08/29/Person-Re-identification-Past-Present-and-Future/./1535177004460.png" alt="图7"></p><p>少数re-ID研究显式地把检测追踪误差考虑了进去。在[[29]中，Zheng等人提出融合local-local和global-local匹配来解决严重遮挡或部分缺失的问题。在[18]中，Xu等人通过GMM编码描述符与先验分布的匹配计算一个“commonness”分。这一得分能用来消除不含行人的FP，或为人体提供不错的定位。Zheng等人[77]以类似的方式把检测信心分（平方根）集成到了re-ID相似度得分。这些研究在检测误差出现后处理它们。但有可能在更早的阶段避免它们。例如在Xiao等人[128]设计的网络中，Faster R-CNN子模块中加入了定位误差。它校准regulate了对re-ID系统很关键的定位质量。</p><p>要揭示re-ID对检测追踪质量的依赖，还需更多研究。开发一个无误差的检测器、追踪器过于理想化，我们提倡研究如何将检测信心分集成到re-ID匹配分中，如怎么通过有效辨识异常值来纠正错误，以及怎样训练一个不单独依赖检测限位框的上下文模型。比如使用聚类算法来筛选出tracklet中不一致的帧，这对净化追踪序列很有效。另一个例子，可以扩大检测到的限位框来包含也许遗漏的身体部分，并训练反过来使用或丢弃强化的上下文信息的视觉特征。</p><p>其次，我们应知晓如果检测和追踪设计得当是能帮助re-ID的。在[77]，在R-CNN模型上调优的IDE网络被证明比直接在ImageNet上预训练的模型调优有效。这说明使用行人检测的大量标注数据的重要性。在[128]中集成了背景检测loss到端到端网络中。[18， 77]中将检测得分集成到re-ID相似度中，能看做检测帮助re-ID的一种方法。</p><p>也许行人检测/追踪帮助re-ID或反过来看起来不那么直接，但如果我们类比泛化图片分类和细粒度分类，我们也许会有一些线索。在细粒度数据集上调优ImageNet预训练的CNN模型有助于更快收敛及更高的细粒度检测准确率。通过反向传播RCCN中的re-ID loss来同时训练行人检测和re-ID模型也是个好主意。不同身份的有效区分也许能受益于从背景区分出行人的任务。反过来也是一样。</p><p>使用无监督追踪数据的想法值得深究。尽管追踪错误不可避免，在视频中追踪行人不是太难的任务。人脸识别、颜色和非背景信息都是提升追踪性能的有用工具，就像哈利波特的活点地图（marauder’s map）[142]。在一个追踪序列中，一个人的出现也许会经历一定程度的变化，但可以期望大部分限位框都是同一个人。在这一场景中，每个tracklet都代表一个人，它包含了一定数量的噪音但是基本可用的训练样本。因此我们能使用追踪结果来训练行人验证/id模型，从而减轻对大规模有监督数据的依赖。另一个有希望的想法是，用来自auto-encoder[143]或对抗生产网络generative adversarial nets(GAN)[144]的检测/追踪数据来预训练CNN模型。直接使用这种无监督网络训练行人描述符来解决person re-ID中的数据问题也很有趣。</p><h2 id="5-Future-Person-Re-ID-in-Very-Large-Galleries"><a href="#5-Future-Person-Re-ID-in-Very-Large-Galleries" class="headerlink" title="5. Future: Person Re-ID in Very Large Galleries"></a>5. Future: Person Re-ID in Very Large Galleries</h2><p>近年来re-ID社区的数据规模大幅增加，从数百的图集如VIPeR何iLIDS到超过100k的PRW和LSPS，这导致了深度学习方法的统治。不过显然目前的数据集离实际规模还差很远。假如一个区域级的监控网络有100个摄像头，如果行人检测每秒取1帧图片，每一帧输出平均10个限位框，那么这个系统12个小时会生成$3600 \times 12 \times 1 \times 10 \times 100 = 43.2 \times 10^6$个限位框。但目前还没有对如此大的图集的re-ID性能报告。似乎文献中使用的最大图集是500k[44]，而且相比Market-1501 19k图集，mAP下降超过7%。不仅如此，在[44]中使用了近似最近邻搜索[145]来快速检索，但这牺牲了准确率。</p><p>不管是研究还是应用领域，极大图集的person re-ID未来都是一个重要方向。准确率和效率都需要提升。</p><p>一方面，对描述符和距离度量的健壮且大规模训练更重要。这与当前研究[71, 73, 75, 81]一致。追随大规模图片视频[78]，person re-ID会向大规模发展。然而当前的方法在一对或多对摄像头间的极有限时间窗口内解决re-ID问题，摄像头网络的极长时间段的健壮性从未被考虑过。在[36, 146]中，摄像头网络re-ID一致性与逐对匹配准确率同时优化，但测试集（WARD和RAiD）仅有3，4个摄像头，少于100个身份。在有n个摄像头的网络中，摄像头对数量为$\mathbb O(n^2)$。考虑录像时间很长和标注数据的缺失，以逐对的形式训练距离度量或CNN描述符是不可行的。因此，训练一个能适应不同光照条件和摄像头位置的全局re-ID模型是首要任务。对于此目标，一个选择是设计无监督的描述符[44, 97]，找到视觉上相似的人并将不相似的当做不匹配。但无监督的方法易于倾向光照变化。</p><p>另一方面，在大规模的设定下效率是另一个重要问题。尽管在小数据集[82, 83]计算时间几乎可以忽略，我们实验在使用3.1GHz Intel Xeon E5-2687w v3(10核)，64GB内存的服务器上运行MATLAB 2014，它需要8.5秒来计算一个100维浮点向量与1千万个100维向量间的距离。如果我们使用从CaffeNet中提取4096维浮点数向量，时间会增大到60.7秒，其中33秒是用于距离计算，26.8秒用于从磁盘加载数据。很显然由于特征维度和图集大小增长，查询时间大幅增长，不适于实际应用。据我们所知，之前的re-ID研究很少关注效率问题，因此效率解决方案比较缺乏，不过幸运的是我们能求助于图片检索社区，本调查提供了两个方向。</p><p><strong>Inverted index-based.</strong> 倒排索引是基于Bag-of-Words（BoW）的检索方法[22, 147, 148]实际使用的数据结构。基于局部描述符的数字化结果，倒排索引有k个条目，k指密码本大小。索引结构有k个条目，每个都关联一个倒排列表，局部描述符在那里索引。基本倒排索引结构见图9。许多研究使用一个账本posting记录图片ID以及索引的描述符的词频term frequency(TF)，许多其他元数据也能被存储，比如二元签名binary signature[148]，特征坐标[149]等。关于倒排索引在实体检索的基础知识和前沿进展，我们推荐一个近期的调查[19]。</p><p><img src="/2018/08/29/Person-Re-identification-Past-Present-and-Future/./1535355590351.png" alt="图9"></p><p>在person re-ID中流行使用局部描述符[30, 34, 44]。从局部块中提取的通常是颜色和纹理特征。尽管有些之前的研究使用了复杂的匹配算法[30]，最好还是使用能在大图集下使用倒排索引加速的匹配流程。码表通常需要把局部描述符量化为视觉词，而且因为局部描述符是高维的，需要较大的码表来降低量化误差。在这些情况下，可用的倒排索引能很大范围节约内存，而且使用得当的话能与自由量化quantization-free的情况有近似的精度。</p><p><strong>Hashing-based.</strong> 哈希被广泛研究以用于近似最近邻搜索，目标是在图集很大或距离计算花费大时降低准确找到最近邻的成本[23]。自从光谱Spectral Hashing里程碑[150]之后，社区里就开始流行训练hash。它是训练哈希函数，$y=h(x)$将向量x映射到压缩的y，目标是在排名表中找到高排名的真值最近邻true nearest neighbor at high-ranks，同时保持搜索过程的高效。有一些经典哈希训练方法，如乘积量化product quantization(PQ)[117]，递归量化iterative quantization(ITQ)[151]等。这些方法训练都很高效，且有着不错的检索准确率。它们不需要标注数据，故非常适合re-ID任务。</p><p>另一个有监督哈希的应用由图片检索组成[152, 153, 154, 155]，也是本节的兴趣。哈希函数在一个深度网络中端到端的训练，给定一个输入图片，输出一个二元向量。这一行专注于多个图片分类数据集如CIFAR-10[156]和NUS-WIDE[157]，以在缺乏泛化的实体检索数据集[22]的情况下获得训练数据。在person re-ID中，图片检索与深度哈希适配得不错。在大规模图集中，非常需要准确且高效的哈希函数，这也是re-ID中较少探索的方向。如表1中所示，re-ID数据集中有训练类，测试流程也是一个标准的检索任务，所以前沿的有监督哈希已准备好适配到re-ID中。我们能找到的唯一相关论文是[158]，在正则化的triplet-loss CNN中训练哈希函数，来强化相邻一致性。这一方法在每个测试集包含100个身份的CUHK03上测试，所以依然缺乏在极大图集上的性能测试。</p><h2 id="6-Other-Important-Yet-Under-Developed-Open-Issues"><a href="#6-Other-Important-Yet-Under-Developed-Open-Issues" class="headerlink" title="6. Other Important Yet Under-Developed Open Issues"></a>6. Other Important Yet Under-Developed Open Issues</h2><h3 id="6-1-Battle-Against-Data-Volumn"><a href="#6-1-Battle-Against-Data-Volumn" class="headerlink" title="6.1 Battle Against Data Volumn"></a>6.1 Battle Against Data Volumn</h3><p>标注大规模数据集一直是视觉社区的焦点。这一问题在re-ID中更有挑战性，因为不仅需要为一个行人画出限位框，还需要为他赋予一个ID。ID赋予不是不重要的事，因为行人可能会重入视野field of view(FOV)或在首次出现后很久进入另一个摄像头的视野。这让协同标注变得困难，因为两个合作者对标注ID的通信花费极高。这些困难也一部分解释了为何当前数据集通常每个ID只有有限数量的图片。近两年已见证多个大规模数据集，如Market-1501，PRW，LSPS和MARS，但它们还远不能满足实际应用。本调查中，我们相信两个策略能有助于绕过数据问题。</p><p>首先，如何使用来自追踪和检测集的标注，这尚未探索。与re-ID相比，追踪和检测标注当一个行人重入FOV时不需要赋予ID：主要花费在于画出限位框。在[77]中，显示了在R-CNN阶段更多的行人和背景训练数据有助于后续IDE描述符的训练。在[50, 75]中，来自独立数据集的属性标注被用于表达re-ID图片。因为属性可以协同标注，且有着不错的泛化性，所以它们也是re-ID数据缺陷的一个不错选择。</p><p>除了章节4.2.2提到的预训练/无监督策略，一个新解决方案是从无标注数据中检索hard negative，它们在独立训练和CNN训练中可被看做true positive。这一策略已在物体分类中评估过，其中一小部分标签在训练前被干扰[159]。它能有效扩大训练集，同时降低模型过拟合的风险。我们的初步试验显示这一方向相对基线产生了相当好的提升。</p><p>第二个策略是把转换训练，从将一个源领域训练好的模型转换到目标领域。前文提到，监督训练所需的大量标注数据限制了re-ID系统放大到其他摄像头。在[160]中提出了一个无监督主题模型，为re-ID匹配找到显著saliant图片块，并同时移除背景簇。在[161]中提出了一个需要来自其它re-ID数据集完整标注和目标场景少量样本的的弱监督方法。在[162, 163]中提出了无监督转换训练，以解决目标数据集未标注的问题。Ma等人[162]使用了跨域cross-domain排序SVM，而Peng等人[163]把转换问题规划为字典训练任务，它学习共享不变隐藏shared invariant latent参数，并偏向bias目标数据集。这些方法说明从源训练一个不错的re-ID模型是可行的，而且可以从无监督数据挖掘关键线索以获益。转换CNN模型到其他re-ID数据集更加困难，因为深度模型与源拟合得不错。Xiao等人[76]收集了一些源re-ID数据集，并在其上同时训练了一个识别模型。</p><h3 id="6-2-Re-ranking-Re-ID-Results"><a href="#6-2-Re-ranking-Re-ID-Results" class="headerlink" title="6.2 Re-ranking Re-ID Results"></a>6.2 Re-ranking Re-ID Results</h3><p>re-ID过程（图5.b）可被看做一个检索任务，其中re-ranking是提升检索准确率的重要步骤。它是指在能找到re-ranking知识时，为初始排序结果进行重排序。关于re-ranking方法的调查，我们推荐[164]。</p><p>这一课题有少量研究。Re-ranking可以由人进行，也可以完全自动化。当在线人力标注可行时，Liu等人[165]提出了post-rank optimization(POP)方法，允许用户提供一个easy negative和初始排序中的少数hard negative。这一稀疏人力反馈允许对查询人的允许值自动化有辨识力特征的选取。一个提示是Wang等人[96]设计的human verification incremental learning(HVIL)模型，它不需要任何预标注的训练数据并累积地从人类反馈中学习，来提供实体模型更新。当不再拥有人力反馈时许多HVIL模型组合为一个模型。以类似的性质，Martinel等人[166]提出为查询找到最相关的图集图片，将其发送给人类劳动力，最终使用标签来更新re-id模型。有不少论文也在研究自动化re-ranking方法。Zheng等人提出了一个查询-适应query-adaptive 融合方法来结合多个re-id系统的排名结果。特别的是，初始得分曲线形状被使用，有争论说一个好的特征曲线会像L。在[95]中，基于CMC曲线的直觉优化与多种独立同时使用。Garcia等人[94]分析了rank list中的无监督判别式discriminant上下文信息。这被进一步与离线训练的re-ranking度量方法结合。Leng等人[168]使用了倒数reciprocal knn来改善离线步骤中基于初始rank list构建的图片关系。</p><p>虽然它已经在实体检索中广泛研究过，Re-ranking仍是person re-ID的一个开放方向。应用场景可以描绘如下。当查找一个人时，由于强烈的图片变化，很有可能某个摄像头下很难找到。但我们也许能在一些摄像头下找到更近似于hard positive的匹配。以这样的方式，每返回一个简单的，就能找到hard positive。在实体检索中的Re-ranking方法已能用于person re-ID[44, 169, 170, 171]。由于re-ID中具有训练数据（表1），基于训练分布设计Re-ranking方法是可能的。比如，当进行K-NN re-ranking时[170]，返回的结果的正确性可以被训练集依据得分判断。因为re-ID只关注行人，故re-ranking方法也可以特别设计。比如，在获取初始rank后，可以选出top-ranked图片子集，包含它们的视频帧可以被检索到。由于没有太多计算负担，我们随后可以通过昂贵的滑动窗口方法找到最近定位，减轻检测器未对准的影响。</p><h3 id="6-3-Open-World-Person-Re-ID"><a href="#6-3-Open-World-Person-Re-ID" class="headerlink" title="6.3 Open-World Person Re-ID"></a>6.3 Open-World Person Re-ID</h3><p>许多re-ID研究可被看做identification任务（等式1）。查询的身份假定存在于图集中，且任务目标是确定查询的身份。相比之下，开放世界的re-ID系统研究人的验证verification问题。即是，基于identification任务，开放世界问题在等式1中增加了一个条件，</p><script type="math/tex;mode=display">sim (q,g_{i^{\star}}) >h \tag 4</script><p>其中h是阈值，如果超过我们可以认为查询q属于身份$i^{\star}$，否则q被认为不包含在图集中，$i^{\star}$是identification过程中排第一的身份。</p><p>开放世界person re-ID仍在其初级阶段，已提出了一些论文来优化这一任务。在[172]中，Zheng等人设计了一个由多个已知身份的监视清单watch list（图集）和一些试样probe（包括目标和非目标）组成的系统。它们的研究目标是获得高的true target recognition TTR和低的false target recognition(FTR)率，即计算被确认是目标身份的查询与总查询数的比值。在[173]中，Liao等人将开放世界re-ID分割为两个子任务，detection和identification；前者决定一个probe是否在图集中出现，后者为通过的probe赋予ID。因此提出了两个不同的评估方法，detection and identification rate(DIR)和false accept rate（FAR），基于它们可以画出receiver operating characteristic(ROC)曲线。</p><p>开放世界的re-ID仍是一个挑战性的任务，因为如[172, 173]显示的低FAR下的TTR很低。挑战主要存在两个方面，检测和识别，它们都受限于不令人满意的匹配准确率。如[173]指出的，100%的FAR相当于标准close-set re-ID，其准确率受限于前沿水平；伴随更低FAR的更低re-ID准确率是因为真匹配的召回率低。因此，从技术观点看，关键目标是提升匹配准确率，基于面向新检测（验证）方法的概率模型。不仅如此，当集中于re-ID准确率时，开放世界re-ID也应考虑图集的动态性[174]。在动态系统中有着不断引入的限位框，如果一个新的身份被判断不属于任何图集已有身份，它就被加入到watch list，反之亦然。动态的新身份登记允许了自动化数据集构建，用一个预组织的图集帮助re-ID过程。</p><p><img src="/2018/08/29/Person-Re-identification-Past-Present-and-Future/./1535513409052.png" alt="图10"></p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] A. Plantinga, “Things and persons,” The Review of Metaphysics, pp. 493–519, 1961.<br>[2] A. O. Rorty, “The transformations of persons,” Philosophy, vol. 48, no. 185, pp. 261–275, 1973.<br>[3] N. B. Cocchiarella, “Sortals, natural kinds and re-identification,” Logique et analyse, vol. 80, pp. 439–474, 1977.<br>[4] T. D’Orazio and G. Cicirelli, “People re-identification and tracking from multiple cameras: a review,” in 2012 19th IEEE International Conference on Image Processing. IEEE, 2012, pp. 1601–1604.<br>[5] A. Bedagkar-Gala and S. K. Shah, “A survey of approaches and trends in person re-identification,” Image and Vision Computing, vol. 32, no. 4, pp. 270–286, 2014.<br>[6] S. Gong, M. Cristani, S. Yan, and C. C. Loy, Person re-identification. Springer, 2014, vol. 1.<br>[7] R. Satta, “Appearance descriptors for person re-identification: a comprehensive review,” arXiv preprint arXiv:1307.5748, 2013.<br>[8] X. Wang, “Intelligent multi-camera video surveillance: A review,” Pattern recognition letters, vol. 34, no. 1, pp. 3–19, 2013.<br>[9] T. Huang and S. Russell, “Object identification in a bayesian context,” in IJCAI, vol. 97, 1997, pp. 1276–1282.<br>[10] W. Zajdel, Z. Zivkovic, and B. Krose, “Keeping track of humans: Have i seen this person before?” in Proceedings of the 2005 IEEE International Conference on Robotics and Automation. IEEE, 2005, pp. 2081–2086.<br>[11] N. Gheissari, T. B. Sebastian, and R. Hartley, “Person reidentification using spatiotemporal appearance,” in 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06), vol. 2. IEEE, 2006, pp. 1528–1535.<br>[12] L. Bazzani, M. Cristani, A. Perina, M. Farenzena, and V. Murino, “Multiple-shot person re-identification by hpe signature,” in Pattern Recognition (ICPR), 2010 20th International Conference on. IEEE, 2010, pp. 1413–1416.<br>[13] M. Farenzena, L. Bazzani, A. Perina, V. Murino, and M. Cristani, “Person re-identification by symmetry-driven accumulation of local features,” in Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEE, 2010, pp. 2360–2367.<br>[14] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with deep convolutional neural networks,” in Advances in Neural Information Processing Systems, 2012, pp. 1097–1105.<br>[15] D. Yi, Z. Lei, S. Liao, S. Z. Li et al., “Deep metric learning for person re-identification.” in ICPR, vol. 2014, 2014, pp. 34–39.<br>[16] W. Li, R. Zhao, T. Xiao, and X. Wang, “Deepreid: Deep filter pairing neural network for person re-identification,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 152–159.<br>[17] J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y. LeCun, C. Moore, E. Sackinger, and R. Shah, “Signature verification using a siamese ¨ time delay neural network,” International Journal of Pattern Recognition and Artificial Intelligence, vol. 7, no. 04, pp. 669–688, 1993.<br>[18] Y. Xu, B. Ma, R. Huang, and L. Lin, “Person search in a scene by jointly modeling people commonness and person uniqueness,” in Proceedings of the 22nd ACM international conference on Multimedia. ACM, 2014, pp. 937–940.<br>[19] L. Zheng, Y. Yang, and Q. Tian, “Sift meets cnn: A decade survey of instance retrieval,” arXiv preprint arXiv:1608.01807, 2016.<br>[20] S. Liao, Y. Hu, X. Zhu, and S. Z. Li, “Person re-identification by local maximal occurrence representation and metric learning,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 2197–2206.<br>[21] L. Zheng, Z. Bie, Y. Sun, J. Wang, C. Su, S. Wang, and Q. Tian, “Mars: A video benchmark for large-scale person re-identification,” in European Conference on Computer Vision, 2016.<br>[22] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman, “Object retrieval with large vocabularies and fast spatial matching,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2007, pp. 1–8.<br>[23] J. Wang, T. Zhang, J. Song, N. Sebe, and H. T. Shen, “A survey on learning to hash,” arXiv:1606.00185, 2016.<br>[24] D. Gray and H. Tao, “Viewpoint invariant pedestrian recognition with an ensemble of localized features,” in European conference on<br>computer vision. Springer, 2008, pp. 262–275.<br>[25] B. Prosser, W.-S. Zheng, S. Gong, T. Xiang, and Q. Mary, “Person re-identification by support vector ranking.” in BMVC, vol. 2, no. 5, 2010, p. 6.<br>[26] W.-S. Zheng, S. Gong, and T. Xiang, “Reidentification by relative distance comparison,” IEEE transactions on pattern analysis and machine intelligence, vol. 35, no. 3, pp. 653–668, 2013.<br>[27] A. J. Ma, P. C. Yuen, and J. Li, “Domain transfer support vector ranking for person re-identification without target camera label information,” in Proceedings of the IEEE International Conference on Computer Vision, 2013, pp. 3567–3574.<br>[28] A. Mignon and F. Jurie, “Pcca: A new approach for distance learning from sparse pairwise constraints,” in IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2012, pp. 2666–2672.<br>[29] W.-S. Zheng, X. Li, T. Xiang, S. Liao, J. Lai, and S. Gong, “Partial person re-identification,” in Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 4678–4686.<br>[30] R. Zhao, W. Ouyang, and X. Wang, “Unsupervised salience learning for person re-identification,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 3586–3593.<br>[31] Z. Li, S. Chang, F. Liang, T. S. Huang, L. Cao, and J. R. Smith, “Learning locally-adaptive decision functions for person verification,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 3610–3617.<br>[32] D. Chen, Z. Yuan, B. Chen, and N. Zheng, “Similarity learning with spatial constraints for person re-identification,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 1268–1277.<br>[33] R. Zhao, W. Ouyang, and X. Wang, “Person re-identification by salience matching,” in Proceedings of the IEEE International<br>Conference on Computer Vision, 2013, pp. 2528–2535.<br>[34] ——, “Learning mid-level filters for person re-identification,” in Proceedings of the IEEE Conference on Computer Vision and Pattern<br>Recognition, 2014, pp. 144–151.<br>[35] Y. Shen, W. Lin, J. Yan, M. Xu, J. Wu, and J. Wang, “Person re-identification with correspondence structure learning,” in<br>Proceedings of the IEEE International Conference on Computer Vision,2015, pp. 3200–3208.<br>[36] A. Das, A. Chakraborty, and A. K. Roy-Chowdhury, “Consistent re-identification in a camera network,” in European Conference on Computer Vision, 2014, pp. 330–345.<br>[37] X. Zhou, N. Cui, Z. Li, F. Liang, and T. S. Huang, “Hierarchical gaussianization for image classification,” in 2009 IEEE 12th International Conference on Computer Vision. IEEE, 2009, pp.1971–1977.<br>[38] D. Chen, Z. Yuan, G. Hua, N. Zheng, and J. Wang, “Similarity learning on an explicit polynomial kernel feature map for person re-identification,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1565–1573.<br>[39] S. Pedagadi, J. Orwell, S. Velastin, and B. Boghossian, “Local fisher discriminant analysis for pedestrian re-identification,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 3318–3325.<br>[40] X. Liu, M. Song, D. Tao, X. Zhou, C. Chen, and J. Bu, “Semi-supervised coupled dictionary learning for person reidentification,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 3550–3557.<br>[41] Y. Yang, J. Yang, J. Yan, S. Liao, D. Yi, and S. Z. Li, “Salient color names for person re-identification,” in European Conference on Computer Vision. Springer, 2014, pp. 536–551.<br>[42] L. Zhang, T. Xiang, and S. Gong, “Learning a discriminative null space for person re-identification,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.<br>[43] Y. Zhang, B. Li, H. Lu, A. Irie, and X. Ruan, “Sample-specific svm learning for person re-identification.”<br>[44] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian, “Scalable person re-identification: A benchmark,” in Proceedings of the IEEEInternational Conference on Computer Vision, 2015, pp. 1116–1124.<br>[45] J. Van De Weijer, C. Schmid, J. Verbeek, and D. Larlus, “Learning color names for real-world applications,” IEEE Transactions on Image Processing, vol. 18, no. 7, pp. 1512–1523, 2009.<br>[46] T. Matsukawa, T. Okabe, E. Suzuki, and Y. Sato, “Hierarchical gaussian descriptor for person re-identification,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 1363–1372.<br>[47] R. Layne, T. M. Hospedales, S. Gong, and Q. Mary, “Person reidentification by attributes.” in BMVC, vol. 2, no. 3, 2012, p. 8.<br>[48] X. Liu, M. Song, Q. Zhao, D. Tao, C. Chen, and J. Bu, “Attributerestricted latent topic model for person re-identification,” Pattern recognition, vol. 45, no. 12, pp. 4204–4213, 2012.<br>[49] C. Liu, S. Gong, C. C. Loy, and X. Lin, “Person re-identification: What features are important?” in European Conference on Computer Vision Workshops. Springer, 2012, pp. 391–401.<br>[50] C. Su, F. Yang, S. Zhang, Q. Tian, L. S. Davis, and W. Gao, “Multitask learning with low rank attribute embedding for person reidentification,” in Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 3739–3747.<br>[51] Z. Shi, T. M. Hospedales, and T. Xiang, “Transferring a semantic representation for person re-identification and search,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 4184–4193.<br>[52] D. Li, Z. Zhang, X. Chen, H. Ling, and K. Huang, “A richly annotated dataset for pedestrian attribute recognition,” arXiv preprint arXiv:1603.07054, 2016.<br>[53] L. Yang and R. Jin, “Distance metric learning: A comprehensive survey,” Michigan State Universiy, vol. 2, p. 78, 2006.<br>[54] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell, “Distance metric learning with application to clustering with side-information,” Advances in neural information processing systems, vol. 15, pp. 505– 512, 2003.<br>[55] M. Kostinger, M. Hirzer, P. Wohlhart, P. M. Roth, and H. Bischof, ¨ “Large scale metric learning from equivalence constraints,” in IEEE<br>Conference on Computer Vision and Pattern Recognition, 2012, pp. 2288–2295.<br>[56] K. Q. Weinberger, J. Blitzer, and L. K. Saul, “Distance metric learning for large margin nearest neighbor classification,” in Advances in neural information processing systems, 2005, pp. 1473– 1480.<br>[57] J. V. Davis, B. Kulis, P. Jain, S. Sra, and I. S. Dhillon, “Informationtheoretic metric learning,” in Proceedings of the 24th international conference on Machine learning. ACM, 2007, pp. 209–216.<br>[58] M. Hirzer, P. M. Roth, M. Kostinger, and H. Bischof, “Relaxed ¨ pairwise learned metric for person re-identification,” in European<br>Conference on Computer Vision. Springer, 2012, pp. 780–793.<br>[59] S. Liao and S. Z. Li, “Efficient psd constrained asymmetric metric learning for person re-identification,” in Proceedings of the IEEE<br>International Conference on Computer Vision, 2015, pp. 3685–3693.<br>[60] Y. Yang, S. Liao, Z. Lei, and S. Z. Li, “Large scale similarity learning using similar pairs for person verification,” in Thirtieth<br>AAAI Conference on Artificial Intelligence, 2016.<br>[61] B. Scholkopft and K.-R. Mullert, “Fisher discriminant analysis with kernels,” Neural networks for signal processing IX, vol. 1, no. 1, p. 1, 1999.<br>[62] F. Xiong, M. Gou, O. Camps, and M. Sznaier, “Person reidentification using kernel-based metric learning methods,” in European Conference on Computer Vision. Springer, 2014, pp. 1–16.<br>[63] X. Liu, H. Wang, Y. Wu, J. Yang, and M.-H. Yang, “An ensemble color model for human re-identification,” in IEEE Winter Conference on Applications of Computer Vision, 2015, pp. 868–875.<br>[64] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies for accurate object detection and semantic segmentation,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 580–587.<br>[65] F. Radenovic, G. Tolias, and O. Chum, “Cnn image retrieval ´ learns from bow: Unsupervised fine-tuning with hard examples,” arXiv:1604.02426, 2016.<br>[66] F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A unified embedding for face recognition and clustering,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 815–823.<br>[67] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” arXiv:1409.1556, 2014.<br>[68] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.<br>[69] E. Ahmed, M. Jones, and T. K. Marks, “An improved deep learning architecture for person re-identification,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3908–3916.<br>[70] L. Wu, C. Shen, and A. v. d. Hengel, “Personnet: Person reidentification with deep convolutional neural networks,” arXiv preprint arXiv:1601.07255, 2016.<br>[71] R. R. Varior, B. Shuai, J. Lu, D. Xu, and G. Wang, “A siamese long short-term memory architecture for human re-identification,” in European Conference on Computer Vision, 2016.<br>[72] R. R. Varior, M. Haloi, and G. Wang, “Gated siamese convolutional neural network architecture for human re-identification,” in European Conference on Computer Vision, 2016.<br>[73] H. Liu, J. Feng, M. Qi, J. Jiang, and S. Yan, “End-to-end comparative attention networks for person re-identification,” arXiv preprint arXiv:1606.04404, 2016.<br>[74] D. Cheng, Y. Gong, S. Zhou, J. Wang, and N. Zheng, “Person re-identification by multi-channel parts-based cnn with improved triplet loss function,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 1335–1344.<br>[75] C. Su, S. Zhang, J. Xing, W. Gao, and Q. Tian, “Deep attributes driven multi-camera person re-identification,” in European Conference on Computer Vision, 2016.<br>[76] T. Xiao, H. Li, W. Ouyang, and X. Wang, “Learning deep feature representations with domain guided dropout for person reidentification,”<br>in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.<br>[77] L. Zheng, H. Zhang, S. Sun, M. Chandraker, and Q. Tian, “Person re-identification in the wild,” arXiv preprint arXiv:1604.02531, 2016.<br>[78] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in Proceedings<br>of the IEEE Conference on Computer Vision and Pattern Recognition, 2009, pp. 248–255.<br>[79] L. Wu, C. Shen, and A. v. d. Hengel, “Deep linear discriminant analysis on fisher networks: A hybrid architecture for person re-identification,” arXiv preprint arXiv:1606.01595, 2016.<br>[80] F. Perronnin, J. Sanchez, and T. Mensink, “Improving the fisher ´ kernel for large-scale image classification,” in European Conference<br>on Computer Vision, 2010, pp. 143–156.<br>[81] S. Wu, Y.-C. Chen, X. Li, A.-C. Wu, J.-J. You, and W.-S. Zheng, “An enhanced deep feature representation for person re-identification,” in 2016 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2016, pp. 1–8.<br>[82] D. Gray, S. Brennan, and H. Tao, “Evaluating appearance models for recognition, reacquisition, and tracking,” in Proc. IEEE International Workshop on Performance Evaluation for Tracking and Surveillance (PETS), vol. 3, no. 5. Citeseer, 2007.<br>[83] W.-S. Zheng, S. Gong, and T. Xiang, “Associating groups of people,” in Proceedings of the British Machine Vision Conference, 2009, pp. 23.1–23.11.<br>[84] C. C. Loy, T. Xiang, and S. Gong, “Multi-camera activity correlation analysis,” in IEEE Conference on Computer Vision and Pattern Recognition, 2009, pp. 1988–1995.<br>[85] D. S. Cheng, M. Cristani, M. Stoppa, L. Bazzani, and V. Murino, “Custom pictorial structures for re-identification,” in British Machine<br>Vision Conference, 2011.<br>[86] M. Hirzer, C. Beleznai, P. M. Roth, and H. Bischof, “Person reidentification by descriptive and discriminative classification,” in<br>Scandinavian conference on Image analysis, 2011, pp. 91–102.<br>[87] N. Martinel and C. Micheloni, “Re-identify people in wide area camera network,” in IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2012, pp. 31–36.<br>[88] W. Li, R. Zhao, and X. Wang, “Human reidentification with transferred metric learning,” in Asian Conference on Computer Vision, 2012, pp. 31–44.<br>[89] W. Li and X. Wang, “Locally aligned feature transforms across views,” in Proceedings of the IEEE Conference on Computer Vision<br>and Pattern Recognition, 2013, pp. 3594–3601.<br>[90] P. M. Roth, M. Hirzer, M. Koestinger, C. Beleznai, and H. Bischof, “Mahalanobis distance learning for person re-identification,” in Person Re-Identification, ser. Advances in Computer Vision and Pattern Recognition, S. Gong, M. Cristani, S. Yan, and C. C. Loy, Eds. London, United Kingdom: Springer, 2014, pp. 247–267.<br>[91] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan, “Object detection with discriminatively trained part-based models,” IEEE transactions on pattern analysis and machine intelligence, vol. 32, no. 9, pp. 1627–1645, 2010.<br>[92] P. Dollar, R. Appel, S. Belongie, and P. Perona, “Fast feature ´ pyramids for object detection,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 36, no. 8, pp. 1532–1545, 2014.<br>[93] W. Huang, R. Hu, C. Liang, Y. Yu, Z. Wang, X. Zhong, and C. Zhang, “Camera network based person re-identification by leveraging spatial-temporal constraint and multiple cameras relations,” in International Conference on Multimedia Modeling. Springer, 2016, pp. 174–186.<br>[94] J. Garcia, N. Martinel, C. Micheloni, and A. Gardel, “Person re-identification ranking optimisation by discriminant context information analysis,” in ICCV, 2015.<br>[95] S. Paisitkriangkrai, C. Shen, and A. van den Hengel, “Learning to rank in person re-identification with metric ensembles,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1846–1855.<br>[96] H. Wang, S. Gong, X. Zhu, and T. Xiang, “Human-in-the-loop person re-identification,” in European Conference on Computer Vision, 2016.<br>[97] B. Ma, Y. Su, and F. Jurie, “Bicov: a novel image representation for person re-identification and face verification,” in British Machive Vision Conference, 2012, p. 11.<br>[98] ——, “Local descriptors encoded by fisher vectors for person re-identification,” in European Conference on Computer Vision. Springer, 2012, pp. 413–422.<br>[99] O. Hamdoun, F. Moutarde, B. Stanciulescu, and B. Steux, “Person re-identification in multi-camera system by signature based on interest point descriptors collected on short video sequences,” in ACM/IEEE International Conference on Distributed Smart Cameras, 2008, pp. 1–6.<br>[100] D. N. T. Cong, C. Achard, L. Khoudour, and L. Douadi, “Video sequences association for people re-identification across multiple<br>non-overlapping cameras,” in International Conference on Image Analysis and Processing. Springer, 2009, pp. 179–189.<br>[101] S. Karaman and A. D. Bagdanov, “Identity inference: generalizing person re-identification scenarios,” in European Conference on Computer Vision. Springer, 2012, pp. 443–452.<br>[102] A. Bedagkar-Gala and S. K. Shah, “Part-based spatio-temporal model for multi-person re-identification,” Pattern Recognition Letters, vol. 33, no. 14, pp. 1908–1915, 2012.<br>[103] S. Karanam, Y. Li, and R. J. Radke, “Sparse re-id: Block sparsity for person re-identification,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2015, pp. 33–40.<br>[104] Y.-J. Cho and K.-J. Yoon, “Improving person re-identification via pose-aware multi-shot matching,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 1354–1362.<br>[105] T. Wang, S. Gong, X. Zhu, and S. Wang, “Person re-identification by video ranking,” in European Conference on Computer Vision, 2014, pp. 688–703.<br>[106] A. Klaser, M. Marszałek, and C. Schmid, “A spatio-temporal descriptor based on 3d-gradients,” in BMVC 2008-19th British Machine Vision Conference. British Machine Vision Association, 2008, pp. 275–1.<br>[107] J. Man and B. Bhanu, “Individual recognition using gait energy image,” IEEE transactions on pattern analysis and machine intelligence, vol. 28, no. 2, pp. 316–322, 2006.<br>[108] K. Liu, B. Ma, W. Zhang, and R. Huang, “A spatiotemporal appearance representation for viceo-based pedestrian re-identification,” in Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 3810–3818.<br>[109] C. Gao, J. Wang, L. Liu, J.-G. Yu, and N. Sang, “Temporally aligned pooling representation for video-based person re-identification,” in IEEE International Conference on Image Processing (ICIP), 2016, pp. 4284–4288.<br>[110] Z. Liu, J. Chen, and Y. Wang, “A fast adaptive spatio-temporal 3d feature for video-based person re-identification,” in 2016 IEEE International Conference on Image Processing (ICIP). IEEE, 2016, pp. 4294–4298.<br>[111] W.-S. Zheng, S. Gong, and T. Xiang, “Transfer re-identification: From person to set-based verification,” in IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 2650–2657.<br>[112] X. Zhu, X.-Y. Jing, F. Wu, and H. Feng, “Video-based person re-identification by simultaneously learning intra-video and intervideo distance metrics,” in IJCAI, 2016.<br>[113] J. You, A. Wu, X. Li, and W.-S. Zheng, “Top-push video-based person re-identification,” in IEEE Conference on Computer Vision and Pattern Recognition, 2016.<br>[114] N. McLaughlin, J. Martinez del Rincon, and P. Miller, “Recurrent convolutional network for video-based person re-identification,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.<br>[115] Y. Yan, B. Ni, Z. Song, C. Ma, Y. Yan, and X. Yang, “Person re-identification via recurrent feature aggregation,” in European Conference on Computer Vision, 2016.<br>[116] Z. Xu, Y. Yang, and A. G. Hauptmann, “A discriminative cnn video representation for event detection,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1798–1807.<br>[117] H. Jegou, M. Douze, C. Schmid, and P. P ´ erez, “Aggregating local ´ descriptors into a compact image representation,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2010, pp. 3304–3311.<br>[118] B. Fernando, E. Gavves, J. Oramas, A. Ghodrati, and T. Tuytelaars, “Rank pooling for action recognition,” IEEE transactions on pattern analysis and machine intelligence, 2016.<br>[119] P. Wang, Y. Cao, C. Shen, L. Liu, and H. T. Shen, “Temporal pyramid pooling based convolutional neural networks for action recognition,” arXiv preprint arXiv:1503.01224, 2015.<br>[120] L. Wu, C. Shen, and A. van den Hengel, “Deep recurrent convolutional networks for video-based person re-identification: An end-to-end approach,” arXiv:1606.01595, 2016.<br>[121] Z. Wu, X. Wang, Y.-G. Jiang, H. Ye, and X. Xue, “Modeling spatial-temporal clues in a hybrid deep learning framework for video classification,” in Proceedings of the 23rd ACM international conference on Multimedia. ACM, 2015, pp. 461–470.<br>[122] A. Ess, B. Leibe, and L. Van Gool, “Depth and appearance for mobile scene analysis,” in IEEE International Conference on Computer Vision. IEEE, 2007, pp. 1–8.<br>[123] D. Baltieri, R. Vezzani, and R. Cucchiara, “3dpes: 3d people dataset for surveillance and forensics,” in Proceedings of the 2011 joint ACM workshop on Human gesture and behavior understanding. ACM, 2011, pp. 59–64.<br>[124] G. Lisanti, I. Masi, A. D. Bagdanov, and A. Del Bimbo, “Person re-identification by iterative re-weighted sparse ranking,” IEEE transactions on pattern analysis and machine intelligence, vol. 37, no. 8, pp. 1629–1642, 2015.<br>[125] A. Dehghan, S. Modiri Assari, and M. Shah, “Gmmcp tracker: Globally optimal generalized maximum multi clique problem for multiple object tracking,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 4091–4099.<br>[126] N. Martinel, A. Das, C. Micheloni, and A. K. Roy-Chowdhury, “Re-identification in the function space of feature warps,” IEEE transactions on pattern analysis and machine intelligence, vol. 37, no. 8, pp. 1656–1669, 2015.<br>[127] Y. Zhang, B. Li, H. Lu, A. Irie, and X. Ruan, “Sample-specific svm learning for person re-identification,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.<br>[128] T. Xiao, S. Li, B. Wang, L. Lin, and X. Wang, “End-to-end deep learning for person search,” arXiv preprint arXiv:1604.01850, 2016.<br>[129] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards realtime object detection with region proposal networks,” in Advances in Neural Information Processing Systems, 2015, pp. 91–99.<br>[130] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies for accurate object detection and semantic segmentation,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2014, pp. 580–587.<br>[131] N. Dalal and B. Triggs, “Histograms of oriented gradients for human detection,” in IEEE Conference on Computer Vision and Pattern Recognition, 2005, pp. 886–893.<br>[132] W. Nam, P. Dollar, and J. H. Han, “Local decorrelation for ´ improved pedestrian detection,” in Advances in Neural Information Processing Systems, 2014, pp. 424–432.<br>[133] J. Berclaz, F. Fleuret, E. Turetken, and P. Fua, “Multiple object tracking using k-shortest paths optimization,” IEEE transactions on pattern analysis and machine intelligence, vol. 33, no. 9, pp. 1806–1819, 2011.<br>[134] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman, “The pascal visual object classes (voc) challenge,” International journal of computer vision, vol. 88, no. 2, pp. 303–338, 2010.<br>[135] P. Dollar, C. Wojek, B. Schiele, and P. Perona, “Pedestrian detection: An evaluation of the state of the art,” IEEE transactions on pattern analysis and machine intelligence, vol. 34, no. 4, pp. 743–761, 2012.<br>[136] S. Zhang, R. Benenson, M. Omran, J. Hosang, and B. Schiele, “How far are we from solving pedestrian detection?” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.<br>[137] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving? the kitti vision benchmark suite,” in IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 3354–3361.<br>[138] J. Hosang, R. Benenson, P. Dollar, and B. Schiele, “What makes for ´ effective detection proposals?” IEEE transactions on pattern analysis and machine intelligence, vol. 38, no. 4, pp. 814–830, 2016.<br>[139] L. Leal-Taixe, A. Milan, I. Reid, S. Roth, and K. Schindler, “Motchal- ´ lenge 2015: Towards a benchmark for multi-target tracking,” arXiv preprint arXiv:1504.01942, 2015.<br>[140] K. Bernardin and R. Stiefelhagen, “Evaluating multiple object tracking performance: the clear mot metrics,” EURASIP Journal on Image and Video Processing, vol. 2008, no. 1, pp. 1–10, 2008.<br>[141] R. Girshick, “Fast r-cnn,” in Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 1440–1448.<br>[142] S.-I. Yu, Y. Yang, and A. Hauptmann, “Harry potter’s marauder’s map: Localizing and tracking multiple persons-of-interest by nonnegative discretization,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 3714–3720.<br>[143] L. Deng, M. L. Seltzer, D. Yu, A. Acero, A.-r. Mohamed, and G. E. Hinton, “Binary coding of speech spectrograms using a deep auto-encoder.” in Interspeech. Citeseer, 2010, pp. 1692–1695.<br>[144] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in Advances in Neural Information Processing Systems, 2014, pp. 2672–2680.<br>[145] J. Wang and S. Li, “Query-driven iterated neighborhood graph search for large scale indexing,” in Proceedings of the 20th ACM international conference on Multimedia. ACM, 2012, pp. 179–188.<br>[146] A. Chakraborty, A. Das, and A. Roy-Chowdhury, “Network consistent data association,” IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014.<br>[147] D. Nister and H. Stewenius, “Scalable recognition with a vocabulary tree,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2006, pp. 2161–2168.<br>[148] H. Jegou, M. Douze, and C. Schmid, “Hamming embedding and weak geometric consistency for large scale image search,” in European conference on computer vision, 2008, pp. 304–317.<br>[149] Y. Zhang, Z. Jia, and T. Chen, “Image retrieval with geometrypreserving visual phrases,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2011, pp. 809–816.<br>[150] Y. Weiss, A. Torralba, and R. Fergus, “Spectral hashing,” in Advances in neural information processing systems, 2009, pp. 1753– 1760.<br>[151] Y. Gong and S. Lazebnik, “Iterative quantization: A procrustean approach to learning binary codes,” in IEEE Conference on Computer Vision and Pattern Recognition, 2011, pp. 817–824.<br>[152] X. Liu, X. Fan, C. Deng, Z. Li, H. Su, and D. Tao, “Multilinear hyperplane hashing,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 5119–5127.<br>[153] Z. Zhang, Y. Chen, and V. Saligrama, “Efficient training of very deep neural networks for supervised hashing,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 1487–1495.<br>[154] F. Zhao, Y. Huang, L. Wang, and T. Tan, “Deep semantic ranking based hashing for multi-label image retrieval,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1556–1564.<br>[155] V. Erin Liong, J. Lu, G. Wang, P. Moulin, and J. Zhou, “Deep hashing for compact binary codes learning,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 2475–2483.<br>[156] A. Krizhevsky and G. Hinton, “Learning multiple layers of features from tiny images,” 2009.<br>[157] T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y. Zheng, “Nuswide: a real-world web image database from national university of singapore,” in Proceedings of the ACM international conference on image and video retrieval, 2009, p. 48.<br>[158] R. Zhang, L. Lin, R. Zhang, W. Zuo, and L. Zhang, “Bit-scalable deep hashing with regularized similarity learning for image retrieval and person re-identification,” IEEE Transactions on Image Processing, vol. 24, no. 12, pp. 4766–4779, 2015.<br>[159] L. Xie, J. Wang, Z. Wei, M. Wang, and Q. Tian, “Disturblabel: Regularizing cnn on the loss layer,” in IEEE Conference on Computer Vision and Pattern Recognition, 2016.<br>[160] H. Wang, S. Gong, and T. Xiang, “Unsupervised learning of generative topic saliency for person re-identification,” 2014.<br>[161] X. Wang, W.-S. Zheng, X. Li, and J. Zhang, “Cross-scenario transfer person re-identification,” 2015.<br>[162] A. J. Ma, J. Li, P. C. Yuen, and P. Li, “Cross-domain person reidentification using domain adaptation ranking svms,” IEEE Transactions on Image Processing, vol. 24, no. 5, pp. 1599–1613, 2015.<br>[163] P. Peng, T. Xiang, Y. Wang, M. Pontil, S. Gong, T. Huang, and Y. Tian, “Unsupervised cross-dataset transfer learning for person re-identification,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.<br>[164] T. Mei, Y. Rui, S. Li, and Q. Tian, “Multimedia search reranking: A literature survey,” ACM Computing Surveys, vol. 46, no. 3, p. 38, 2014.<br>[165] C. Liu, C. Change Loy, S. Gong, and G. Wang, “Pop: Person reidentification post-rank optimisation,” in Proceedings of the IEEE International Conference on Computer Vision, 2013, pp. 441–448.<br>[166] N. Martinel, A. Das, C. Micheloni, and A. K. Roy-Chowdhury, “Temporal model adaptation for person re-identification,” in European Conference on Computer Vision, 2016.<br>[167] L. Zheng, S. Wang, L. Tian, F. He, Z. Liu, and Q. Tian, “Queryadaptive late fusion for image search and person re-identification,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1741–1750.<br>[168] Q. Leng, R. Hu, C. Liang, Y. Wang, and J. Chen, “Person reidentification with content and context re-ranking,” Multimedia Tools and Applications, vol. 74, no. 17, pp. 6989–7014, 2015.<br>[169] D. Qin, S. Gammeter, L. Bossard, T. Quack, and L. Van Gool, “Hello neighbor: Accurate object retrieval with k-reciprocal nearest neighbors,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2011, pp. 777–784.<br>[170] X. Shen, Z. Lin, J. Brandt, S. Avidan, and Y. Wu, “Object retrieval and localization with spatially-constrained similarity measure and k-nn re-ranking,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 3013–3020.<br>[171] R. Arandjelovic and A. Zisserman, “Three things everyone should ´ know to improve object retrieval,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 2911–2918.<br>[172] W.-S. Zheng, S. Gong, and T. Xiang, “Towards open-world person re-identification by one-shot group-based verification,” IEEE transactions on pattern analysis and machine intelligence, vol. 38, no. 3, pp. 591–606, 2016.<br>[173] S. Liao, Z. Mo, J. Zhu, Y. Hu, and S. Z. Li, “Open-set person re-identification,” arXiv preprint arXiv:1408.0872, 2014.<br>[174] B. DeCann and A. Ross, “Modelling errors in a biometric reidentification system,” IET Biometrics, vol. 4, no. 4, pp. 209–219, 2015.</p></div><div class="popular-posts-header">相关文章</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="/2018/09/06/Aligned-ReID/" rel="bookmark">Aligned ReID</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/2018/09/07/Improving-Person-Re-identification-by-Attribute-and-Identity-Learning/" rel="bookmark">Improving Person Re-identification by Attribute and Identity Learning</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/2018/08/21/Batch-Normalization/" rel="bookmark">Batch Normalization</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/2018/09/11/Multi-attribute-learning-for-pedestrian-attribute-recognition-in-surveillance-scenarios/" rel="bookmark">Multi-attribute learning for pedestrian attribute recognition in surveillance scenarios</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/2018/09/14/SIMPLE-ONLINE-AND-REALTIME-TRACKING-WITH-A-DEEP-ASSOCIATION-METRIC/" rel="bookmark">SIMPLE ONLINE AND REALTIME TRACKING WITH A DEEP ASSOCIATION METRIC</a></div></li></ul><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>慕湮</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="http://muyaan.com/2018/08/29/Person-Re-identification-Past-Present-and-Future/" title="Person Re-identification: Past, Present and Future">http://muyaan.com/2018/08/29/Person-Re-identification-Past-Present-and-Future/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noopener noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a> <a href="/tags/Paper/" rel="tag"># Paper</a> <a href="/tags/re-ID/" rel="tag"># re-ID</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2018/08/21/Batch-Normalization/" rel="next" title="Batch Normalization"><i class="fa fa-chevron-left"></i> Batch Normalization</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2018/09/02/Cross-Entropy-Loss-——-交叉熵Loss/" rel="prev" title="Cross Entropy Loss —— 交叉熵Loss">Cross Entropy Loss —— 交叉熵Loss <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4" alt="慕湮"><p class="site-author-name" itemprop="name">慕湮</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">22</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">31</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/tycallen" target="_blank" title="GitHub" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:tyc.allen@gmail.com" target="_blank" title="E-Mail" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-envelope"></i>E-Mail</a> </span><span class="links-of-author-item"><a href="http://weibo.com/pojunallen" target="_blank" title="微博" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-weibo"></i>微博</a> </span><span class="links-of-author-item"><a href="https://tuchong.com/1070837" target="_blank" title="图虫" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-globe"></i>图虫</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-block"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> 友情链接</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="http://dotrabbit.tk" title="dotrabbit" target="_blank" rel="external nofollow noopener noreferrer">dotrabbit</a></li></ul></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Person-Re-identification-Past-Present-and-Future"><span class="nav-text">Person Re-identification: Past, Present and Future</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text"></span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Introduction"><span class="nav-text">1. Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-Organization-of-This-Survey"><span class="nav-text">1.1 Organization of This Survey</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-A-Brief-History-of-Person-Re-ID"><span class="nav-text">1.2 A Brief History of Person Re-ID</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-Relationship-with-Classification-and-Retrieval"><span class="nav-text">1.3 Relationship with Classification and Retrieval</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Image-Based-Person-Re-ID"><span class="nav-text">2. Image-Based Person Re-ID</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-Hand-crafted-Systems"><span class="nav-text">2.1 Hand-crafted Systems</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-1-Pedestrian-Description"><span class="nav-text">2.1.1 Pedestrian Description</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-2-Distance-Metric-Learning"><span class="nav-text">2.1.2 Distance Metric Learning</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Deeply-learned-Systems"><span class="nav-text">2.2 Deeply-learned Systems</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-Datasets-and-Evaluation"><span class="nav-text">2.3 Datasets and Evaluation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-1-Datasets"><span class="nav-text">2.3.1 Datasets</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-2-Evaluation-Metrics"><span class="nav-text">2.3.2 Evaluation Metrics</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-3-Re-ID-Accuracy-Over-the-Years"><span class="nav-text">2.3.3 Re-ID Accuracy Over the Years</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Video-Based-Person-Re-ID"><span class="nav-text">3. Video-Based Person Re-ID</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Hand-crafted-Systems"><span class="nav-text">3.1 Hand-crafted Systems</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-Deeply-learned-Systems"><span class="nav-text">3.2 Deeply-learned Systems</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-Datasets-and-Evaluation"><span class="nav-text">3.3 Datasets and Evaluation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Future-Detection-Tracking-and-Person-Re-ID"><span class="nav-text">4. Future: Detection, Tracking and Person Re-ID</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-Previous-Works"><span class="nav-text">4.1 Previous Works</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-Future-Issues"><span class="nav-text">4.2 Future Issues</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-1-System-Performance-Evaluation"><span class="nav-text">4.2.1 System Performance Evaluation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-2-The-Influence-of-Detector-Tracker-on-Re-ID"><span class="nav-text">4.2.2 The Influence of Detector/Tracker on Re-ID</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Future-Person-Re-ID-in-Very-Large-Galleries"><span class="nav-text">5. Future: Person Re-ID in Very Large Galleries</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-Other-Important-Yet-Under-Developed-Open-Issues"><span class="nav-text">6. Other Important Yet Under-Developed Open Issues</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-Battle-Against-Data-Volumn"><span class="nav-text">6.1 Battle Against Data Volumn</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-Re-ranking-Re-ID-Results"><span class="nav-text">6.2 Re-ranking Re-ID Results</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3-Open-World-Person-Re-ID"><span class="nav-text">6.3 Open-World Person Re-ID</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-text">References</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; 2015 – <span itemprop="copyrightYear">2018</span> <span class="with-love" id="animate"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">慕湮</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="站点总字数">320k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">9:42</span></div><div class="powered-by">由 <a class="theme-link" target="_blank" rel="external nofollow noopener noreferrer" href="https://hexo.io">Hexo</a> 强力驱动</div><span class="post-meta-divider">|</span><div class="theme-info">主题 – <a class="theme-link" target="_blank" rel="external nofollow noopener noreferrer" href="https://theme-next.org">NexT.Muse</a></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="//cdn.staticfile.org/jquery/2.1.3/jquery.min.js"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="//cdn.staticfile.org/velocity/1.2.1/velocity.min.js"></script><script type="text/javascript" src="//cdn.staticfile.org/velocity/1.2.1/velocity.ui.min.js"></script><script type="text/javascript" src="//cdn.staticfile.org/fancybox/2.1.5/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=6.4.1"></script><script type="text/javascript" src="/js/src/motion.js?v=6.4.1"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=6.4.1"></script><script type="text/javascript" src="/js/src/post-details.js?v=6.4.1"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.1"></script><script>function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();

      Counter('get', '/classes/Counter', { where: JSON.stringify({ url }) })
        .done(function ({ results }) {
          if (results.length > 0) {
            var counter = results[0];
            
              var $element = $(document.getElementById(url));
              $element.find('.leancloud-visitors-count').text(counter.time + 1);
            
            Counter('put', `/classes/Counter/${counter.objectId}`, JSON.stringify({ time: { "__op":"Increment", "amount":1 } }))
            
            .fail(function ({ responseJSON }) {
                console.log('Failed to save Visitor num, with error message: ' + responseJSON.error);
            })
          } else {
            
              Counter('post', '/classes/Counter', JSON.stringify({ title: title, url: url, time: 1}))
                .done(function () {
                  var $element = $(document.getElementById(url));
                  $element.find('.leancloud-visitors-count').text(1);
                })
                .fail(function () {
                  console.log('Failed to create');
                });
            
          }
        })
      .fail(function ({ responseJSON }) {
        console.log('LeanCloud Counter Error:' + responseJSON.code + " " + responseJSON.error);
      });
    }
    

    $(function() {
      $.get('https://app-router.leancloud.cn/2/route?appId=' + "UWGXBMGbguCDhSCbC3NrUQtL-gzGzoHsz")
        .done(function ({ api_server }) {
          var Counter = function (method, url, data) {
            return $.ajax({
              method: method,
              url: `https://${api_server}/1.1${url}`,
              headers: {
                'X-LC-Id': "UWGXBMGbguCDhSCbC3NrUQtL-gzGzoHsz",
                'X-LC-Key': "ke1jrA5b6VyR89Kqqqwf2kPP",
                'Content-Type': 'application/json',
              },
              data: data,
            });
          };
          
          addCount(Counter);
          
        })
    });</script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="http://muyaan.com/js/src/async.js"></script></body></html>