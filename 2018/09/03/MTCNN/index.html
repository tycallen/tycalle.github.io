<!DOCTYPE html><html class="theme-next muse use-motion" lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><script src="https://cdn.staticfile.org/pace/1.0.2/pace.min.js"></script><link href="https://cdn.staticfile.org/pace/1.0.2/themes/blue/pace-theme-big-counter.min.css" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="//cdn.staticfile.org/fancybox/2.1.5/jquery.fancybox.min.js" rel="stylesheet" type="text/css"><link href="//cdn.staticfile.org/font-awesome/4.6.2/css/font-awesome.min.css" rel="stylesheet" type="text/css"><link href="/css/main.css?v=6.4.1" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.4.1"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.4.1"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.4.1"><link rel="mask-icon" href="/images/logo.svg?v=6.4.1" color="#222"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Muse",version:"6.4.1",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!0,fastclick:!1,lazyload:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="description" content="公司其它项目组在复现的一篇论文，经典人脸检测与对齐。自上一篇论文翻吐了之后，打算总结性的记录论文了。这一篇没有全文翻译，看得非常轻松，不过跟论文本身高度精炼，不吹闲B有关系吧。喜欢这样的paper！     MTCNN: Joint Face Detection and Alignment using Multi-task Cascaded Convolutional NetworksKaipen"><meta name="keywords" content="Computer Vision,Deep Learning,Paper,Face Detection,Face Alignment"><meta property="og:type" content="article"><meta property="og:title" content="MTCNN"><meta property="og:url" content="http://muyaan.com/2018/09/03/MTCNN/index.html"><meta property="og:site_name" content="慕湮"><meta property="og:description" content="公司其它项目组在复现的一篇论文，经典人脸检测与对齐。自上一篇论文翻吐了之后，打算总结性的记录论文了。这一篇没有全文翻译，看得非常轻松，不过跟论文本身高度精炼，不吹闲B有关系吧。喜欢这样的paper！     MTCNN: Joint Face Detection and Alignment using Multi-task Cascaded Convolutional NetworksKaipen"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="http://muyaan.com/2018/09/03/MTCNN/1535953283802.png"><meta property="og:image" content="http://muyaan.com/2018/09/03/MTCNN/1535955102668.png"><meta property="og:image" content="http://muyaan.com/2018/09/03/MTCNN/1535957304770.png"><meta property="og:image" content="http://muyaan.com/2018/09/03/MTCNN/1535963902762.png"><meta property="og:image" content="http://muyaan.com/2018/09/03/MTCNN/1535964541018.png"><meta property="og:image" content="http://muyaan.com/2018/09/03/MTCNN/1535964779720.png"><meta property="og:image" content="http://muyaan.com/2018/09/03/MTCNN/1535964796705.png"><meta property="og:updated_time" content="2018-09-12T03:22:25.014Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="MTCNN"><meta name="twitter:description" content="公司其它项目组在复现的一篇论文，经典人脸检测与对齐。自上一篇论文翻吐了之后，打算总结性的记录论文了。这一篇没有全文翻译，看得非常轻松，不过跟论文本身高度精炼，不吹闲B有关系吧。喜欢这样的paper！     MTCNN: Joint Face Detection and Alignment using Multi-task Cascaded Convolutional NetworksKaipen"><meta name="twitter:image" content="http://muyaan.com/2018/09/03/MTCNN/1535953283802.png"><link rel="canonical" href="http://muyaan.com/2018/09/03/MTCNN/"><script type="text/javascript" id="page.configurations">CONFIG.page={sidebar:""}</script><title>MTCNN | 慕湮</title><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?ca5844321cfb80fdf6f12b4dcc326991";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style type="text/css">.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.logo-line-after i{right:initial}</style></noscript></head><body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">慕湮</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">白日放歌须纵酒</h1></div><div class="site-nav-toggle"><button aria-label="切换导航栏"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://muyaan.com/2018/09/03/MTCNN/"><span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="慕湮"><meta itemprop="description" content=""><meta itemprop="image" content="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4"></span><span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="慕湮"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">MTCNN</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2018-09-03 16:59:11" itemprop="dateCreated datePublished" datetime="2018-09-03T16:59:11+08:00">2018-09-03</time> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2018-09-12 11:22:25" itemprop="dateModified" datetime="2018-09-12T11:22:25+08:00">2018-09-12</time> </span><span id="/2018/09/03/MTCNN/" class="leancloud_visitors" data-flag-title="MTCNN"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数：</span> <span title="本文字数">9.5k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">17 分钟</span></div></div></header><div class="post-body" itemprop="articleBody"><p>公司其它项目组在复现的一篇论文，经典人脸检测与对齐。自上一篇论文翻吐了之后，打算总结性的记录论文了。这一篇没有全文翻译，看得非常轻松，不过跟论文本身高度精炼，不吹闲B有关系吧。喜欢这样的paper！</p><blockquote><h1 id="MTCNN-Joint-Face-Detection-and-Alignment-using-Multi-task-Cascaded-Convolutional-Networks"><a href="#MTCNN-Joint-Face-Detection-and-Alignment-using-Multi-task-Cascaded-Convolutional-Networks" class="headerlink" title="MTCNN: Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks"></a><a href="https://kpzhang93.github.io/MTCNN_face_detection_alignment/paper/spl.pdf" rel="external nofollow noopener noreferrer" target="_blank">MTCNN: Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks</a></h1><p>Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, Yu Qiao<br>IEEE 2016</p></blockquote><h2><a href="#" class="headerlink"></a><a id="more"></a></h2><h2 id="I-Introduction"><a href="#I-Introduction" class="headerlink" title="I. Introduction"></a>I. Introduction</h2><p>人脸检测和对齐是很多人脸应用的基础，如人脸识别和表情分析facial expression analysis。但脸部视觉变化过大，如遮挡，姿势和光照都为实际应用提出了挑战。</p><p>Viola和Jones提出的层叠人脸检测$^{[2]}$使用了Haar-Like特征和AdaBoost取得了不错的性能，也有实时效率。但在实际应用效果不佳。也有使用Deformable part models DPM的$^{[5,6,7]}$。Yang等人$^{[11]}$使用了卷积神经网络进行人脸属性识别，来进一步产生人脸窗口候选。Li等人$^{[19]}$使用了层叠CNN，但需要限位框校准，且忽略了人脸关键点位置的相关性和限位框回归。</p><p>人脸对齐可以分为两类，基于回归的方法$^{[12,13,16]}$和模板匹配template fitting方法$^{[14, 15, 7]}$。近期Zhang等人$^{[22]}$提出了人脸属性识别作为额外的任务，以加强人脸对齐性能。</p><p>但之前所有人脸检测和对齐方法都忽略了它们之间的相关性。有的研究尝试了同时解决它们，但仍有限制。Chen等人$^{[18]}$使用不同像素值的随机森林同时进行对齐和检测，但性能受限于手工特征。Zhang等人$^{[20]}$使用多任务CNN来提升多view人脸检测，但检测召回率受限于弱人脸检测器产生的初始检测窗口。</p><p>传统Hard sample mining使用离线形式，极大增加了手工操作。需要在线的才能自动适应当前训练状态。</p><p>本论文提出了一个新的框架，用多任务训练的层叠CNN集成了这两个任务。该CNN包含3个阶段。第一阶段通过浅CNN快速产生候选窗口。接着通过一个更复杂的CNN，拒绝大量无脸窗口。最终使用一个更强力的CNN来再次精炼结果，输出5个人脸关键点坐标。</p><h2 id="II-Approach"><a href="#II-Approach" class="headerlink" title="II. Approach"></a>II. Approach</h2><h3 id="A-Overall-Framework"><a href="#A-Overall-Framework" class="headerlink" title="A. Overall Framework"></a>A. Overall Framework</h3><p>我们方法流程见图1。<br><img src="/2018/09/03/MTCNN/./1535953283802.png" alt="图1"></p><p>首先缩放到不同尺度，构建图片金字塔。</p><p><strong>Stage 1：</strong>我们提出了一个FCN，叫P-Net（proposal net），来获取候选脸部窗口和它们的限位框回归向量。接着根据回归向量校正候选。最后使用非极大值抑制NMS来合并高度重合的候选。</p><p><strong>Stage 2：</strong>所有候选输入到另一个CNN，叫Refine CNN（R-Net），它拒绝大量错误候选，使用限位框回归进行校正，使用了NMS。</p><p><strong>Stage 3：</strong>类似阶段2，但本阶段更偏向有监督的人类区域识别。网络会输出5个人类关键点。</p><h3 id="B-CNN-Architecture"><a href="#B-CNN-Architecture" class="headerlink" title="B. CNN Architecture"></a>B. CNN Architecture</h3><p>现有CNN方式有以下方面的限制：（1）一些卷积层的卷积核缺少多样性，也许会影响其辨识力。（2）与多类物体检测和分类任务相比，人脸检测是一个挑战性的二分类任务，也许每层只需更少的卷结核。因此我们降低了核数量，将$5\times5$改为$3\times3$卷积核来降低计算量，增大深度以获得更好性能。我们因此用更少的运行时间获得了更高性能。（训练阶段结果见表1）。CNN架构见图2，我们使用了PReLU$^{[30]}$作为非线性激活。</p><p><img src="/2018/09/03/MTCNN/./1535955102668.png" alt="表1"></p><p><img src="/2018/09/03/MTCNN/./1535957304770.png" alt="图2"></p><h3 id="C-Training"><a href="#C-Training" class="headerlink" title="C. Training"></a>C. Training</h3><p>我们用了3个任务训练CNN检测器：人脸/无人脸分类，限位框回归和人脸关键点定位。</p><p><em>1) Face classification:</em> 训练目标表达为二分类问题，每个样本$x_i$我们使用交叉熵loss：</p><script type="math/tex;mode=display">L_i ^{det} = - (y_i^{det} \log (p_i) + (1 - y_i ^{det} ) (1 - \log (p_i))  )  \ \  \  \tag {1}</script><p>其中$p_i$是网络输出的样本$x_i$是一张脸的概率。$y_i^{det} \in { 0, 1}$标记了gt标签</p><p><em>2) Bounding box regression:</em> 对于每个候选窗口，我们预测它和最近gt的偏移。训练目标表达为回归问题，我们为每个样本使用了欧几里得loss：</p><script type="math/tex;mode=display">L_i^{box} = \begin {Vmatrix} \hat y_i^{box} - y _i ^{box} \end {Vmatrix} \tag 2</script><p>其中$\hat y_i^{box} $是来自网络的回归目标，$ y _i ^{box} $是gt坐标。共四个坐标，包括左上，宽和高，故$ y _i ^{box} \in \mathbb R^4 $</p><p><em>3) Facial landmark localization:</em> 类似限位框，人脸关键点检测也化为回归问题，我们最小化欧几里得loss：</p><script type="math/tex;mode=display">L_i^{landmark} =  \begin {Vmatrix} \hat y_i^{landmark} - y _i ^{landmark} \end {Vmatrix} \tag 3</script><p>共5个关键点，包括左右眼，鼻子和左右嘴角。</p><p><em>4) Multi-source training:</em> 因为每个CNN有多个任务，因此训练过程会有多种训练图片，比如脸，没有脸和部分对齐的脸。有些loss函数（公式1-3）不会用到。例如对于背景样本，我们只会计算$L_i^{det}$，另两个loss设为0。这可以通过样本类型指示器实现。总体训练目标为：</p><script type="math/tex;mode=display">\min \sum_{i=1} ^N \sum _{j\in \{ det, box, landmark\}} \alpha _j \beta _i ^j L_i ^j \tag 4</script><p>其中N是训练样本数量，$\alpha<em>j$标识任务重要性。在P-Net和R-Net中，$\alpha </em>{det} = 1 , \alpha <em>{box} = 0.5 , \alpha </em>{landmark} = 0.5$，而在O-Net中为$\alpha <em>{det} = 1 , \alpha </em>{box} = 0.5 , \alpha _{landmark} = 1$。$\beta_i ^j \in { 0 ,1}$是样本类型指示器。使用了SGD训练这些CNN。</p><p><em>5) Online Hard sample mining:</em> 与传统HEM在原始分类器训练完成后使用不同，我们进行在线HEM，自动使用训练过程。</p><p>特别的，在每个mini-batch，我们为前向传播计算的所有样本loss排序，选择前70%作为困难样本用于反向传播。抛弃了剩余简单样本。效果见章节III。</p><h2 id="III-Experiments"><a href="#III-Experiments" class="headerlink" title="III. Experiments"></a>III. Experiments</h2><p>本节我们评估了HEM的有效性，用我们的人脸检测和对齐方法在FDDB，WIDER FACE和Wild（AFLW）上与前沿方法进行了比较。</p><h3 id="A-Training-Data"><a href="#A-Training-Data" class="headerlink" title="A. Training Data"></a>A. Training Data</h3><p>因为我们同时进行人脸检测和对齐，在训练中我们使用了四中数据标注：(i) Negative: 与任意gt脸IoU都低于0.3的区域； (2) Positive: 与一个gt脸IoU大于0.65；(3) Part faces: 与一个gt脸$IoU \in [0.4, 0.65]；(4)关键点脸: 有5个关键点的脸。部分脸和负样本之间有模糊地带，不同数据集标注中它们都不同。正负样本用于分类任务，正样本和部分脸用于限位框回归，关键点脸用于关键点定位。总体训练数据组成为3:1:1:2(Negative/ positive/ part face/ landmark face)。每个网络训练集如下：</p><p>1) P-Net: 随机从WIDER FACE取几小块，获得正、负和部分脸样本，从CelebA随机取关键点脸</p><p>2) R-Net: 使用第一阶段的框架在WIDER FACE中检测脸，获得获得正、负和部分脸样本，从CelebA中检测关键点脸</p><p>3) O-Net: 类似R-Net，但我们使用框架前两步进行检测以获得样本。</p><h3 id="B-The-effectiveness-of-online-hard-sample-mining"><a href="#B-The-effectiveness-of-online-hard-sample-mining" class="headerlink" title="B. The effectiveness of online hard sample mining"></a>B. The effectiveness of online hard sample mining</h3><p>我们训练了两个P-Net（是否使用OHEM）并在FDDB上比较性能（图3.a ）。OHEM提升了1.5%的整体性能。</p><p><img src="/2018/09/03/MTCNN/./1535963902762.png" alt="图3"></p><h3 id="C-The-effectiveness-of-joint-detection-and-alignment"><a href="#C-The-effectiveness-of-joint-detection-and-alignment" class="headerlink" title="C. The effectiveness of joint detection and alignment"></a>C. The effectiveness of joint detection and alignment</h3><p>我们在FDDB上（用同样的P-Net和R-Net）评估两个O-Net（是否联合训练关键点回归）。我们也比较了两个O-Net的限位框回归性能（图3.b）。</p><h3 id="D-Evaluation-on-face-detection"><a href="#D-Evaluation-on-face-detection" class="headerlink" title="D. Evaluation on face detection"></a>D. Evaluation on face detection</h3><p>我们在FDDB上与前沿方法[1, 5, 6, 11, 18, 19, 26, 27, 28, 29]进行了比较。图4 a-d显示了我们领先一大截。<br><img src="/2018/09/03/MTCNN/./1535964541018.png" alt="图4"></p><h3 id="E-Evaluation-on-face-alignment"><a href="#E-Evaluation-on-face-alignment" class="headerlink" title="E. Evaluation on face alignment"></a>E. Evaluation on face alignment</h3><p>我们与以下人脸对齐方法比较了性能：RCPR[12], TSPM[7], Luxand face SDK[17], ESR[13], CDM[15], SDM[21], TCDCN[22]。平均误差通过预估关键点与gt距离计算，并用眼间距离interocular归一化。图5说明我们领先很多。也显示了我们的方法在嘴角定位不那么优秀，也许是因为我们训练集的表情变化小，这对嘴角位置影响很大。</p><p><img src="/2018/09/03/MTCNN/./1535964779720.png" alt="图5"></p><h3 id="F-Runtime-efficiency"><a href="#F-Runtime-efficiency" class="headerlink" title="F. Runtime efficiency"></a>F. Runtime efficiency</h3><p>见表2。我们代码是未优化的Matlab 代码。</p><p><img src="/2018/09/03/MTCNN/./1535964796705.png" alt="表2"></p><h2 id="IV-Conclusion"><a href="#IV-Conclusion" class="headerlink" title="IV. Conclusion"></a>IV. Conclusion</h2><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] B. Yang, J. Yan, Z. Lei, and S. Z. Li, “Aggregate channel features for multi-view face detection,” in IEEE International Joint Conference on Biometrics, 2014, pp. 1-8.<br>[2] P. Viola and M. J. Jones, “Robust real-time face detection. International journal of computer vision,” vol. 57, no. 2, pp. 137-154, 2004<br>[3] M. T. Pham, Y. Gao, V. D. D. Hoang, and T. J. Cham, “Fast polygonal integration and its application in extending haar-like features to improve object detection,” in IEEE Conference on Computer Vision and Pattern Recognition, 2010, pp. 942-949.<br>[4] Q. Zhu, M. C. Yeh, K. T. Cheng, and S. Avidan, “Fast human detection using a cascade of histograms of oriented gradients,” in IEEE Computer Conference on Computer Vision and Pattern Recognition, 2006, pp. 1491-1498.<br>[5] M. Mathias, R. Benenson, M. Pedersoli, and L. Van Gool, “Face detection without bells and whistles,” in European Conference on Computer Vision, 2014, pp. 720-735.<br>[6] J. Yan, Z. Lei, L. Wen, and S. Li, “The fastest deformable part model for object detection,” in IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 2497-2504.<br>[7] X. Zhu, and D. Ramanan, “Face detection, pose estimation, and landmark localization in the wild,” in IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 2879-2886.<br>[8] M. Köstinger, P. Wohlhart, P. M. Roth, and H. Bischof, “Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization,” in IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2011, pp. 2144-2151.<br>[9] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with deep convolutional neural networks,” in Advances in neural information processing systems, 2012, pp. 1097-1105.<br>[10] Y. Sun, Y. Chen, X. Wang, and X. Tang, “Deep learning face representation by joint identification-verification,” in Advances in Neural Information Processing Systems, 2014, pp. 1988-1996.<br>[11] S. Yang, P. Luo, C. C. Loy, and X. Tang, “From facial parts responses to face detection: A deep learning approach,” in IEEE International Conference on Computer Vision, 2015, pp. 3676-3684.<br>[12] X. P. Burgos-Artizzu, P. Perona, and P. Dollar, “Robust face landmark estimation under occlusion,” in IEEE International Conference on Computer Vision, 2013, pp. 1513-1520.<br>[13] X. Cao, Y. Wei, F. Wen, and J. Sun, “Face alignment by explicit shape regression,” International Journal of Computer Vision, vol 107, no. 2, pp. 177-190, 2012.<br>[14] T. F. Cootes, G. J. Edwards, and C. J. Taylor, “Active appearance models,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 23, no. 6, pp. 681-685, 2001.<br>[15] X. Yu, J. Huang, S. Zhang, W. Yan, and D. Metaxas, “Pose-free facial landmark fitting via optimized part mixtures and cascaded deformable shape model,” in IEEE International Conference on Computer Vision, 2013, pp. 1944-1951.<br>[16] J. Zhang, S. Shan, M. Kan, and X. Chen, “Coarse-to-fine auto-encoder networks (CFAN) for real-time face alignment,” in European Conference on Computer Vision, 2014, pp. 1-16.<br>[17] Luxand Incorporated: Luxand face SDK, <a href="http://www.luxand.com/" rel="external nofollow noopener noreferrer" target="_blank">http://www.luxand.com/</a><br>[18] D. Chen, S. Ren, Y. Wei, X. Cao, and J. Sun, “Joint cascade face detection and alignment,” in European Conference on Computer Vision, 2014, pp. 109-122.<br>[19] H. Li, Z. Lin, X. Shen, J. Brandt, and G. Hua, “A convolutional neural network cascade for face detection,” in IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 5325-5334.<br>[20] C. Zhang, and Z. Zhang, “Improving multiview face detection with multi-task deep convolutional neural networks,” IEEE Winter Conference on Applications of Computer Vision, 2014, pp. 1036-1041.<br>[21] X. Xiong, and F. Torre, “Supervised descent method and its applications to face alignment,” in IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 532-539.<br>[22] Z. Zhang, P. Luo, C. C. Loy, and X. Tang, “Facial landmark detection by deep multi-task learning,” in European Conference on Computer Vision, 2014, pp. 94-108.<br>[23] Z. Liu, P. Luo, X. Wang, and X. Tang, “Deep learning face attributes in the wild,” in IEEE International Conference on Computer Vision, 2015, pp. 3730-3738.<br>[24] S. Yang, P. Luo, C. C. Loy, and X. Tang, “WIDER FACE: A Face Detection Benchmark”. arXiv preprint arXiv:1511.06523.<br>[25] V. Jain, and E. G. Learned-Miller, “FDDB: A benchmark for face detection in unconstrained settings,” Technical Report UMCS-2010-009, University of Massachusetts, Amherst, 2010.<br>[26] B. Yang, J. Yan, Z. Lei, and S. Z. Li, “Convolutional channel features,” in IEEE International Conference on Computer Vision, 2015, pp. 82-90.<br>[27] R. Ranjan, V. M. Patel, and R. Chellappa, “A deep pyramid deformable part model for face detection,” in IEEE International Conference on Biometrics Theory, Applications and Systems, 2015, pp. 1-8.<br>[28] G. Ghiasi, and C. C. Fowlkes, “Occlusion Coherence: Detecting and Localizing Occluded Faces,” arXiv preprint arXiv:1506.08347.<br>[29] S. S. Farfade, M. J. Saberian, and L. J. Li, “Multi-view face detection using deep convolutional neural networks,” in ACM on International Conference on Multimedia Retrieval, 2015, pp. 643-650.<br>[30] K. He, X. Zhang, S. Ren, J. Sun, “Delving deep into rectifiers: Surpassing human-level performance on imagenet classification,” in IEEE International Conference on Computer Vision, 2015, pp. 1026-1034.</p></div><div class="popular-posts-header">相关文章</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="/2018/08/07/SSD/" rel="bookmark">SSD</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/2018/08/12/YOLOv2-and-YOLO9000/" rel="bookmark">YOLOv2 and YOLO9000</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/2018/07/22/ResNet/" rel="bookmark">ResNet</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/2018/08/09/VGGNet/" rel="bookmark">VGGNet</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/2018/07/28/Fast-R-CNN/" rel="bookmark">Fast R-CNN</a></div></li></ul><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>慕湮</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="http://muyaan.com/2018/09/03/MTCNN/" title="MTCNN">http://muyaan.com/2018/09/03/MTCNN/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noopener noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Computer-Vision/" rel="tag"># Computer Vision</a> <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a> <a href="/tags/Paper/" rel="tag"># Paper</a> <a href="/tags/Face-Detection/" rel="tag"># Face Detection</a> <a href="/tags/Face-Alignment/" rel="tag"># Face Alignment</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2018/09/02/Cross-Entropy-Loss-——-交叉熵Loss/" rel="next" title="Cross Entropy Loss —— 交叉熵Loss"><i class="fa fa-chevron-left"></i> Cross Entropy Loss —— 交叉熵Loss</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2018/09/05/GAN-生成式对抗网络/" rel="prev" title="GAN 生成式对抗网络">GAN 生成式对抗网络 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4" alt="慕湮"><p class="site-author-name" itemprop="name">慕湮</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">21</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">30</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/tycallen" target="_blank" title="GitHub" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:tyc.allen@gmail.com" target="_blank" title="E-Mail" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-envelope"></i>E-Mail</a> </span><span class="links-of-author-item"><a href="http://weibo.com/pojunallen" target="_blank" title="微博" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-weibo"></i>微博</a> </span><span class="links-of-author-item"><a href="https://tuchong.com/1070837" target="_blank" title="图虫" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-globe"></i>图虫</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-block"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> 友情链接</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="http://dotrabbit.tk" title="dotrabbit" target="_blank" rel="external nofollow noopener noreferrer">dotrabbit</a></li></ul></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#MTCNN-Joint-Face-Detection-and-Alignment-using-Multi-task-Cascaded-Convolutional-Networks"><span class="nav-text">MTCNN: Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#"><span class="nav-text"></span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#I-Introduction"><span class="nav-text">I. Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#II-Approach"><span class="nav-text">II. Approach</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#A-Overall-Framework"><span class="nav-text">A. Overall Framework</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-CNN-Architecture"><span class="nav-text">B. CNN Architecture</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C-Training"><span class="nav-text">C. Training</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#III-Experiments"><span class="nav-text">III. Experiments</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#A-Training-Data"><span class="nav-text">A. Training Data</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-The-effectiveness-of-online-hard-sample-mining"><span class="nav-text">B. The effectiveness of online hard sample mining</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C-The-effectiveness-of-joint-detection-and-alignment"><span class="nav-text">C. The effectiveness of joint detection and alignment</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#D-Evaluation-on-face-detection"><span class="nav-text">D. Evaluation on face detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#E-Evaluation-on-face-alignment"><span class="nav-text">E. Evaluation on face alignment</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#F-Runtime-efficiency"><span class="nav-text">F. Runtime efficiency</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#IV-Conclusion"><span class="nav-text">IV. Conclusion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reference"><span class="nav-text">Reference</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; 2015 – <span itemprop="copyrightYear">2018</span> <span class="with-love" id="animate"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">慕湮</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="站点总字数">313k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">9:29</span></div><div class="powered-by">由 <a class="theme-link" target="_blank" rel="external nofollow noopener noreferrer" href="https://hexo.io">Hexo</a> 强力驱动</div><span class="post-meta-divider">|</span><div class="theme-info">主题 – <a class="theme-link" target="_blank" rel="external nofollow noopener noreferrer" href="https://theme-next.org">NexT.Muse</a></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="//cdn.staticfile.org/jquery/2.1.3/jquery.min.js"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="//cdn.staticfile.org/velocity/1.2.1/velocity.min.js"></script><script type="text/javascript" src="//cdn.staticfile.org/velocity/1.2.1/velocity.ui.min.js"></script><script type="text/javascript" src="//cdn.staticfile.org/fancybox/2.1.5/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=6.4.1"></script><script type="text/javascript" src="/js/src/motion.js?v=6.4.1"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=6.4.1"></script><script type="text/javascript" src="/js/src/post-details.js?v=6.4.1"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.1"></script><script>function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();

      Counter('get', '/classes/Counter', { where: JSON.stringify({ url }) })
        .done(function ({ results }) {
          if (results.length > 0) {
            var counter = results[0];
            
              var $element = $(document.getElementById(url));
              $element.find('.leancloud-visitors-count').text(counter.time + 1);
            
            Counter('put', `/classes/Counter/${counter.objectId}`, JSON.stringify({ time: { "__op":"Increment", "amount":1 } }))
            
            .fail(function ({ responseJSON }) {
                console.log('Failed to save Visitor num, with error message: ' + responseJSON.error);
            })
          } else {
            
              Counter('post', '/classes/Counter', JSON.stringify({ title: title, url: url, time: 1}))
                .done(function () {
                  var $element = $(document.getElementById(url));
                  $element.find('.leancloud-visitors-count').text(1);
                })
                .fail(function () {
                  console.log('Failed to create');
                });
            
          }
        })
      .fail(function ({ responseJSON }) {
        console.log('LeanCloud Counter Error:' + responseJSON.code + " " + responseJSON.error);
      });
    }
    

    $(function() {
      $.get('https://app-router.leancloud.cn/2/route?appId=' + "UWGXBMGbguCDhSCbC3NrUQtL-gzGzoHsz")
        .done(function ({ api_server }) {
          var Counter = function (method, url, data) {
            return $.ajax({
              method: method,
              url: `https://${api_server}/1.1${url}`,
              headers: {
                'X-LC-Id': "UWGXBMGbguCDhSCbC3NrUQtL-gzGzoHsz",
                'X-LC-Key': "ke1jrA5b6VyR89Kqqqwf2kPP",
                'Content-Type': 'application/json',
              },
              data: data,
            });
          };
          
          addCount(Counter);
          
        })
    });</script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="http://muyaan.com/js/src/async.js"></script></body></html>