<!DOCTYPE html><html class="theme-next muse use-motion" lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><script src="https://cdn.staticfile.org/pace/1.0.2/pace.min.js"></script><link href="https://cdn.staticfile.org/pace/1.0.2/themes/blue/pace-theme-big-counter.min.css" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="//cdn.staticfile.org/fancybox/2.1.5/jquery.fancybox.min.js" rel="stylesheet" type="text/css"><link href="//cdn.staticfile.org/font-awesome/4.6.2/css/font-awesome.min.css" rel="stylesheet" type="text/css"><link href="/css/main.css?v=6.4.1" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.4.1"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.4.1"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.4.1"><link rel="mask-icon" href="/images/logo.svg?v=6.4.1" color="#222"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Muse",version:"6.4.1",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!0,fastclick:!1,lazyload:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="description" content="本论文是我阅读的第二篇深度学习相关论文，2015年提出的ResNet让卷积网络深度从40层左右突破到了150层左右，同样也带来了图像分类的精确度提升。CNN系列我只阅读了最老的LeNet和本论文这一最新的ResNet，后续因工作需要，将开始阅读物体检测相关论文。Deep Residual Learning for Image RecognitionKaiming He, Xiangyu Zhang"><meta name="keywords" content="Computer Vision,Deep Learning,Convolutional Network,Paper,Image Recognition,ResNet"><meta property="og:type" content="article"><meta property="og:title" content="ResNet"><meta property="og:url" content="http://muyaan.com/2018/07/22/ResNet/index.html"><meta property="og:site_name" content="慕湮"><meta property="og:description" content="本论文是我阅读的第二篇深度学习相关论文，2015年提出的ResNet让卷积网络深度从40层左右突破到了150层左右，同样也带来了图像分类的精确度提升。CNN系列我只阅读了最老的LeNet和本论文这一最新的ResNet，后续因工作需要，将开始阅读物体检测相关论文。Deep Residual Learning for Image RecognitionKaiming He, Xiangyu Zhang"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="http://muyaan.com/2018/07/22/ResNet/1531877507263.png"><meta property="og:image" content="http://muyaan.com/2018/07/22/ResNet/1531880756568.png"><meta property="og:image" content="http://muyaan.com/2018/07/22/ResNet/1531898711144.png"><meta property="og:image" content="http://muyaan.com/2018/07/22/ResNet/1531900613404.png"><meta property="og:image" content="http://muyaan.com/2018/07/22/ResNet/1531900650087.png"><meta property="og:image" content="http://muyaan.com/2018/07/22/ResNet/1531900719425.png"><meta property="og:image" content="http://muyaan.com/2018/07/22/ResNet/1531901596651.png"><meta property="og:image" content="http://muyaan.com/2018/07/22/ResNet/1531901709574.png"><meta property="og:image" content="http://muyaan.com/2018/07/22/ResNet/1531901721857.png"><meta property="og:image" content="http://muyaan.com/2018/07/22/ResNet/1531902076734.png"><meta property="og:image" content="http://muyaan.com/2018/07/22/ResNet/1531905732182.png"><meta property="og:image" content="http://muyaan.com/2018/07/22/ResNet/1531906829980.png"><meta property="og:image" content="http://muyaan.com/2018/07/22/ResNet/1531906843178.png"><meta property="og:image" content="http://muyaan.com/2018/07/22/ResNet/1531911630930.png"><meta property="og:updated_time" content="2018-09-12T03:22:25.020Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="ResNet"><meta name="twitter:description" content="本论文是我阅读的第二篇深度学习相关论文，2015年提出的ResNet让卷积网络深度从40层左右突破到了150层左右，同样也带来了图像分类的精确度提升。CNN系列我只阅读了最老的LeNet和本论文这一最新的ResNet，后续因工作需要，将开始阅读物体检测相关论文。Deep Residual Learning for Image RecognitionKaiming He, Xiangyu Zhang"><meta name="twitter:image" content="http://muyaan.com/2018/07/22/ResNet/1531877507263.png"><link rel="canonical" href="http://muyaan.com/2018/07/22/ResNet/"><script type="text/javascript" id="page.configurations">CONFIG.page={sidebar:""}</script><title>ResNet | 慕湮</title><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?ca5844321cfb80fdf6f12b4dcc326991";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style type="text/css">.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.logo-line-after i{right:initial}</style></noscript></head><body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">慕湮</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">白日放歌须纵酒</h1></div><div class="site-nav-toggle"><button aria-label="切换导航栏"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://muyaan.com/2018/07/22/ResNet/"><span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="慕湮"><meta itemprop="description" content=""><meta itemprop="image" content="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4"></span><span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="慕湮"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">ResNet</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2018-07-22 10:23:34" itemprop="dateCreated datePublished" datetime="2018-07-22T10:23:34+08:00">2018-07-22</time> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2018-09-12 11:22:25" itemprop="dateModified" datetime="2018-09-12T11:22:25+08:00">2018-09-12</time> </span><span id="/2018/07/22/ResNet/" class="leancloud_visitors" data-flag-title="ResNet"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数：</span> <span title="本文字数">10k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">19 分钟</span></div></div></header><div class="post-body" itemprop="articleBody"><p>本论文是我阅读的第二篇深度学习相关论文，2015年提出的ResNet让卷积网络深度从40层左右突破到了150层左右，同样也带来了图像分类的精确度提升。CNN系列我只阅读了最老的LeNet和本论文这一最新的ResNet，后续因工作需要，将开始阅读物体检测相关论文。</p><blockquote><p><a href="https://arxiv.org/abs/1512.03385" rel="external nofollow noopener noreferrer" target="_blank">Deep Residual Learning for Image Recognition</a><br>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun<br>Microsoft Research 2015</p></blockquote><h2><a href="#" class="headerlink"></a><a id="more"></a></h2><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><em>更深的神经网络更难训练。我们提出了一个残差学习框架，减轻比曾用过的深得多的网络的训练难度。我们明确地把layers重构为引用了自身输入的残差函数形式。我们提供了这些残差网络更容易优化，能从深度的增长中获得精确性的大量经验性证明。在ImageNet数据集上，我们测试了一个深达152层的残差网络——比VGG$^{[41]}$深8倍，还能有着更低的复杂性。这个残差网络的组合在ImageNet测试集上获得了3.57%的错误率。赢得了ILSVRC 2015的第一名。我们还呈现了在CIFAR-10上100层和1000层的分析</em><br><em>对于许多视觉识别任务来说，表达的深度是最重要的。仅仅因为我们表达极深，我们就在COCO对象检测集上获得了28%的相对提升。深度残差网络是我们在ILSVRC和COCO 2015竞赛提交模型的基础，我们获得了ImageNet detection, ImageNet localization, COCO detection和COCO segmentation的第一名。</em></p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>深度卷积神经网络$^{[22,21]}$引领了一系列图像分类上的突破$^{[21,50,40]}$。深度网络用多层、端对端的形式整合了低/中/高维的特征$^{[50]}$和分类器，特征的维度能通过增加深度来丰富。近期研究$^{[41,44]}$揭露了网络深度是最关键的，ImageNet数据集挑战$^{[36]}$上的领先结果$^{[41,44,13,16]}$都利用了“非常深”$^{[41]}$的模型，深度范围为16$^{[41]}$到30$^{[16]}$。很多其它不平凡的视觉识别任务$^{[8,12,7,32,27]}$也从很深的模型中收益。被深度的重要性所驱动，问题产生了：<em>是不是简单地堆叠层就能获得更佳的网络</em>。回答这一问题的一个障碍是臭名昭著的梯度消失/爆炸问题$^{[1,9]}$，它们从一开始就在阻碍模型的收敛。然而，这一问题已通过归一化$^{[23,9,37,13]}$和中间层归一化$^{[16]}$解决，使得数十层的网络能通过随机梯度下降SGD和反向传播BP$^{[22]}$收敛。</p><blockquote><p><img src="/2018/07/22/ResNet/1531877507263.png" alt="Alt text"><br>图1. CIFAR-10上20层和56层正常网络的训练和测试误差。更深的网络有着更高的训练和测试误差，ImageNet上同样的现象在图4中呈现</p></blockquote><p>随着更深的网络开始能收敛，<em>退化</em>问题就开始出现了：随着网络深度的增加，准确率逐渐饱和（这并不意外），然后很快退化。令人意外的是，这种退化不是<em>过拟合导致的</em>。就像$^{[11,42]}$报告的那样，为有着适当深度的模型增加更多的层会导致<em>更高的训练误差</em>，也被我们的实验彻底证明。图1就是一个典型的例子。<br>训练准确率的退化指出了不是所有系统都能轻松优化。让我们考虑一个更浅的架构和它的一个增加了许多层的更深的副本。有一个对更深模型的解决方案：新增的层都是一致映射，其它的层都复制自训练好的浅模型。这样的解决方案的存在说明一个更深的模型不应该产生比浅版更高的训练误差。然而实验显示目前我们的模型无法找到一样好甚至更佳的解。<br>本论文中，我们通过采用<em>深度残差网络</em>解决了退化问题。与其希望临近几层直接拟合到我们需要的那种映射，我们直接为它们安装了一个残差映射。把我们期望的映射记做$\mathcal{H}(x)$，我们让临近几层符合另一种映射$\mathcal{F(x)\ \colon =H(x)-x}$，期望的映射就能转换为$\mathcal{F(x)+x}$。我们猜测优化残差映射要比原映射简单。极端的说，如果一致性映射是最优的，把临近几层的残差推向0比拟合一个一致映射简单。<br><img src="/2018/07/22/ResNet/1531880756568.png" alt="Alt text"></p><p>方程$\mathcal{F(x)+x}$可通过带“短路”连接(图2)的前向传播神经网络实现。短路连接$^{[2,34,49]}$就是跳过一层或多层的连接。在这个例子里，短路连接只是简单的实现了一致映射，它们的输出添加到了这几层的输出中。一致性短路连接既没有增加额外的参数也没有增加计算复杂度。整个网络依然能通过SGD反向传播进行端到端的训练，也能轻松地用已有的库加以实现（如Caffe$^{[19]}$）。<br>我们通过ImageNet上广泛的实验，呈现了退化问题并评估了我们的方法。我们发现了</p><ol><li>我们极深的残差网络很容易优化，但对照的普通网络在深度增加时，呈现出了更高的训练误差。</li><li>我们的深度残差网络能从大量增加的深度中，轻松获得更高的准确率，产生比之前的网络优秀得多的结果</li></ol><p>CIFAR-10$^{[20]}$上也出现了同样的现象，说明了我们方法的优化难度和效果不仅仅只在特定的数据集上有用。我们在这个数据集上提出了超过100层的成功训练的模型，并探索了超过1000层的模型。<br>在ImageNet分类数据集上，我们通过极深的残差网络获得了卓越的结果。我们的152层残差网络是ImageNet上从未出现过的深度，还有着比VGG$^{[41]}$更低的复杂度。在ImageNet测试集上，我们的组合模型只有<strong>3.57%</strong>的top-5误差，并<em>赢得了ILSVRC 2015分类竞赛的第一名</em>。这极深的表达能力在其他识别任务上也有着卓越的通用表现，让我们获得了以下项目的第一名：<em>ImageNet Detection, ImageNet localization, COCO detection和COCO segmentation</em>。这说明了残差学习原理具有通用性，而且我们预测它在其他视觉和非视觉任务上都适用。</p><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p><strong>残差表达 Residual Representations。</strong>在图像识别中，<strong>VLAD</strong>$^{[18]}$是一种通过对残差向量使用字典编码的表达，而<strong>Fisher Vector</strong>$^{[30]}$可被变形为VLAD的一种概率性版本$^{[18]}$。它们对于图像检索和分类都是强有力的浅表达$^{[4,48]}$。对于向量量子化，对残差向量编码显得比对原始向量编码更有效。<br>在低级视觉和计算机图形中，为了解偏微分方程Partial Differential Equations(PDEs)，最常用的多重网格方法$^{[3]}$将系统重构为不同规模下的子问题，每个子问题都负责粗糙到细粒度规模的残差的解。多重网格的一个变体是基于多层的预处理Hierarchical basis preconditioning$^{[45,46]}$，依赖于用来表达两个规模之间的残差向量的变量。已被验证${[3,45,46]}$的是比起不关注解的残差特性的标准solver，这些solver收敛得快得多。这些方法说明一个好的的变形或预处理能简化优化过程。</p><p><strong>短路连接。</strong>通向短路连接的实验和理论$^{[2,34,49]}$已经持续了很长时间。一个早期实践是在训练多层感知机MLPs时，添加一个从输入到输出的线性层$^{[45,49]}$。在$^{[44,24]}$中，少数中间层直接连到辅助分类器上，来解决梯度消失/爆炸的问题。论文$[39,38,31,47]$中提出了方法来集中层的结果、梯度和传播误差，通过短路连接实现的。在$[44]$中，“inception”层就是由短路分支和一些更深的分支组成。<br>和我们的工作同时的“高速公路网络 highway networks”$^{[42,43]}$提出带有门函数$^{[15]}$的短路连接。这些门依赖于数据且有参数，与之相反我们的一致短路是没有参数的。当门关闭时（接近0），该层就演化为非残差函数。与其相反，我们的方程总是会学习残差函数；我们的一致短路连接始终开放，所有的信息都能通过，同时额外的残差函数能被学习。并且，高速公路网络没有展示在极大的深度增长时的精确度增长。</p><h2 id="3-Deep-Residual-Network"><a href="#3-Deep-Residual-Network" class="headerlink" title="3. Deep Residual Network"></a>3. Deep Residual Network</h2><h3 id="3-1-Residual-Learning"><a href="#3-1-Residual-Learning" class="headerlink" title="3.1 Residual Learning"></a>3.1 Residual Learning</h3><p>把几个相邻层需要拟合到的映射记做$\mathcal{H(x)}$，$x$代表第一层的输入。假如多个非线性层能渐进地拟合复杂函数$^2$，那么它们能渐进地拟合残差函数。如$\mathcal{H(x)-x}$（假定输入输出维度一致）。那么与其期望这几层能被训练拟合为$\mathcal{H(x)}$，我们明确地让它们拟合为$\mathcal{F(x)\ \colon =H(x)-x}$，因此原始函数变为$\mathcal {F(x)+x}$，尽管两种形式都能渐进地拟合需要的函数，学习的难易度也许会不同。<br>这一变形的想法来自于反直觉counterintuitive的退化问题现象。正如我们在introduction中讨论的那样，如果新增的层能被构造为一致映射，更深的模型不该有比浅的对照组的训练误差更高。退化问题暗示了solver也许通过多层非线性层拟合一致映射非常困难。而加上残差学习形式，如果一致映射是最优的，solver可以简单地让多层的权重趋近0来接近一致映射。<br>在实际情况中，一致映射几乎不可能是最优的，但我们这一变形可以是问题的前提。如果最优的函数更接近一致映射而不是零映射，solver找到基于一致映射的perturbation要比从头学习这一函数要简单。我们通过实验（图7）说明了训练的残差函数通常有更小的反应时间，暗示了一致映射提供了合理的预处理。</p><h3 id="3-2-Identity-Mapping-by-Shortcuts"><a href="#3-2-Identity-Mapping-by-Shortcuts" class="headerlink" title="3.2 Identity Mapping by Shortcuts"></a>3.2 Identity Mapping by Shortcuts</h3><p>对每几个连续的层我们采取一个残差学习。在图2中展示了这样的一个块。正式地说，这篇论文我们把一个块定义为：<script type="math/tex">\mathcal{y=F(x,\{W_i\})+x} \qquad \qquad (1)</script>这里$x$和$y$是这个块的输入、输出向量。函数$\mathcal{F(x,\{W_i\})}$是要学习的残差映射。在图2的例子里有两层，$\mathcal{F=W_2 \sigma(W_1x)}$，$\sigma$表ReLU函数$^{[29]}$而截距biases出于简介省略了。$\mathcal{F+x}$操作通过短路连接与元素间加法实现。我们在加法后采用了第二次非线性激活（如$\sigma(y)$，见图2）。<br>公式（1）中的短路连接没有引入额外的参数和计算量。这不仅仅在实际中很有吸引力，在我们对残差网络和普通网络进行比较时也很重要。我们能公平地比较有着同样数量的参数、深度、宽度和计算量（除了微不足道的元素间加法）的两种网络。<br>在公式（1）中的$x$和$\mathcal{F}$的维度必须相等。当这一条件不满足的时候，我们可以在短路连接使用一个线性投影$\mathcal{W_s}$来匹配它们的维度：<script type="math/tex">\mathcal{y=F(x,\{W_i\})+W_sx} \qquad \qquad (2)</script>我们也能在公式（1）中使用方阵$\mathcal{W_s}$，但我们会通过实验表明，一致映射对退化问题的有效性和它的经济性，因此$\mathcal{W_s}$仅用于匹配维度。<br>残差函数$\mathcal{F}$的形式很灵活。本论文的实验涉及有着两层、三层的$\mathcal{F}$（图5），而更多的层也是可能的。但如果$\mathcal{F}$只有一层，公式（1）就变成了线性层：$\mathcal{y=W_1x+x}$，我们还没有发现它的优势。<br>值得说明的是尽管上方的定义都是针对全连接层的——出于简单，对卷积层也是可行的。函数$\mathcal{F(x,\{W_i\})}$能代表多个卷积层。元素间加法是在两个各自的feature map上进行，channel by channel地进行。</p><h3 id="3-3-Network-Architectures"><a href="#3-3-Network-Architectures" class="headerlink" title="3.3 Network Architectures"></a>3.3 Network Architectures</h3><p>我们测试了各种各样的普通/残差网络，观察到了一致的现象。为了给讨论提供对象，我们将介绍两个ImageNet的模型。</p><p><strong>Plain Network</strong>我们的普通网络基线（图3中间）主要由VGG的原理所启示$^{[41]}$。卷积层主要是3*3的核并有两个简单的设计原则：</p><ol><li>有着同样输出feature map大小的层，有着相同数量的核</li><li>如果feature map大小减半，核的数量翻倍来保持每层的时间复杂度</li></ol><p>我们在卷积层后跟一个步长为2的下采样层。网络结束于一个平均池层和一个1000路softmax全连接层。图3中的带参数层数量为34。<br>值得注意的是，比起VGG我们的模型有更少的核和更低的复杂度。我们的34层基准网络有36亿次FLOPs（floating-point operations per second）每秒浮点运算次数，只有VGG的18%（198亿FLOPs）。<br>WX20180718-152432.png<img src="/2018/07/22/ResNet/1531898711144.png" alt="Alt text"></p><p><strong>Residual Network</strong> 基于上方的普通网络，我们插入短路连接将它变成残差版本。一致性短路能在输入输出维度相等时直接使用（图3中的实线短路连接）。当维度增加时（图3中的点线短路连接），我们有两个选择：</p><ol><li>依然使用一致短路连接，额外的维度用0填充。这一方案不会引入额外参数；</li><li>使用公式（2）的连接来匹配维度（通过1*1卷积完成）</li></ol><p>两种选择中，短路连接的步长都是2。</p><h3 id="3-4-Implementation"><a href="#3-4-Implementation" class="headerlink" title="3.4 Implementation"></a>3.4 Implementation</h3><p>我们的实现参照了之前的成果$^{[21,41]}$。图片的短边被随机重设大小到[256,480]来实现比例放大$^{[41]}$。再从图片和它的水平镜像中随机截取224*224，并对每个像素减去平均值$^{[21]}$。还使用了标准颜色扩充$^{[21]}$。我们在每次卷积后和激活前使用BN$^{[16]}$。我们像[13]一样初始化权重并训练。我们使用SGD，每批大小为256。学习率从0.1开始并在进入错误率平台期时除以10，模型训练了$60 \times 10^4 $次迭代。权重衰减率weight decay为0.001，动量momentum为0.9。我们依据[16]的实践，没有使用dropout$^{[14]}$。<br>在测试过程中，对于用于比较的，我们采用了标准10分测试 10-crop testing$^{[21]}$。为了得到最好的结果，我们采用了[41,14]那样的全卷积形式，并在不同的规模计算平均得分（图片短边缩放到{224,256,384,480,640}）。</p><h2 id="4-Experiment"><a href="#4-Experiment" class="headerlink" title="4. Experiment"></a>4. Experiment</h2><h3 id="4-1-ImageNet-Classification"><a href="#4-1-ImageNet-Classification" class="headerlink" title="4.1 ImageNet Classification"></a>4.1 ImageNet Classification</h3><p>我们使用有1000个类的ImageNet 2012 classification 数据集$^{[36]}$评估我们的方法。模型使用128万张训练图片训练，在5万张验证集图片上测试。我们还从服务器上得到了10万张测试图片的最终结果。top-1和top-5错误率都领先。<br><img src="/2018/07/22/ResNet/1531900613404.png" alt="Alt text"><br><img src="/2018/07/22/ResNet/1531900650087.png" alt="Alt text"></p><p><strong>Plain Networks</strong> 我们先评估了18和34层的普通网络。表1是详细结构。<br>表2是结果，显示深的34层网络比18层网络的验证误差要高。为了揭露原因，图4的左边我们比较了它们随着训练的进行，各自训练、验证误差。我们能观察到退化现象——34层的训练误差始终比18层的高，尽管18层只是34层的一个子空间。<br>我们认为优化的难度不像是梯度消失带来的。网络是使用了BN$^{[16]}$训练的，保证前向传播信号有着非0的差。我们也确认了反向传播的梯度表现为正常的BN标准。所以前向和后向的信号都没有消失。实际上34层网络仍然能够获得不错的准确度（表3），表明solver一定程度是有效的。我们推测深的普通网络也许有着指数级的收敛速率，影响了训练误差的降低$^3$。未来会研究这一优化难的问题。<br><img src="/2018/07/22/ResNet/1531900719425.png" alt="Alt text"></p><p><strong>Residual Networks</strong> 接下来我们评估了18层和34层残差网络（<em>ResNet</em>）。同之前的普通网络的基础架构一致，除了像图3一样，每两个3*3卷积添加一个短路连接。我们使用了一致映射因此没有额外的参数。<br>我们从表2和图4中得到三个主要观测结果：</p><ol><li>情况反了过来，更深的残差网络要更好（2.8%）。更重要的是，更深的网络有着低得多的训练误差，还能泛化到验证集上。这说明退化问题在这一设置中被很好地得到了解决，并且我们能从深度的增长中获得准确率。</li><li>比起普通的网络，34层的top-1错误率降低了3.5%（表2），训练集错误率降低得更多（图4左右对比）。这一比较验证了在极深系统中残差学习的有效性。</li><li>我们还注意到18层的普通、残差网络有着近似的准确率（表2），但18层残差网络收敛得更快（图4左右对比）。当网络没有“过深”时，目前的SGD solver能在普通网络中找到不错的解。在这种情况下，ResNet通过在早期更快地收敛来简化了优化。</li></ol><p><img src="/2018/07/22/ResNet/1531901596651.png" alt="Alt text"><br><img src="/2018/07/22/ResNet/1531901709574.png" alt="Alt text"><br><img src="/2018/07/22/ResNet/1531901721857.png" alt="Alt text"></p><p><strong>Identity vs. Projection Shortcuts</strong> 我们已经演示了无参的、一致短路能帮助训练。接下来我们探索了投影短路，即方程（2）。在表3中我们比较了3个选项：</p><ul><li>A 填充的短路用于增长的维度，所有的短路都是无参的（跟表2和图4右一样）。</li><li>B 投影短路用于维度增长，其余使用一致短路</li><li>C 所有短路都用投影</li></ul><p>表格3说明三种选项都比普通网络优秀。B略优于A。我们认为这是因为用零填充的维度实际上没有残差学习。C稍稍好于B，我们认为这是因为额外引入的投影短路（13条）的参数导致的。但三种选项的结果差距的细微说明投影短路不是解决退化问题的主要功臣。因此我们在论文后续部分不再使用C，来降低内存和时间复杂度，减少模型大小。接下来也会介绍无参的一致连接对于不增加瓶颈结构复杂度的极度重要性。<br><img src="/2018/07/22/ResNet/1531902076734.png" alt="Alt text"></p><p><strong>Deeper Bottleneck Architectures</strong> 接下来我们将介绍用于ImageNet的更深的网络。考虑到我们能承受的训练时间，我们把基础块修改为<em>瓶颈</em>设计。对每个残差函数$\mathcal{F}$，我们使用3层的堆叠而不是2（图5）。三层分别是$1 \times 1$，$3 \times 3$和$1 \times 1$的卷积核，其中$1 \times 1$的层用于降低并恢复维度，让$3 \times 3$这一层作为瓶颈，有着更少的输入输出维度。图5是一个例子，两种设计的时间复杂度相似。<br>无参的一致短路连接对于瓶颈结构非常重要。如果图5右的无参短路替换为投影，能看出时间复杂度和模型大小将翻倍，因为短路连接了两个高维的端。因此一致映射为瓶颈设计带来了更高效的模型。</p><p><strong>50-layer ResNet：</strong>我们将34层的模型中的两层基础块替换为三层瓶颈块，得到50层ResNet（表1）。对于维度增长我们使用了B选项。模型有38亿次浮点数计算。</p><p><strong>101-layer and 152-layer ResNets：</strong> 我们使用更多的三层块得到了101/152层ResNet。卓越之处在于深度增长如此之大，152层的ResNet（113亿FLOPs）仍比VGG-16/19（153/196亿FLOPs）复杂度低。<br>50/101/152层ResNet相比34层精确度有着相当程度的增加。我们没有发现退化问题因此从深度的增长中获得了准确度增长。在所有度量中，深度的好处都得到了证明（表3、4）。</p><p><strong>与最新方法的比较。</strong>在表4中我们与目前最好单模型结果进行了比较。我们的34层ResNet就有着不错的表现。152层ResNet的单模型top-5错误率为4.49%。单模型超越了之前所有组合的结果。我们组合了6个不同深度的模型（提交时只用了两个152层模型）。得到了测试集上3.57的top-5错误率（表5），赢得了ILSVRC 2015的第一名。</p><h3 id="4-2-CIFAR-10-and-Analysis"><a href="#4-2-CIFAR-10-and-Analysis" class="headerlink" title="4.2 CIFAR-10 and Analysis"></a>4.2 CIFAR-10 and Analysis</h3><p>我们在CIFAR-10$^{[20]}$上进行了更多研究，它有5万张训练集，10万张测试集，分为10类。我们的关注点在于极深网络的表现，而不是提升目前最好成绩，因此我们有意地使用了如下的简单结构。<br>普通/残差网络结构遵循图3中的形式。网络输入为$32 \times 32$的图片，像素减去了其平均值。第一层为$3 \times 3$卷积。接下来我们用了$6n$层$3 \times 3$的卷积，feature map大小分别为{32, 16, 8}，每一种feature map大小有$2n$层。对应核数量为{16, 32, 64}。卷积的下采样步长为2。网络结束于一个平均池化层，一个10路全连接层和一个softmax层。下面的表总结了它的结构：<br>| output map size | 32 * 32 | 16 * 16 | 8 * 8 |<br>| :———— | ————:| :—: ||<br>| # layers | 1+2n | 2n |2n|<br>| # filters|16 | 32 | 64 |</p><p>使用短路连接时，它们连接到每对$3 \times 3$卷积层（共2n个短路）。在这个数据集上我们全部使用一致短路（选择A），因此和普通网络有着同样数量的参数、深度、宽度和计算量。<br>我们设定权重衰减为0.0001动量为0.9，采用了[13]中的初始化和BN，没有采用dropout。模型在两块GPU上，mini-batch大小为128进行训练。初始lr为0.1，在32000和48000次迭代时除以10，在64000次迭代时结束，取决于45000/5000的训练/测试集的划分。我们用了[24]中的简单数据扩展方法：每边填充4个像素，随机在扩展后的图片或其翻转上截取32*32。测试时，只用原始的32*32图片。<br><img src="/2018/07/22/ResNet/1531905732182.png" alt="Alt text"></p><p>我们比较了$n=\{3,5,7,9\}$的情况，分别是20，32，44和56层的网络。图6的左边是普通网络的表现。更深的普通网络遭受了深度的增加，测试误差降低了。现象类似于ImageNet和MNIST上，说明这一优化问题是根本问题。<br>图6中间显示了ResNet的表现。也类似于ImageNet中的情况，我们的ResNet能克服优化难的问题，在深度增加时，准确度也增长了。<br>我们还探索了n = 18也就是110层ResNet。这一情况下我们发现初始lr有点过大，开始拟合了。因此我们用0.01来进行热身，知道训练错误率低于80%（400次迭代）后，回到0.1的lr并继续训练。其余的同之前一样。110层的ResNet收敛得不错，相比其它网络如FitNet$^{[35]}$和Highway$^{[42]}$参数更少，结果同样优秀（6.43%，表6）</p><p><img src="/2018/07/22/ResNet/1531906829980.png" alt="Alt text"><br><img src="/2018/07/22/ResNet/1531906843178.png" alt="Alt text"></p><p><strong>Analysis of Layer Responses.</strong> 图7展示了层的response的标准差。层的response是每个3*3层的输出，BN之后和其余非线性处理前（ReLU/addition）。这一分析揭示了残差函数输出的strength。图7显示出ResNet比起普通对照组有通常更小的response。这一结果也支持了我们3.1的猜想，即残差函数通常比非残差函数更接近0。我们同样注意到，更深的ResNet的量级magnitude更小。也就是说层数越多，ResNet中的单层更趋向于将信号减少。</p><p><strong>Exploring Over 1000 layers</strong><br>我们探索了一个深达1000层的模型。设置n=200得到一个1202层的网络，训练方式同上。我们的方法仍然没有优化困难，这一$10^3$层的网络能达到低于0.1%的训练误差（图6右）。它的测试误差仍然相当不错（7.93%，表6）。<br>但该模型仍有未决的问题，尽管它们的训练误差相似，但测试误差比我们的110层要差。我们认为这是过拟合的问题。1202层网络对于这个小数据集来说可能过于大了。在这个数据集上，强的正则化如maxout$^{[10]}$或dropout$^{[14]}$可以用于得到最佳结果$^{[10,25,24,35]}$。在本论文中，为了我们的主要目标不被分心，我们没有用maxout和dropout，仅仅通过设计上的深和厚来进行正则化。<br><img src="/2018/07/22/ResNet/1531911630930.png" alt="Alt text"></p><h3 id="4-3-Object-Detection-on-PASCAL-and-MS-COCO"><a href="#4-3-Object-Detection-on-PASCAL-and-MS-COCO" class="headerlink" title="4.3 Object Detection on PASCAL and MS COCO"></a>4.3 Object Detection on PASCAL and MS COCO</h3><p>我们的方法在其它识别任务上也有很好的普遍性表现。表7、8展示了对PASCAL VOC 2007、2012和COCO的物体检测baseline结果。我们采用<em>Faster R-CNN</em>$^{[32]}$作为检测方法。我们将VGG-16替换为ResNet-101，提升很不错。两种模型检测方法的实现（见附录）都一样，因此提升归功于更好的网络。在COCO上，我们的标准度量（mAP@[.5.95]）提升了6.0%，28%的相对提升。</p><h2 id="A-Object-Detection-Baselines"><a href="#A-Object-Detection-Baselines" class="headerlink" title="A. Object Detection Baselines"></a>A. Object Detection Baselines</h2><p>学习一定检测算法后看<br>本节将介绍我们基于Faster R-CNN的检测系统。模型通过ImageNet分类模型初始化，并在物体检测数据上调优。在ILSVRC和COCO 2015竞赛时我们实验了ResNet-50/101。<br>不像VGG-16，我们没有hidden全连接层。</p></div><div class="popular-posts-header">相关文章</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="/2018/07/21/LeNet/" rel="bookmark">LeNet</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/2018/09/06/Aligned-ReID/" rel="bookmark">Aligned ReID</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/2018/08/03/YOLO/" rel="bookmark">YOLO</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/2018/08/09/VGGNet/" rel="bookmark">VGGNet</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/2018/07/28/Fast-R-CNN/" rel="bookmark">Fast R-CNN</a></div></li></ul><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>慕湮</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="http://muyaan.com/2018/07/22/ResNet/" title="ResNet">http://muyaan.com/2018/07/22/ResNet/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noopener noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Computer-Vision/" rel="tag"># Computer Vision</a> <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a> <a href="/tags/Convolutional-Network/" rel="tag"># Convolutional Network</a> <a href="/tags/Paper/" rel="tag"># Paper</a> <a href="/tags/Image-Recognition/" rel="tag"># Image Recognition</a> <a href="/tags/ResNet/" rel="tag"># ResNet</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2018/07/21/LeNet/" rel="next" title="LeNet"><i class="fa fa-chevron-left"></i> LeNet</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2018/07/26/R-CNN/" rel="prev" title="R-CNN">R-CNN <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4" alt="慕湮"><p class="site-author-name" itemprop="name">慕湮</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">21</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">30</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/tycallen" target="_blank" title="GitHub" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:tyc.allen@gmail.com" target="_blank" title="E-Mail" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-envelope"></i>E-Mail</a> </span><span class="links-of-author-item"><a href="http://weibo.com/pojunallen" target="_blank" title="微博" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-weibo"></i>微博</a> </span><span class="links-of-author-item"><a href="https://tuchong.com/1070837" target="_blank" title="图虫" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-globe"></i>图虫</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-block"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> 友情链接</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="http://dotrabbit.tk" title="dotrabbit" target="_blank" rel="external nofollow noopener noreferrer">dotrabbit</a></li></ul></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text"></span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Introduction"><span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Related-Work"><span class="nav-text">2. Related Work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Deep-Residual-Network"><span class="nav-text">3. Deep Residual Network</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Residual-Learning"><span class="nav-text">3.1 Residual Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-Identity-Mapping-by-Shortcuts"><span class="nav-text">3.2 Identity Mapping by Shortcuts</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-Network-Architectures"><span class="nav-text">3.3 Network Architectures</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-Implementation"><span class="nav-text">3.4 Implementation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Experiment"><span class="nav-text">4. Experiment</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-ImageNet-Classification"><span class="nav-text">4.1 ImageNet Classification</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-CIFAR-10-and-Analysis"><span class="nav-text">4.2 CIFAR-10 and Analysis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-Object-Detection-on-PASCAL-and-MS-COCO"><span class="nav-text">4.3 Object Detection on PASCAL and MS COCO</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#A-Object-Detection-Baselines"><span class="nav-text">A. Object Detection Baselines</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; 2015 – <span itemprop="copyrightYear">2018</span> <span class="with-love" id="animate"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">慕湮</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="站点总字数">312k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">9:27</span></div><div class="powered-by">由 <a class="theme-link" target="_blank" rel="external nofollow noopener noreferrer" href="https://hexo.io">Hexo</a> 强力驱动</div><span class="post-meta-divider">|</span><div class="theme-info">主题 – <a class="theme-link" target="_blank" rel="external nofollow noopener noreferrer" href="https://theme-next.org">NexT.Muse</a></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="//cdn.staticfile.org/jquery/2.1.3/jquery.min.js"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="//cdn.staticfile.org/velocity/1.2.1/velocity.min.js"></script><script type="text/javascript" src="//cdn.staticfile.org/velocity/1.2.1/velocity.ui.min.js"></script><script type="text/javascript" src="//cdn.staticfile.org/fancybox/2.1.5/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=6.4.1"></script><script type="text/javascript" src="/js/src/motion.js?v=6.4.1"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=6.4.1"></script><script type="text/javascript" src="/js/src/post-details.js?v=6.4.1"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.1"></script><script>function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();

      Counter('get', '/classes/Counter', { where: JSON.stringify({ url }) })
        .done(function ({ results }) {
          if (results.length > 0) {
            var counter = results[0];
            
              var $element = $(document.getElementById(url));
              $element.find('.leancloud-visitors-count').text(counter.time + 1);
            
            Counter('put', `/classes/Counter/${counter.objectId}`, JSON.stringify({ time: { "__op":"Increment", "amount":1 } }))
            
            .fail(function ({ responseJSON }) {
                console.log('Failed to save Visitor num, with error message: ' + responseJSON.error);
            })
          } else {
            
              Counter('post', '/classes/Counter', JSON.stringify({ title: title, url: url, time: 1}))
                .done(function () {
                  var $element = $(document.getElementById(url));
                  $element.find('.leancloud-visitors-count').text(1);
                })
                .fail(function () {
                  console.log('Failed to create');
                });
            
          }
        })
      .fail(function ({ responseJSON }) {
        console.log('LeanCloud Counter Error:' + responseJSON.code + " " + responseJSON.error);
      });
    }
    

    $(function() {
      $.get('https://app-router.leancloud.cn/2/route?appId=' + "UWGXBMGbguCDhSCbC3NrUQtL-gzGzoHsz")
        .done(function ({ api_server }) {
          var Counter = function (method, url, data) {
            return $.ajax({
              method: method,
              url: `https://${api_server}/1.1${url}`,
              headers: {
                'X-LC-Id': "UWGXBMGbguCDhSCbC3NrUQtL-gzGzoHsz",
                'X-LC-Key': "ke1jrA5b6VyR89Kqqqwf2kPP",
                'Content-Type': 'application/json',
              },
              data: data,
            });
          };
          
          addCount(Counter);
          
        })
    });</script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="http://muyaan.com/js/src/async.js"></script></body></html>