<!DOCTYPE html><html class="theme-next muse use-motion" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><script src="https://cdn.staticfile.org/pace/1.0.2/pace.min.js"></script><link href="https://cdn.staticfile.org/pace/1.0.2/themes/blue/pace-theme-big-counter.min.css" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="//cdn.staticfile.org/font-awesome/4.6.2/css/font-awesome.min.css" rel="stylesheet" type="text/css"><link href="/css/main.css?v=6.3.0" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.3.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.3.0"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.3.0"><link rel="mask-icon" href="/images/logo.svg?v=6.3.0" color="#222"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Muse",version:"6.3.0",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!1,fastclick:!1,lazyload:!1,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="description" content="YOLOv2论文翻译。它提出了一个升级版的YOLO，以及利用分类数据集提升检测模型检测类别范围的方法，和一个使用该方法训练出的模型：YOLO9000。  YOLO9000: Better, Faster, StrongerJoseph Redmon, Ali Farhadi, 2016University of Washington, Allen Institute for AI"><meta name="keywords" content="Computer Vision,Deep Learning,Paper,Object Detection,YOLO,YOLOv2,YOLO9000"><meta property="og:type" content="article"><meta property="og:title" content="YOLOv2 and YOLO9000"><meta property="og:url" content="http://muyaan.com/archives/423509a9.html"><meta property="og:site_name" content="慕湮"><meta property="og:description" content="YOLOv2论文翻译。它提出了一个升级版的YOLO，以及利用分类数据集提升检测模型检测类别范围的方法，和一个使用该方法训练出的模型：YOLO9000。  YOLO9000: Better, Faster, StrongerJoseph Redmon, Ali Farhadi, 2016University of Washington, Allen Institute for AI"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://muyaan.com/archives/1533899007916.png"><meta property="og:image" content="http://muyaan.com/archives/1533819658440.png"><meta property="og:image" content="http://muyaan.com/archives/1533887862915.png"><meta property="og:image" content="http://muyaan.com/archives/1533893529616.png"><meta property="og:image" content="http://muyaan.com/archives/1533893496455.png"><meta property="og:image" content="http://muyaan.com/archives/1533899031740.png"><meta property="og:image" content="http://muyaan.com/archives/1533991952869.png"><meta property="og:image" content="http://muyaan.com/archives/1533992067219.png"><meta property="og:image" content="http://muyaan.com/archives/1534040450942.png"><meta property="og:image" content="http://muyaan.com/archives/1534041231161.png"><meta property="og:image" content="http://muyaan.com/archives/1533802901317.png"><meta property="og:image" content="http://muyaan.com/archives/1533893474011.png"><meta property="og:image" content="http://muyaan.com/archives/1534041247166.png"><meta property="og:updated_time" content="2018-09-12T03:22:25.035Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="YOLOv2 and YOLO9000"><meta name="twitter:description" content="YOLOv2论文翻译。它提出了一个升级版的YOLO，以及利用分类数据集提升检测模型检测类别范围的方法，和一个使用该方法训练出的模型：YOLO9000。  YOLO9000: Better, Faster, StrongerJoseph Redmon, Ali Farhadi, 2016University of Washington, Allen Institute for AI"><meta name="twitter:image" content="http://muyaan.com/archives/1533899007916.png"><link rel="canonical" href="http://muyaan.com/archives/423509a9.html"><script type="text/javascript" id="page.configurations">CONFIG.page={sidebar:""}</script><meta name="google-site-verification" content="Ojd_HrL_PelaXvKK5IkbhLbjZ_sHt6IxRzP-XPaaTw4"><meta name="baidu-site-verification" content="skdT7mIEAb"><title>YOLOv2 and YOLO9000 | 慕湮</title><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?ca5844321cfb80fdf6f12b4dcc326991";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style type="text/css">.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.logo-line-after i{right:initial}</style></noscript></head><body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">慕湮</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">白日放歌须纵酒</h1></div><div class="site-nav-toggle"><button aria-label="Toggle navigation bar"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-archives menu-item-active"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li></ul></nav><script type="text/javascript" color="99,184,255" opacity="0.5" zindex="-2" count="99" src="//cdn.staticfile.org/canvas-nest.js/1.0.0/canvas-nest.js"></script></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><div class="reading-progress-bar"></div><article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://muyaan.com/archives/423509a9.html"><span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="慕湮"><meta itemprop="description" content=""><meta itemprop="image" content="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4"></span><span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="慕湮"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">YOLOv2 and YOLO9000</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2018-08-12 21:08:19" itemprop="dateCreated datePublished" datetime="2018-08-12T21:08:19+08:00">2018-08-12</time> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">Edited on</span> <time title="Modified: 2018-09-12 11:22:25" itemprop="dateModified" datetime="2018-09-12T11:22:25+08:00">2018-09-12</time> </span><span id="/archives/423509a9.html" class="leancloud_visitors" data-flag-title="YOLOv2 and YOLO9000"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Views: </span><span class="leancloud-visitors-count"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数: </span><span title="Symbols count in article">13k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="Reading time">23 mins.</span></div></div></header><div class="post-body" itemprop="articleBody"><p>YOLOv2论文翻译。它提出了一个升级版的YOLO，以及利用分类数据集提升检测模型检测类别范围的方法，和一个使用该方法训练出的模型：YOLO9000。</p><blockquote><p><a href="https://arxiv.org/abs/1612.08242" rel="external nofollow noopener noreferrer" target="_blank">YOLO9000: Better, Faster, Stronger</a><br>Joseph Redmon, Ali Farhadi, 2016<br>University of Washington, Allen Institute for AI</p></blockquote><h2><a href="#" class="headerlink"></a><a id="more"></a></h2><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>通用的物体检测应该能快速、准确、能识别许多种物体。自从神经网络的引入，检测框架变得非常快速、准确。但大部分检测方法所能检测的仅仅是一小部分物体。</p><p>当前的物体检测数据集相比其它数据集，如分类和标签，相当有限。常见检测数据集包含数百万或数十万图片，有着数十到数百个标签$^{[3,10,2]}$。而分类数据集多为数百万张图片，多达数万或数十万种类$^{[20,2]}$。</p><p>我们非常希望检测也能达到分类数据集的尺度。但是，用于检测的图片标注要比用于分类或标签的昂贵得多（标签通常由用户免费贡献）。因此我们不太可能在近期看到检测数据集尺度接近分类数据集。</p><p>我们提出了一种利用已有的大量分类数据并用其扩展当前检测系统范围的方法。我们的方法使用了一种物体分类的分层视野（hierarchical view），让我们能将有区别的数据集合并起来。</p><p>我们还提出了一种联合训练算法，能在检测和分类数据集上训练物体检测器。我们的方法杠杆式地利用带标注的检测图片来训练其精确定位物体的能力，同时利用分类图片来增加其词汇量和健壮性。</p><p>我们使用这一方法训练了YOLO9000，一个能实时检测超过9000种物体的检测器。首先我们在YOLO检测系统的基础上加以提升，得到YOLOv2，一个前沿的实时检测器。接着我们使用数据集结合方法和联合训练算法在COCO检测集和ImageNet超过9000类的数据集上训练这个模型。</p><p>我们所有代码和预训练的模型已<a href="http://pjreddie.com/yolo9000/" rel="external nofollow noopener noreferrer" target="_blank">开源</a>。</p><h2 id="2-Better"><a href="#2-Better" class="headerlink" title="2. Better"></a>2. Better</h2><p>YOLO相比前沿检测系统，有许多短处。YOLO与FRCN的错误分析相比显示出YOLO出现了很多定位错误。此外，与基于候选区域的方法相比，YOLO的召回率相对低。因此我们专注于提高召回率和定位，同时维持分类准确率。</p><p>计算机视觉通常倾向于更大、更深的网络$^{[6,18,17]}$。更高的性能常常依靠训练更大的网络或组合多个模型。但对于YOLOv2，我们希望它是一个又精确又快速的检测器。我们没有放大网络，而是简化了网络让其表达更易训练。我们从过去的研究中的许多想法与我们自己的新概念联合起来提升YOLO的性能。表2为结果总结。</p><p><img src="/archives/./1533899007916.png" alt="表2"></p><p><strong>Batch Normalization.</strong> BN能显著提升收敛性，同时排除对其余正则化形式的需要$^{[7]}$。通过为YOLO的所有卷积层增加BN，我们得到了高于2%的mAP提升。BN也能正则化模型。有了BN后我们可以移除模型中的dropout，而不会导致过拟合。</p><p><strong>High Resolution Classifier.</strong> 所有前沿检测器都使用在ImageNet上预训练的分类器。从AlexNet开始，大部分分类器都在小于$256\times256$的输入上进行。原始YOLO在$224\times224$的图片上训练分类网络，并在物体检测时将分辨率提升到448。这意味着网络不得不同时学习物体检测和适应新的输入分辨率。</p><p>在YOLOv2时我们首先使用$448\times448$分辨率图片来调优分类网络10个epoche。这给与了网络调优其核在更高分辨率输入时性能的时间。接着我们调优网络的检测性能。这一高分辨率分类网络为我们增加了近4%的mAP。</p><p><strong>Convolutional With Anchor Boxes.</strong> YOLO直接使用在卷积特征抽取器上的全连接层来预测限位框坐标。而Faster R-CNN没有直接预测坐标，它们使用人工选择的priors来预测限位框。Faster R-CNN使用RPN的卷积网络来为锚点框预测偏移和信心。因为进行预测的是卷积层，RPN在特征图上的每个位置预测这些偏移。预测偏移而不是坐标简化了问题，让网络更容易学习。</p><p>我们移除了YOLO中的全连接层，并用锚点框来预测限位框。我们首先移除了一个池化层，让卷积层的输出分辨率更高。我们同样降低了输入，从448到416。我们需要一个奇数，使特征图中有单个中心点。物体，特别是大物体，倾向于占据图片中心，因此只有一个中心来预测这些物体要好过4个位置。YOLO的卷积层对图片以32的因子对图片进行下采样，因此使用416的图片，会得到$13\times13$的特征图。</p><p>随着我们迁移到锚点框机制，我们也将类预测机制从空间位置上解耦，为每个锚点框预测类和是否有物体。跟YOLO一样，是否有物体依然是预测gt框和锚点框的IoU，类预测则是预测那儿是否有给定类的物体的条件概率。</p><p>使用锚点框让我们的准确度有小下降。YOLO只为每个图片预测98个框，而使用锚点框后，模型预测超过1000个框。移除锚点框后，我们的中间态模型mAP为69.5，召回率为81。加入锚点框后mAP为69.2，召回率为88%。尽管mAP下降了，但召回率的提升说明我们的模型仍有提升空间。</p><p><strong>Dimension Clusters.</strong> 在YOLO中使用锚点框时我们遭遇了两个问题。第一个是框的维度是手选的。网络能训练到适配它，但如果我们能选择更好的priors，使网络更易学习做出好的预测。</p><p>与其手选priors，我们在训练集限位框上运行了K-means聚类来自动寻找好的priors。如果我们使用标准的欧几里得距离K-means聚类，则更大的框的误差会更大。而我们期望priors能引出好的IoU得分，这与框大小无关。因此我们的测距方法是：</p><p>$d(box, centroid) = 1 - IoU(box,centroid)$</p><p>我们使用不同的k值运行k-means，并把有着平均IoU用最近的图心画出，见图2。我们选择k=5，作为模型复杂度和高召回率的一个好的权衡点。聚类中心点与手选锚点框区别巨大。它们中短而宽的更少，高而窄的更多。</p><p><img src="/archives/./1533819658440.png" alt="图2"></p><p>我们比较了我们的聚类策略下离最近聚类中心的平均IoU，以及手选锚点框，见表1。仅需5个聚类中心点便与9个锚点框的IoU近似。如果我们使用9个聚类中心点，则会看到一个高得多的平均IoU。这说明使用k-means聚类来生成我们的锚点框，会让我们的模型有更好的起始表达，更易训练。</p><p><img src="/archives/./1533887862915.png" alt="表1"></p><p><strong>Direct location prediction.</strong> 当将锚点框与YOLO结合时，我们遭遇了另一个问题：模型不稳定性，尤其是早期迭代中。大部分不稳定性来自于预测限位框的(x, y)坐标。在RPN网络中，它预测值$t_x$和$t_y$，(x, y)中心坐标可以如下计算：</p><script type="math/tex;mode=display">\begin{align}
x=(t_x*w_a)-x_a  \notag \\
y=(t_y*h_a)-y_a \notag \\
\end{align}</script><p>举个例子，预测值$t_x = 1$会将限位框向右平移一个锚点框宽度的距离，-1会向左平移同样距离。</p><p>这一方程是无条件限制的，所以任意锚点框能平移到图片中任意点，而不管它是在哪儿被预测的。这个模型使用随机初始化的话会花很长时间来稳定下来，预测合理的偏移。</p><p>与其预测偏移，我们按照YOLO的方法，预测相对格子的坐标。这让gt落入01之间。我们使用了逻辑激活函数，限制网络预测落入此范围。</p><p>网络在输出的特征图中每个格子预测5个限位框。为每个限位框预测5个坐标，$t_x,t_y,t_w,t_h,t_o$。如果该格子偏离图片左上角$(c_x,c_y)$且限位框prior的宽高为$p_w,p_h$，则预测为：</p><script type="math/tex;mode=display">\begin{align}
b_x &= \sigma(t_x) + c_x   \notag \\
b_y &= \sigma(t_y) + c_y \notag\\
b_w &= p_w e ^{t_w} \notag\\
b_h &= p_h e ^{t_h} \notag\\
P_r(object) * IoU(b, object)&=\sigma(t_o)\notag\\
\end{align}</script><p>因为我们约束了位置预测，使网络更易学习、更稳定。使用维度距离与直接预测限位框中心位置比使用锚点框的版本提升了5%的mAP。</p><p><strong>Fine-Grained Features.</strong> 这一YOLO修改版在一个$13\times13$的特征图上预测。尽管这对于大物体已足够，它也许能从更细纹理的特征中获利：更易定位小物体。Faster R-CNN和SSD都在它们网络中的多个特征图上运行候选网络，来得到更多的分辨率范围。我们使用了另一种方法，加入了一个转移passthrough层，将前一层的$26\times26$分辨率的特征图带过来。</p><p>转移层通过将临近特征堆叠到不同通道而不是空间位置，类似ResNet的一致性映射，将高低分辨率特征连接了起来。这将$26\times26\times512$的特征图变为$13\times13\times2048$的特征图，可以连接到最初的特征。我们的检测器在这一扩展特征图上运行，因此它能获取更细纹理的特征，这得到了接近1%的提升。</p><p><strong>Multi-Scale Training.</strong> YOLO输入分辨率为$448\times448$。加入锚点框后，我们将分辨率调整为$416\times416$。但因为我们的模型仅使用卷积层和池化层，它能实时调整大小。我们希望YOLOv2能在更多尺寸图片上健壮运行，因此我们将其训练入模型。</p><p>与其固定输入图片大小，我们每几次迭代后就改变网络。每10个batch我们的网络会随机选择一个新图片大小。因我们的模型下采样因子为32，我们从如下32的倍数中选择:{320,352,…,608}。因此最小选项是$320\times320$，最大为$608\times608$。我们调整网络到那个尺寸后继续训练。</p><p>这一制度强迫网络学习如何很好地预测各个输入尺寸。这意味着同一网络能在多个分辨率下预测。模型在尺寸较小时更快，因此YOLOv2提供了简便的速度、精度权衡方法。</p><p>在低分辨率时，YOLOv2表现为便宜而相对精确的检测器。在$288\times288$时帧率超过90，而mAP与FRCN一样好。这使其对小GPU、高帧率视频、多视频流应用非常理想。</p><p>在高分辨率时，YOLOv2是前沿的检测器，在VOC07上有78.6的mAP，同时仍保持实时以上的速度。见表3和图4，YOLOv2与其他框架在VOC2007进行了比较。</p><p><img src="/archives/./1533893529616.png" alt="表3"></p><p><img src="/archives/./1533893496455.png" alt="图4"></p><p><strong>Further Experiments.</strong> 我们训练了YOLOv2用于在VOC 2012上进行检测。表4显示了YOLOv2与其余前沿检测系统相比有相当的性能。YOLOv2达到了73.4的mAP，同时比其余模型更快。我们也在COCO上训练，并与其它方法比较，见表5。在VOC评价下（IoU=.5）YOLOv2得到了44.0的mAP，等于SSD和Faster R-CNN。</p><p><img src="/archives/./1533899031740.png" alt="表4"></p><p><img src="/archives/./1533991952869.png" alt="表5"></p><h2 id="3-Faster"><a href="#3-Faster" class="headerlink" title="3. Faster"></a>3. Faster</h2><p>我们既希望检测精确，又希望它快速。大多数检测应用，如机器人或自动驾驶，依赖于低延迟预测。为了最大化其性能，我们将YOLOv2设计为从头到尾的快。</p><p>大多数检测网络依赖VGG-16作为基本特征抽取器$^{[17]}$。VGG-16是一个强力、准确的分类网络，但它复杂得不必要。VGG-16<br>的卷积层前向处理一个$224\times224$的图片需要306.9亿次浮点数计算。</p><p>YOLO使用了一个基于GoogLeNet架构$^{[19]}$的自定义网络。它快过VGG-16，前向传播仅需85.2亿次浮点数运算。不过它的精确度略低于VGG-16。对于单切片，$224\times224$的top-5精确度，在ImageNet上YOLO自定义模型为88.0%，VGG-16为90.0%。</p><p><strong>Darknet-19.</strong> 我们提出了一个新的供YOLOv2使用的分类模型。我们的模型建于之前的网络设计工作和领域内共识。类似VGG模型，我们主要使用了$3\times3$卷积核，并在池化步骤后对通道数量加倍。按照NIN的研究我们使用了global average pooling，让预测像$1\times1$卷积核一样压缩$3\times3$卷积间的特征表达。我们使用了BN来稳定训练，加速收敛并正则化模型。</p><p>我们最终的模型称为Darknet-19，有19个卷积层和5个maxpooling层。表6为全面的描述。Darknet-19处理一张图片仅需55.8亿次计算，在ImageNet上仍有72.9%的top-1准确度和91.2%的top-5准确度的。</p><p><img src="/archives/./1533992067219.png" alt="表6"></p><p><strong>Training for classification.</strong> 我们在ImageNet标准1000类分类数据集上使用SGD训练了Darknet-19模型160个epoch，起始学习率为0.1，幂为4的正态学习率衰减，权重衰减为0.0005，动量为0.9。在训练中我们使用了标准数据增广技巧，如随机裁剪，旋转，hue，饱和度和曝光度偏移。</p><p>如上文讨论一样，在我们初始使用$224\times224$训练，然后使用更大尺寸的图片448来调优网络。调优时我们仍使用上述参数，但仅进行了10个epoch，且初始学习率为0.001。通过高分辨率的使用，我们网络的top-1准确率为76.5%，top-5为93.3%。</p><p><strong>Training for detection.</strong> 我们修改网络以进行检测：移除最后的全卷积层，并添加3个$3\times3$卷积层，每个有着1024个卷积核，最终一个$1\times1$卷积层，其输出数量与检测所需相关。对于VOC我们预测5个限位框，每个限位框有5个坐标和20类。因此共125个卷积核。我们为最后一个$3\times3\times5$卷积层到倒数第二个卷积层增加了一个passthrough层，让我们的模型能使用精细纹理特征。</p><p>我们使用初始学习率0.001训练了网络160个epoch，在第60和90个epoch除以10。权重衰减为0.0005，动量为0.9。我们使用了与YOLO和SSD相似的增广方法，如随机裁剪，颜色偏移等。我们在COCO和VOC上使用了同样的训练策略。</p><h2 id="4-Stronger"><a href="#4-Stronger" class="headerlink" title="4. Stronger"></a>4. Stronger</h2><p>我们提出了一个在分类和检测数据集上联合训练的机制。我们的方法使用带标注的检测数据来学习检测相关的信息，如限位框坐标预测和objectness以及分类常见物体。它仅使用只有类标注的图片来扩展其能检测的类别数量。</p><p>在训练时我们混合用于分类和检测的数据。当我们的网络发现一个带检测标注的图片时，我们能基于完整YOLOv2 loss函数进行反向传播。当它发现一个分类图片时，只是用来自架构中的分类相关loss进行反向传播。</p><p>这一方法提出了一些挑战。检测数据集仅有常见物体和大致标签，如狗或船。分类数据集有着广泛得多且细致的标签。ImageNet中有超过100种狗，如诺福克梗，约克郡犬和贝灵顿狗。如果我们要在两个数据集上训练，我们需要一个方法来合并这些标签。</p><p>大部分分类方法使用一个softmax层来对所有可能种类计算最终概率分布。使用softmax即假定这些类互斥。这为合并数据集造成了问题，你不会想用这一模型合并ImageNet和COCO，因为类“狗”和“诺福克梗”不是互斥的。</p><p>我们可以使用一个多标签模型来合并两个数据集，它不假定类互斥。这一方法忽略了我们关于数据所知的所有结构，如所有COCO类之间都是互斥的。</p><p><strong>Hierarchical classification.</strong> ImageNet标签来自WordNet，一个有着结构化概念和之间关联的语言数据库。在WordNet中，“诺福克梗”和“约克郡犬”都是“梗类犬”的下义词，一种“猎犬”，一种“狗”。大部分分类方法需要标签有着扁平化的结构，而合并数据库，这一种结构正是我们需要的。</p><p>WordNet以有向图而不是树的结构组织，因为语言的复杂性。比如一个“狗”，它既是一种“犬类”，又是一种“家畜”，都是“物体”的一个同义词组。与其使用完全的图结构，我们通过为ImageNet中的概念建立一个层级树来简化此问题。</p><p>为了建立此树，我们检查了ImageNet中的名词并查看了它们在WordNet图中到根节点的路径。许多同义词都只有一条路径，因此我们首先将所有这些路径加入我们的树中。接着我们检查剩余的概念，并选择树增长最小的路径加入。故如一概念有两个到根节点的路径，一个会加入三条边到我们的树中，而另一条仅增加一条，我们会选择更短的路径。</p><p>最终结果即是WordTree，一个视觉概念的层级模型。要使用WordTree来进行预测，我们需要在每个节点预测该同义词集的所有下义词基于此同义词集的条件概率。比如，对于“terrier”梗类犬节点我们进行如下预测：</p><script type="math/tex;mode=display">\begin{align}
P_r(Norfolk\   terrier\ &| \ terrier) \notag \\
P_r(Yorkshire\   terrier\ &| \ terrier) \notag \\
P_r(Bedlington\   terrier\ &| \ terrier) \notag \\
... \notag
\end{align}</script><p>如果我们想计算某个节点的绝对概率，我们仅需沿着到根节点的路径并将条件概率相乘。故如果我们想知道一个图片是不是“诺福克梗”Norfold terrier，我们计算：</p><script type="math/tex;mode=display">\begin{align}
P_r(Norfolk \ terrier) &\ = P_r(Norfolk \ terrier \ | \ terrier) \notag \\
*P_r(terrier \  &|\ hunting \ dog) \notag \\
*...&* \notag \\
*P_r(mammal \  &|\ animal) \notag \\
*P_r(animal \  &|\ physical \  object) \notag \\
\end{align}</script><p>对于分类任务我们假定图片包含一个物体：$P_r(physical\ object) = 1$。</p><p>为了验证这一方法我们在ImageNet 1000类数据集上训练了基于WordTree的Darknet-19模型。为了建立WordTree1k，我们增加的中间节点使标签空间从1000增大到1369。在训练中，gt标签会沿树向上传播，故如一张图被标记为“诺福克梗”，它也会被记做“狗”和“哺乳动物mammal”。为了计算条件概率，我们的模型预测了一个1369的向量，我们为属于某个概念的下义词的所有同义词集计算softmax，见图5。</p><p><img src="/archives/./1534040450942.png" alt="图5"></p><p>使用与之前一样的训练参数，我们的层级化Darknet-19获得了71.9%的top-1准确度和90.4%的top-5准确度。尽管增加了369个额外概念，而且让我们的网络预测一个树形结构，准确度仅仅下降少许。以这一方式进行分类也有好处。在新的或未知的物体类别时性能下降很温和。比如，如果网络碰到一张不确定种类的狗，它依然会以高信心预测“狗”和更低信心的展开预测。</p><p>这一方法对检测依然奏效。与其假定所有图片都有一个物体，我们使用YOLOv2的objectness预测器来给我们$P_r(physical\ object)$的值。检测器预测一个限位框和概率树。我们向下遍历概率树，在遇到分叉时选择高信心的路径，直到达到某个阈值，我们就预测那个物体类。</p><p><strong>Dataset combination with WordTree.</strong> 我们能用WordTree合理地合并多个数据集。我们简单地将数据集中的类别映射为树中的同义词集。图6是一个用WordTree合并来自ImageNet和COCO标签的例子。WordNet是如此多样，因此我们能将此技术应用于大部分数据集上。</p><p><img src="/archives/./1534041231161.png" alt="图6"></p><p><strong>Joint classification and detection.</strong> 现在我们能使用WordTree合并数据集，并联合分类和检测训练我们的模型。我们想要训练一个极大的检测器，因此我们使用COCO检查数据集和ImageNet的top9000类创建了我们自己的合并数据集。我们还需要评估我们的方法，因此所有ImageNet中未加入合并数据集的都引入了。对应的WordTree共9418个类。因为ImageNet是一个大得多的数据集，故我们通过对COCO过采样来平衡它们，让ImageNet仅大4倍。</p><p>我们使用这个数据集训练了YOLO9000。我们使用了YOLOv2基础架构，但仅3个而不是5个prior，以限制输出大小。当我们的网络碰见一个检测图片时，它正常进行反向传播。对于分类loss，我们仅在标签对应层级当层或之上进行loss的反向传播。如果标签为狗，我们不会为树下的预测赋予任何error，因为我们没有相关信息。</p><p>当它碰到分类图片时，我们仅对分类loss进行反向传播。我们找到预测该类有着最高概率的限位框，并在它的预测概率树上计算loss。我们也假定预测框应与gt框有至少0.3的IoU，并基于此假设反向传播objectness loss。</p><p>使用了联合训练后，YOLO9000学会了使用COCO的检测数据在图片中寻找物体，并学会了用ImageNet的数据辨认非常多种类的物体。</p><p>我们在ImageNet检测任务上评估了YOLO9000。这一任务与COCO有着44种相同的种类，这意味着对于YOLO9000来说，大部分测试图片它仅有分类的数据，而不是检测数据。YOLO9000在整体测试集中得到了19.7的mAP，在156种从未见过带标注的检测数据的物体类别的集合中得到16.0的mAP。这一mAP高于DPM所获得的，但YOLO9000是部分监督的在不同数据集上训练的。它同时能实时检测9000种其余物体类别。</p><p>当我们在ImageNet上分析YOLO9000的性能时，我们看到它对于新动物识别得不错，但对于衣物和装备比较挣扎。新动物容易训练，因为objectness预测容易从COCO中的动物泛化。但COCO没有任何衣物限位框标签，只有人，故YOLO9000对于如“墨镜”或“泳裤”非常挣扎。</p><h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5. Conclusion"></a>5. Conclusion</h2><p>我们提出了YOLOv2和YOLO9000，实时的检测系统。YOLOv2是一个前沿的检测系统，在许多检测数据集上都快过其余系统。而且它能在许多图片尺寸上使用，以在精确度和速度之前权衡。</p><p>YOLO9000是一个通过联合优化检测和分类的，能检测超过9000种物体的实时架构。我们使用WordTree来合并不同的数据源，并用我们的联合优化技术同时在ImageNet和COCO上训练。YOLO9000是一个拉近检测和分类数据集差距的强有力的一步。</p><p>我们许多技术能泛化于物体检测领域之外。我们的ImageNet的WordTree表达为图片分类提供了一个更丰富、更细节的输出空间。基于层级分类的数据集合并能用于分类和分割领域。训练技术，如多尺度训练，能为许多视觉任务提供帮助。</p><p>对于未来的研究，我们希望在弱监督图片分割上使用类似的技术。我们同样计划使用更强力的匹配技术，为训练时的分类数据赋予弱标签，以提升检测结果。计算机视觉受益于海量标记数据。我们会继续探索合并不同来源、结构的数据的方法，为视觉领域构建更强力的模型。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] S. Bell, C. L. Zitnick, K. Bala, and R. Girshick. Insideoutside<br>net: Detecting objects in context with skip<br>pooling and recurrent neural networks. arXiv preprint<br>arXiv:1512.04143, 2015. 6<br>[2] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei.<br>Imagenet: A large-scale hierarchical image database.<br>In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248–255. IEEE, 2009. 1<br>[3] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and<br>A. Zisserman. The pascal visual object classes (voc) challenge.<br>International journal of computer vision, 88(2):303–338, 2010. 1<br>[4] P. F. Felzenszwalb, R. B. Girshick, and D. McAllester.<br>Discriminatively trained deformable part models, release 4.<br><a href="http://people.cs.uchicago.edu/" rel="external nofollow noopener noreferrer" target="_blank">http://people.cs.uchicago.edu/</a> pff/latent-release4/. 8<br>[5] R. B. Girshick. Fast R-CNN. CoRR, abs/1504.08083, 2015.4, 5, 6<br>[6] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning<br>for image recognition. arXiv preprint arXiv:1512.03385, 2015. 2, 4, 5<br>[7] S. Ioffe and C. Szegedy. Batch normalization: Accelerating<br>deep network training by reducing internal covariate shift.<br>arXiv preprint arXiv:1502.03167, 2015. 2, 5<br>[8] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet<br>classification with deep convolutional neural networks. In<br>Advances in neural information processing systems, pages 1097–1105, 2012. 2<br>[9] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv<br>preprint arXiv:1312.4400, 2013. 5<br>[10] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,<br>P. Dollar, and C. L. Zitnick. Microsoft coco: Com- ´<br>mon objects in context. In European Conference on Computer<br>Vision, pages 740–755. Springer, 2014. 1, 6<br>[11] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, and S. E. Reed.<br>SSD: single shot multibox detector. CoRR, abs/1512.02325, 2015. 4, 5, 6<br>[12] G. A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. J.<br>Miller. Introduction to wordnet: An on-line lexical database.<br>International journal of lexicography, 3(4):235–244, 1990.6<br>[13] J. Redmon. Darknet: Open source neural networks in c.<br><a href="http://pjreddie.com/darknet/" rel="external nofollow noopener noreferrer" target="_blank">http://pjreddie.com/darknet/</a>, 2013–2016. 5<br>[14] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You<br>only look once: Unified, real-time object detection. arXiv<br>preprint arXiv:1506.02640, 2015. 4, 5<br>[15] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards<br>real-time object detection with region proposal networks.<br>arXiv preprint arXiv:1506.01497, 2015. 2, 3, 4, 5,6<br>[16] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,<br>S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,<br>A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual<br>Recognition Challenge. International Journal of Computer<br>Vision (IJCV), 2015. 2<br>[17] K. Simonyan and A. Zisserman. Very deep convolutional<br>networks for large-scale image recognition. arXiv preprint<br>arXiv:1409.1556, 2014. 2, 5<br>[18] C. Szegedy, S. Ioffe, and V. Vanhoucke. Inception-v4,<br>inception-resnet and the impact of residual connections on<br>learning. CoRR, abs/1602.07261, 2016. 2<br>[19] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,<br>D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.<br>Going deeper with convolutions. CoRR, abs/1409.4842, 2014. 5<br>[20] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni,<br>D. Poland, D. Borth, and L.-J. Li. Yfcc100m: The new<br>data in multimedia research. Communications of the ACM,<br>59(2):64–73, 2016. 1</p><p><img src="/archives/./1533802901317.png" alt="图1"><br><img src="/archives/./1533893474011.png" alt="图3"><br><img src="/archives/./1534041247166.png" alt="表7"></p></div><div class="popular-posts-header">Related Posts</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/562809c1.html" rel="bookmark">YOLO</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/2c51a51a.html" rel="bookmark">YOLOv3</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/9bab3bc6.html" rel="bookmark">Fast R-CNN</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/719cc2e5.html" rel="bookmark">SSD</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/fc798de3.html" rel="bookmark">Faster R-CNN</a></div></li></ul><div><ul class="post-copyright"><li class="post-copyright-author"><strong>Post author: </strong>慕湮</li><li class="post-copyright-link"><strong>Post link: </strong><a href="http://muyaan.com/archives/423509a9.html" title="YOLOv2 and YOLO9000">http://muyaan.com/archives/423509a9.html</a></li><li class="post-copyright-license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noopener noreferrer" target="_blank">CC BY-NC-SA 4.0</a> unless stating additionally.</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Computer-Vision/" rel="tag"><i class="fa fa-tag"></i> Computer Vision</a> <a href="/tags/Deep-Learning/" rel="tag"><i class="fa fa-tag"></i> Deep Learning</a> <a href="/tags/Paper/" rel="tag"><i class="fa fa-tag"></i> Paper</a> <a href="/tags/Object-Detection/" rel="tag"><i class="fa fa-tag"></i> Object Detection</a> <a href="/tags/YOLO/" rel="tag"><i class="fa fa-tag"></i> YOLO</a> <a href="/tags/YOLOv2/" rel="tag"><i class="fa fa-tag"></i> YOLOv2</a> <a href="/tags/YOLO9000/" rel="tag"><i class="fa fa-tag"></i> YOLO9000</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/archives/b2e32499.html" rel="next" title="VGGNet"><i class="fa fa-chevron-left"></i> VGGNet</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/archives/2c51a51a.html" rel="prev" title="YOLOv3">YOLOv3 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div><div class="comments" id="comments"><div id="lv-container" data-id="city" data-uid="MTAyMC8zODEwMy8xNDYzMw=="></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">Table of Contents</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">Overview</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4" alt="慕湮"><p class="site-author-name" itemprop="name">慕湮</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">21</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">30</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/tycallen" target="_blank" title="GitHub" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-github"></i>GitHub</a></span><br><span class="links-of-author-item"><a href="mailto:tyc.allen@gmail.com" target="_blank" title="E-Mail" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span><br><span class="links-of-author-item"><a href="http://weibo.com/pojunallen" target="_blank" title="微博" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-weibo"></i>微博</a></span><br><span class="links-of-author-item"><a href="https://tuchong.com/1070837" target="_blank" title="图虫" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-globe"></i>图虫</a></span><br></div><div class="links-of-blogroll motion-element links-of-blogroll-block"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> 友情链接</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="https://dotrabbit.github.io/" title="dotrabbit" target="_blank" rel="external nofollow noopener noreferrer">dotrabbit</a></li></ul></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#"><span class="nav-text"></span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Introduction"><span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Better"><span class="nav-text">2. Better</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Faster"><span class="nav-text">3. Faster</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Stronger"><span class="nav-text">4. Stronger</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Conclusion"><span class="nav-text">5. Conclusion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reference"><span class="nav-text">Reference</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; 2015 &mdash; <span itemprop="copyrightYear">2018</span> <span class="with-love" id="animate"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">慕湮</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="Symbols count total">313k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="Reading time total">9:30</span></div><div class="powered-by">Powered by <a class="theme-link" target="_blank" rel="external nofollow noopener noreferrer" href="https://hexo.io">Hexo</a></div><span class="post-meta-divider">|</span><div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" rel="external nofollow noopener noreferrer" href="https://theme-next.org">NexT.Muse</a></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="//cdn.staticfile.org/jquery/2.1.3/jquery.min.js"></script><script type="text/javascript" src="//cdn.staticfile.org/velocity/1.2.1/velocity.min.js"></script><script type="text/javascript" src="//cdn.staticfile.org/velocity/1.2.1/velocity.ui.min.js"></script><script type="text/javascript" src="/lib/reading_progress/reading_progress.js"></script><script type="text/javascript" src="/js/src/utils.js?v=6.3.0"></script><script type="text/javascript" src="/js/src/motion.js?v=6.3.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=6.3.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=6.3.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=6.3.0"></script><script type="text/javascript">!function(e,t){var n,c=e.getElementsByTagName(t)[0];"function"!=typeof LivereTower&&((n=e.createElement(t)).src="https://cdn-city.livere.com/js/embed.dist.js",n.async=!0,c.parentNode.insertBefore(n,c))}(document,"script")</script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script><script>AV.initialize("UWGXBMGbguCDhSCbC3NrUQtL-gzGzoHsz","ke1jrA5b6VyR89Kqqqwf2kPP")</script><script>function showTime(e){var t=new AV.Query(e),c=[],u=$(".leancloud_visitors");u.each(function(){c.push($(this).attr("id").trim())}),t.containedIn("url",c),t.find().done(function(e){var t=".leancloud-visitors-count";if(0!==e.length){for(var n=0;n<e.length;n++){var o=e[n],i=o.get("url"),s=o.get("time"),r=document.getElementById(i);$(r).find(t).text(s)}for(n=0;n<c.length;n++){i=c[n],r=document.getElementById(i);var l=$(r).find(t);""==l.text()&&l.text(0)}}else u.find(t).text(0)}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(i){var e=$(".leancloud_visitors"),s=e.attr("id").trim(),r=e.attr("data-flag-title").trim(),t=new AV.Query(i);t.equalTo("url",s),t.find({success:function(e){if(0<e.length){var t=e[0];t.fetchWhenSave(!0),t.increment("time"),$(document.getElementById(s)).find(".leancloud-visitors-count").text(t.get("time")),t.save(null,{success:function(e){},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var n=new i,o=new AV.ACL;o.setPublicReadAccess(!0),o.setPublicWriteAccess(!0),n.setACL(o),n.set("title",r),n.set("url",s),n.set("time",1),n.save(null,{success:function(e){$(document.getElementById(s)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):1<$(".post-title-link").length&&showTime(e)})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script></body></html>