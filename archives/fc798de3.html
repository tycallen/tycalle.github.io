<!DOCTYPE html><html class="theme-next muse use-motion" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-big-counter.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="//www.googlefonts.cn//css?family=Lato:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=6.3.0" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.3.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.3.0"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.3.0"><link rel="mask-icon" href="/images/logo.svg?v=6.3.0" color="#222"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Muse",version:"6.3.0",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!1,fastclick:!1,lazyload:!1,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="description" content="Faster R-CNN论文翻译Faster R-CNN: Towards Real-Time Object Detection with Region Proposal NetworksShaoqing Ren, Kaiming He,Ross Girshick, Jian Sun 2016MSRA"><meta name="keywords" content="Computer Vision,Deep Learning,Paper,Object Detection,Faster R-CNN"><meta property="og:type" content="article"><meta property="og:title" content="Faster R-CNN"><meta property="og:url" content="http://muyaan.com/archives/fc798de3.html"><meta property="og:site_name" content="慕湮"><meta property="og:description" content="Faster R-CNN论文翻译Faster R-CNN: Towards Real-Time Object Detection with Region Proposal NetworksShaoqing Ren, Kaiming He,Ross Girshick, Jian Sun 2016MSRA"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://muyaan.com/archives/1532773855377.png"><meta property="og:image" content="http://muyaan.com/archives/1532775900123.png"><meta property="og:image" content="http://muyaan.com/archives/1532944624469.png"><meta property="og:image" content="http://muyaan.com/archives/1533038989594.png"><meta property="og:image" content="http://muyaan.com/archives/1533040298659.png"><meta property="og:image" content="http://muyaan.com/archives/1533041733869.png"><meta property="og:image" content="http://muyaan.com/archives/1533125022507.png"><meta property="og:image" content="http://muyaan.com/archives/1533041748513.png"><meta property="og:image" content="http://muyaan.com/archives/1533084646278.png"><meta property="og:image" content="http://muyaan.com/archives/1533084671171.png"><meta property="og:image" content="http://muyaan.com/archives/1533041762061.png"><meta property="og:image" content="http://muyaan.com/archives/1533084685081.png"><meta property="og:image" content="http://muyaan.com/archives/1533084700597.png"><meta property="og:image" content="http://muyaan.com/archives/1533087019596.png"><meta property="og:image" content="http://muyaan.com/archives/1533087053268.png"><meta property="og:image" content="http://muyaan.com/archives/1533124317161.png"><meta property="og:image" content="http://muyaan.com/archives/1533124977460.png"><meta property="og:image" content="http://muyaan.com/archives/1533124344258.png"><meta property="og:updated_time" content="2018-08-03T09:07:12.648Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Faster R-CNN"><meta name="twitter:description" content="Faster R-CNN论文翻译Faster R-CNN: Towards Real-Time Object Detection with Region Proposal NetworksShaoqing Ren, Kaiming He,Ross Girshick, Jian Sun 2016MSRA"><meta name="twitter:image" content="http://muyaan.com/archives/1532773855377.png"><link rel="canonical" href="http://muyaan.com/archives/fc798de3.html"><script type="text/javascript" id="page.configurations">CONFIG.page={sidebar:""}</script><meta name="google-site-verification" content="Ojd_HrL_PelaXvKK5IkbhLbjZ_sHt6IxRzP-XPaaTw4"><meta name="baidu-site-verification" content="skdT7mIEAb"><title>Faster R-CNN | 慕湮</title><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?ca5844321cfb80fdf6f12b4dcc326991";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style type="text/css">.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.logo-line-after i{right:initial}</style></noscript></head><body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">慕湮</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">白日放歌须纵酒</h1></div><div class="site-nav-toggle"><button aria-label="Toggle navigation bar"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-archives menu-item-active"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li></ul></nav><script type="text/javascript" color="99,184,255" opacity="0.5" zindex="-2" count="99" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><div class="reading-progress-bar"></div><article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://muyaan.com/archives/fc798de3.html"><span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="慕湮"><meta itemprop="description" content=""><meta itemprop="image" content="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4"></span><span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="慕湮"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">Faster R-CNN</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2018-08-01 20:27:57" itemprop="dateCreated datePublished" datetime="2018-08-01T20:27:57+08:00">2018-08-01</time> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">Edited on</span> <time title="Modified: 2018-08-03 17:07:12" itemprop="dateModified" datetime="2018-08-03T17:07:12+08:00">2018-08-03</time> </span><span id="/archives/fc798de3.html" class="leancloud_visitors" data-flag-title="Faster R-CNN"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Views: </span><span class="leancloud-visitors-count"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数: </span><span title="Symbols count in article">20k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="Reading time">37 mins.</span></div></div></header><div class="post-body" itemprop="articleBody"><p>Faster R-CNN论文翻译</p><blockquote><p><a href="https://arxiv.org/abs/1506.01497" rel="external nofollow noopener noreferrer" target="_blank">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a><br>Shaoqing Ren, Kaiming He,Ross Girshick, Jian Sun 2016<br>MSRA</p></blockquote><h2><a href="#" class="headerlink"></a><a id="more"></a></h2><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>近期在物体检测领域获得的进步主要来自region proposal方法和基于region的CNN的成功。尽管R-CNN在刚提出时$^{[5]}$计算量很大，近期已通过跨proposal共享卷积大幅下降$^{[1],[2]}$。最新的典型Fast R-CNN在极深的网络获得了近实时的处理率（忽略花在region proposal时间）。在目前最新的检测系统中，proposal成为了测试时计算瓶颈。</p><p>Region proposal方法通常依赖便宜的inexpensive特征和经济地推论方法economical inference schemes。Selective search是当前最流行的方法之一，基于设计好的低级特征，贪婪地合并超像素。不过与高效检测网络$^{[2]}$相比，SS要慢上一个量级。在CPU上需要两秒才能处理完一张图片。EdgeBoxes$^{[6]}$是目前将速度和proposal质量平衡得最好的，每0.2s处理一张图片。不过，region proposal步骤依然消耗了与检测网络一样的时间。</p><p>也许有人会注意到FRCN利用了GPU的优势，而研究中使用的region proposal方法仍在CPU上实现，让这样的运行时间对比不对等。一个加速proposal计算的明显方法是实现GPU版本。这也许是一个高效的工程解决方案，但重实现方案忽略了后续检测网络，从而失去了重要的共享计算的机会。</p><p>在本论文中，我们展示了算法的变化——用CNN计算proposal——引出的一个优雅高效的解决方案，proposal计算相比检测网络的计算量近似于无。为了这个目的，我们介绍最新的<em>Region Proposal Networks(RPNs)</em>来与最新物体检测网络共享卷积层。通过在测试时共享卷积，计算proposal的边际成本极低（10ms/image）。</p><p>我们发现用于基于region的检测器，如FRCN，所用的卷积特征，同样能用于生成region proposal。基于这些卷积特征，我们通过增加少量额外卷积层构建了一个RPN，同时地<br>对每个规则网格的位置进行region框回归和物体评分。因为RPN是一种全连接卷积网络（FCN），故能端到端地训练，特别是对于生成检测proposal的任务。</p><p>RPN被设计在大范围的尺度和长宽比上高效地预测region proposal。与流行的方法$^{[8],[9],[1],[2]}$使用图片金字塔（图1 a）或过滤器金字塔（图1 b）相反，我们提出了一个新的锚点框，用作不同尺度和长宽比的索引。可以把我们的方法想象为回归索引金字塔（图1 c），避免了为不同不同尺度和长宽比枚举图片。在用单尺度图片训练和测试时，这一模型表现良好，提供了更快的运行速度。</p><p><img src="/archives/./1532773855377.png" alt="图1"></p><p>为了统一RPN和FRCN，我们提出了一个在为region proposal任务调优和为物体检测任务调优轮流间的训练方法，同时保持proposal固定。这一方法收敛迅速并产生一个在两个任务间共享卷积特征的网络。</p><p>我们在PASCAL VOC检测benchmark$^{[11]}$上综合地评估了我们的方法，RPN with FRCN的准确率高于FRCN。同时，我们的方法近乎完全抛弃了测试时SS的计算负担——用于生成proposal的运行时间仅10毫秒。在使用高昂的极深模型$^{[3]}$时，我们的检测方法使用GPU仍有5fps的帧率（包括所有步骤），这是一个兼具速度和准确性的实际物体检测系统。我们也报告了在MS COCO数据集集的成绩并调查了通过COCO数据在VOC上取得的提升。代码已公开：<a href="https://github.com/shaoqingren/faster_rcnn" rel="external nofollow noopener noreferrer" target="_blank">MATLAB</a>和<a href="https://github.com/rbgirshick/py-faster-rcnn" rel="external nofollow noopener noreferrer" target="_blank">Python</a>。</p><p>原稿的早期版本曾发布过$^{[10]}$。自那时起，这一方法的框架就已形成并泛化到其他方法如3D物体检测$^{[12]}$，part-based检测$^{[14]}$，instance segmentation$^{[15]}$和为图片加标题$^{[16]}$。我们的快速高效物体检测系统也已在商用系统中建成，如Pinterests$^{[17]}$。</p><p>在ILSVRC和COCO 15竞赛中，FRCN和RPN是各个第一名的基础。RPN完全用于学习从数据提出region，因此仅仅从更深和更具表达力的特征（如ResNet）就能获益。这一方法同样用于许多其他竞赛的领先成绩中。这些结果说明我们的方法不仅仅是实用的高效方案，也是提升准确率的有效途径。</p><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p><strong>Object Proposals.</strong> 物体proposal方法有大量相关论文。[19],[20],[21]对各方法进行了广泛地测量和比较。常用的方法或基于合并超像素（如Selective Search$^{[4]}$，CPMC$^{[22]}$，MCG$^{[23]}$）或基于滑动窗口（如objectness in windows$^{[24]}$，EdgeBoxes）。在过去，proposal方法用作独立于检测系统的外部模块。</p><p><strong>Deep Networks for Object Detection.</strong> R-CNN方法端到端地训练CNN来将proposal region分类。R-CNN主要扮演分类器的角色，并不预测物体框（除了通过限位框回归调优）。其准确率依赖于region proposal模块性能（见[20]中的比较）。多篇论文提出来使用深度网络预测物体限位框的方法$^{[25],[9],[26],[27]}$。在OverFeat方法中$^{[9]}$训练了一个全连接层来预测框的坐标。MultiBox方法$^{[26],[27]}$从一个最后的全连接层同时预测多个类不可知的框的网络中生成region proposal。这些框用于R-CNN。与我们的FCN相反，MultiBox在单个image或多个大image（224*224）上运行。MultiBox没有在proposal和检测网络间共享特征。我们后续会深入讨论OverFeat和MultiBox。与我们工作同时的DeepMask方法$^{[28]}$用于学习segementation proposals。</p><p>卷积共享计算吸引了对高效且准确的识别的日益增长的注意力。OverFeat从图片金字塔计算卷积特征用于分类、定位和检测。SPP用于高效的物体检测和语义分割。FRCN运行端到端的检测器训练并展现了引人注目的准确率和速度。</p><h2 id="3-Faster-R-CNN"><a href="#3-Faster-R-CNN" class="headerlink" title="3. Faster R-CNN"></a>3. Faster R-CNN</h2><p>我们的物体检测系统Faster R-CNN是由两个模块组成的。第一个是生成proposal region的深度全卷积网络，第二个是FRCN使用region进行检测。整个系统是一个统一的网络。使用了近期流行的神经网络术语“注意力attention”$^{[31]}$机制，RPN模块告知FRCN模块往哪儿看。在3.1中我们介绍了用于region proposal的网络的设计和属性。在3.2中我们设计了一个算法来用共享的特征训练两个模块。</p><h3 id="3-1-Region-Proposal-Networks"><a href="#3-1-Region-Proposal-Networks" class="headerlink" title="3.1 Region Proposal Networks"></a>3.1 Region Proposal Networks</h3><p>RPN接收一张任意尺寸的图片作为输入，输出一个矩型object proposal集合，每个都有一个objectness得分。我们用一个全卷积网络建模此过程。由于我们最终目标是与FRCN物体检测网络共享特征，我们假定两个网络共享了一个公共卷积层集。在我们的实验中，我们调研了有着5个可共享的卷积层的Zeiler and Fergus模型ZF$^{[32]}$和有着13个可共享的卷积层的Simonyan and Zisserman模型$^{[3]}$（VGG-16）。</p><p>我们在最后一个共享卷积层生成的卷积特征图上滑动一个小网络来生成region proposal。这个小网络接收n * n的输入。每个滑动窗口都映射到一个低维特征（对ZF是256维，VGG是512维，后跟一个ReLU）。这个特征提供给两个兄弟全连接层，分别进行限位框回归（<em>reg</em>）和限位框分类（<em>cls</em>）。本论文中，我们使用n = 3，注意在输入图片上的有效感受野很大（ZF和VGG分别是171和228像素）。图3中这个迷你网络只画了一个位置。但因为这个迷你网络用作滑动窗口，两个全连接层在所有空间上都得到了共享。自然地形成了一个n*n卷积层后跟两个兄弟1*1卷积层（分别用于reg和cls）的结构。</p><p><img src="/archives/./1532775900123.png" alt="图2"></p><h4 id="3-1-1-Anchors"><a href="#3-1-1-Anchors" class="headerlink" title="3.1.1 Anchors"></a>3.1.1 Anchors</h4><p>在每个滑动窗口的位置，我们同时预测多个proposal，将每个位置可能的proposal最大数量记做k。那么reg层就有4k个输出，编码了k个框的坐标。cls层为每个proposal估计是否是物体，输出2k个得分（出于简单我们将cls层实现为2类softmax）。我们称为k个anchor。一个anchor位于滑动窗口中心，有自己的尺寸和长宽比。默认我们使用3个尺寸和3个长宽比，即每个滑动窗口位置有9个锚点。对于尺寸为W*H（通常约等于2400）的feature map，共有WHk个锚点。</p><p><strong>Translation-Invariant Anchors</strong><br>我们方法的一个重要特性是平移不变性——锚点和基于锚点计算proposal的方法。如果平移图片中一个物体，proposal应该也会平移，方法也能在那个位置预测proposal。这一属性是由我们方法保证的。MultiBox方法使用k-means来生成800个锚点，并不具备平移不变性。</p><p>平移不变性也降低了模型大小。MultiBox有一个$(4+1)\times 800$维的全连接层，而我们的方法是一个$(4+2) \times 9$维的卷积输出层——k=9的情况。因此我们的输出层有$2.8 \times 10^4$个参数($512 \times (4+2) \times 9$，VGG16)，比MultiBox的输出层的$6.1 \times 10^6$（$1536\times(4+1)\times 800$，GoogleNet）要少两个量级。我们预计我们的方法在小数据集如PASCAL VOC上更不容易过拟合。</p><p><strong>Multi-Scale Anchors as Regression References</strong><br>我们锚点的设计方式提供了一种新的定位多scale（和多长宽比）的方法。如图1所示，过去曾有两种常用的多scale预测方法。第一种基于图片/特征金字塔，如在DPM和基于CNN的方法。图片被缩放到多个scale，并未每个scale计算feature map（图1 a）。这一方法通常有用但过于耗时。第二种方法是使用多个scale（和/或长宽比）的滑动窗口。参考DPM中，有不同长宽比的模型分别用不同过滤器大小训练（如5*7和7*5）。如果这用在定位多scale，可以想象成过滤器金字塔（图1 b）。第二种方式常常与第一种同时使用。</p><p>与其相比，我们的基于锚点的方法建立在锚点金字塔上，是一种更高效的方法。我们的方法依据不同scale和长宽比的锚点框进行分类和回归。它只依赖单scale的图片和feature map，同样只使用了单个大小的filter（在feature map上滑动）。我们会用实验显示这种方法的效果。</p><p>由于这一基于锚点的多scale设计，我们能简单地使用单scale图片计算出的特征。这一设计是无需为多scale付出额外花销的特征共享的关键。</p><h4 id="3-1-2-Loss-Function"><a href="#3-1-2-Loss-Function" class="headerlink" title="3.1.2 Loss Function"></a>3.1.2 Loss Function</h4><p>为了训练RPN，我们为每个锚点赋值一个二元标签（是不是一个物体）。我们为两种锚点赋予正标签：与一个gt框有着最高的IoU的锚点，或与任一gt框IoU大于0.7的锚点。注意一个gt框可能会为多个锚点赋予正值。通常第二个条件足够决定正样本，但我们仍采用第一条件的原因是，在某些少见的场景下第二条件无法找到正样本。我们为一个非负的与所有gt框IoU都小于0.3的锚点标作负样本。对于训练目标没有帮助的锚点既不为正也不为负。</p><p>有了这些定义，我们按照FRCN的多任务loss来进行优化，我们的图片loss函数定义如下：</p><script type="math/tex;mode=display">L(\{ p_i \},\{ t_i \}) = \frac 1 {N_{cls}} \sum _i L_{cls} (p_i,p_i^*) + \lambda \frac 1 {N_{reg}} \sum_i p_i^* L_{reg} (t_i,t_i^*) \qquad (1)</script><p>这里i是mini-batch中一个锚点的序号，$p_i$是锚点i是物体的预测概率。$p_i^<em>$指锚点i的标签，如为正样本则等于1，否则等于0。$t_i$是代表预测框的4个参数化坐标的向量，$t_i^</em>$是正样本相关的gt框位置。分类loss $L_{cls}$是对两类（是否是物体）的log loss。对于回归loss，我们使用$L_{reg}(t_i,t_i^<em>) = R(t_i-t_i^</em>)$，其中R是[2]中定义的robust loss函数（smooth $L_1$）。$p_i^*L_{reg}$代表回归loss只有对正样本锚点激活。cls和reg层的输出分别组成$\{p_i\}$和$\{t_i\}$。</p><p>两项分别用$N_{cls}$和$N_{reg}$归一化，并用参数$\lambda$进行平衡。在我们当前的实现中（已公布的代码），公式中的第一项由mini-batch大小归一化（$N_{cls} = 256$），reg项由锚点坐标数量归一化（$N_{reg} \sim 2400$）。默认我们设置$\lambda = 10$，因此两项权重近似相等。我们通过实验发现结果对$\lambda$的值在很大范围内不敏感（表9）。我们也注意到上面提到的归一化是不需要的，可以被简化。</p><p>对于限位框回归，我们按照[5]的方法对四个坐标参数化：</p><script type="math/tex;mode=display">\begin{array}{lcl}
t_x = (x-x_a)/w_a, t_y=(y-y_a)/h_a,
\\ t_w = \log (w/w_a),  t_h = \log (h/h_a)
\\ t_x^* = (x^* - x_a)/w_a, t_y^*=(y^*-y_a)/h_a,
\\ t_w^* = \log(w^*/w_a), t_h^* = \log(h^*/h_a)
\end{array} \qquad (2)</script><p>其中x,y,w,h表示框的中心坐标和宽高。变量$x,x_a,x^*$分别用作预测框，锚点框和gt框（y,w,h类似）。这可以想象为从一个锚点框向一个附近的gt框进行回归。</p><p>我们的方法与[1],[2]的限位框回归方法不同。在它们的方法中，是在从任意尺寸的RoI中提取的特征上进行限位框回归的，所有region size都共享了同样的回归权重。在我们的方法中，用于回归的特征在feature map上都有同样的空间大小（3 * 3）。为了处理多种大小，一共会训练k个限位框回归器。每个都负责特定的尺寸和宽高比，它们之间不会共享。</p><h4 id="3-1-3-Training-RPNs"><a href="#3-1-3-Training-RPNs" class="headerlink" title="3.1.3 Training RPNs"></a>3.1.3 Training RPNs</h4><p>RPN能通过反向传播和SGD端到端地训练。我们按照[2]的”image-centric”抽样方法来训练网络。每个mini-batch都来自一张图片，其中包含了很多正负样本锚点。可以为一张图的所有锚点优化loss函数，但这会偏向负样本，因为它们数量更多。我们只为图中随机抽样的256个锚点计算loss，抽样正负比为1:1。如果正样本少于128个则用负样本填充。</p><p>我们用均值为0，标准差为0.01的高斯分布初始化所有新层的权重。所有其它层（如共享的卷积层）使用预训练的ImageNet分类模型初始化。对于ZFnet我们调优所有层，对VGGnet调优conv3_1及以上的层，为了conserve 内存。在PASCAL VOC上，前60k次迭代学习率为0.001，后20k为0.0001。动量为0.9，decay为0.0005。</p><h3 id="3-2-Sharing-Features-for-RPN-and-Fast-R-CNN"><a href="#3-2-Sharing-Features-for-RPN-and-Fast-R-CNN" class="headerlink" title="3.2 Sharing Features for RPN and Fast R-CNN"></a>3.2 Sharing Features for RPN and Fast R-CNN</h3><p>目前为止我们在不考虑使用proposal的FRCN的情况下，介绍了如何训练用于生成region proposal的网络。接下来我们会介绍如何训练一个由RPN和FRCN组成的，共享了卷积层的网络的算法（图2）。</p><p>分别训练RPN和FRCN的话，它们都会用不同的方式修改卷积层。因此我们需要开发一个技术使它们共享卷积层，而不是独立地训练两个网络。我们讨论了三种方法：</p><ol><li><em>Alternating training 交替训练</em> 我们先训练RPN，再用proposal训练FRCN。被FRCN调优后的网络用于初始化，再继续这个过程。这是本论文所有实验所用方法。</li><li><em>Approximate joint training 近似共同训练</em> 这一方案中，RPN和FRCN在训练中像图2一样合并为一个网络。在每个SGD迭代中，前向传播产生的proposal被当做固定的、预计算好的，像训练FRCN时一样。反向传播也像通常一样，不过共享层会触发两个loss并将其结合。这一方案实现很容易。但它忽略了限位框坐标的偏导。我们的实验经验是这一方法有相近的成绩，但与交替训练比训练时间缩短25~50%。</li><li><em>Non-approximate joint training</em> 如之前讨论，RPN预测的限位框也是输入的函数。FRCN中的RoI池化层接收卷积特征和预测框作为输入。因此理论上正确的反向传播solver应该涉及限位框坐标的梯度。在本方法中，我们需要一个对限位框坐标可微的RoI池化层。这不是个简单的问题，可以通过[15]那样通过”RoI warping”层解决。</li></ol><p><strong>4-Step Alternating Training.</strong> 本论文中，我们通过一个使用的四步训练算法来通过交替训练学习共享特征。首先，我们按3.1.3描述那样训练RPN。网络初始化自ImageNet训练的模型，为region proposal任务调优。接着，我们用步骤1的RPN产生的proposal训练一个单独的FRCN。这个检测网络也初始化自ImageNet预训练模型。在这时两个网络还没有共享卷积层。第三步，我们使用检测网络来初始化RPN，但我们固定共享卷积层，只对RPN独有的层调优。现在两个网络共享卷积层了。最终，保持共享卷积层，为FRCN独有的层调优。可以训练更多次，但我们只观察到微不足道的提升。</p><h3 id="3-3-Implementation-Details"><a href="#3-3-Implementation-Details" class="headerlink" title="3.3 Implementation Details"></a>3.3 Implementation Details</h3><p>我们在单尺度图片上训练和测试两个网络。我们把图片缩放，让短边s = 600像素。多尺度度特征提取（使用图片金字塔）也许能提升准确度，但不是一个速度-精度的好的权衡。在缩放后的图片，ZF和VGG的最后一个卷积层步长都是16像素。即PASCAL原图（500*375）上的10像素。尽管这种大步长提供了很好的结果，更小的步长可能会更好。</p><p>对于锚点框，我们使用3种尺度，面积分别为$128^2,256^2,512^2$像素，3种长宽比1:1,1:2,2:1。这些超参数不是为特定数据集小心选取的，在下一张我们提供了消融测试，验证其效果。图3右显示了我们的方法所能兼容的大范围的尺度和长宽比。表1显示了使用ZFnet所学习到的每个锚点proposal平均大小。我们注意到我们的算法允许大于感受野的预测。这些预测是不可能的</p><p><img src="/archives/./1532944624469.png" alt="图3"></p><p><img src="/archives/./1533038989594.png" alt="表1"></p><p>超过了图片边界的锚点框需要小心处理。在训练中，我们忽略了所有跨边界的锚点框。对于1000 * 600的图片，约有20000个（60*40*9）锚点框。去除掉跨界锚点框，剩余6000个可训练的。如果这些不被忽略，会引入大量、难以纠正的目标错误，让训练难以收敛。在测试中，我们会对整个图片进行full convolutional RPN。产生的跨界proposal，我们会在图片边缘裁减掉。</p><p>有些RPN proposal之间高度重叠。为了减少冗余，我们对proposal region基于其分类得分采用非极大值抑制（NMS）。用于NMS的IoU阈值为0.7，留给我们每张图片约2000个proposal。NMS不会影响精度，但显著地减少了proposal数量。NMS后，我们选择top-N proposal用于检测。训练时，我们用2000个region训练FRCN，但测试时数量不一样。</p><h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h2><h3 id="4-1-Experiments-on-PASCAL-VOC"><a href="#4-1-Experiments-on-PASCAL-VOC" class="headerlink" title="4.1 Experiments on PASCAL VOC"></a>4.1 Experiments on PASCAL VOC</h3><p>我们在VOC 2007 detection benchmark上充分评估了我们的方法。这一数据集有5k训练验证图片和5k测试图片，约20种类型。我们也提供了少量模型在VOC 2012 benchmark上的结果。对于ImageNet预训练的网络，我们选择有着5个卷积层和3个全连接层的快速版ZF-net，有着13个卷积层和3个全连接层的VGG-16模型。我们主要用mAP进行评估。</p><p>表2顶部显示了用不同的region proposal方法训练测试FRCN的结果。这些结果使用ZF-net。对于SS，我们使用fast模式生成2000个proposal，对于EdgeBoxes，我们用默认调优为0.7 IoU的设定。它们各自mAP为58.7%和58.6%。RPN使用最多300个proposal就取得了59.9%的mAP。由于共享卷积计算，RPN比SS和EB快很多，更少的proposal同样减少了全连接层的花销（表5）。</p><p><strong>Ablation Experiments on RPN</strong> 为了调查RPN作为proposal方法的表现，我们进行了多个消融测试。首先，我们演示了在RPN和FRCN间共享卷积层的作用。为了这一目的，我们在4步算法中的第二步就停下。使用独立的网络稍微减少了mAP，58.7%（表2中 RPN+ZF unshared）。我们观察到这一点是因为第三步用检测器调优的特征用于调优RPN，提升了proposal质量。</p><p><img src="/archives/./1533040298659.png" alt="表2"></p><p>接下来，我们消除RPN对训练FRCN的影响。我们使用2000 SS和ZFnet训练FRCN模型。我们固定这一检测器，评估测试时不同的proposal方法的mAP，这些测试中RPN没有与检测器共享特征。</p><p>在测试时用300RPN替换SS得到56.8%的mAP。这一损失来自训练测试所用的proposal的不一致。这一结果将作为后续比较的基线。</p><p>在测试时使用RPN top-100 proposal仍得到了55.1%的mAP，说明高排名的proposal很准确。在另一个极端，使用top-rank 6000 RPN proposal（无NMS）的mAP为55.2%，说明NMS没有伤害mAP，也许减少了很多错误警报。</p><p>接下来，我们分别调查RPN的cls和reg输出的作用。在测试时分别关闭其中一个。当测试时移除cls层时（因此没有NMS/ranking），我们随机从未评分的region中抽样N个proposal。当N=1000时，mAP几乎未变（55.8%），但当N=100时，mAP急剧下降到44.6%。这说明cls得分对于找到最精确的proposal的作用。</p><p>另一方面，当测试时移除reg层（proposal变成锚点框），mAP掉到52.1%。这说明高质量的proposal来自限位框回归。尽管锚点框有多重尺度和长宽比，但仍不足以精确检测。</p><p>我们还评估了更强力的网络应用于RPN的proposal质量。我们使用VGG-16训练RPN，仍然使用前面的SS+ZF检测器。mAP从56.8%(RPN+ZF)提升到59.2%(RPN+VGG)。这是一个充满希望的结果，因为它说明RPN+VGG比RPN+ZFproposal质量高。由于RPN+ZF与SS的proposal质量类似（当训练测试都一致使用时，均为58.7%），我们可以期望RPN+VGG高过SS。后续实验会证明这一猜想。</p><p><img src="/archives/./1533041733869.png" alt="表3"></p><p><strong>Performance of VGG-16.</strong> 表3显示了VGG用于proposal和检测时的结果。在不共享特征时，RPN+VGG结果为68.5%，略高于SS基线。不像SS那样是预先定义好的，RPN是实际训练的，并从更好的网络中受益。对于共享特征版本，mAP为69.9%——接近strong ss基线，proposal消耗还极低。我们在VOC 07和12训练验证集上联合训练了RPN和检测网络。mAP为73.2%。图5显示了在VOC 07测试集上的一些结果。在VOC 12测试集上（表4），我们的联合训练的模型mAP为70.4%。表6和表7显示了详细数字。</p><p><img src="/archives/./1533125022507.png" alt="图5"><br><img src="/archives/./1533041748513.png" alt="表4"><br><img src="/archives/./1533084646278.png" alt="表6"><br><img src="/archives/./1533084671171.png" alt="表7"></p><p>在表5中我们总结了整个物体检测系统的运行时间。SS视内容而定，通常需要1-2秒（平均1.5秒），使用VGG-16的FRCN需要320ms来处理2000个SS proposal（如果在全连接层使用SVD，则是223ms）。我们的用VGG-16的系统proposal+detection总共需要198ms。由于共享了卷积特征，RPN仅需10ms计算其额外的层。由于proposal数量更少（300个每张），region-wise计算耗时也更少。我们的使用ZFnet的系统帧率达到了17fps。</p><p><img src="/archives/./1533041762061.png" alt="表5"></p><p><strong>Sensitivities to Hyper-parameters.</strong> 在表8中，我们调查了锚点的设定。默认我们使用了三种尺度和长宽比（表8中的69.9% mAP）。如果每个位置只使用一个锚点，mAP会降低3-4%。如果使用3种尺度（1种长宽比）或3种长宽比（1种尺度），mAP会稍高。说明用不同大小的锚点框是有用的。使用3种尺度1种长宽比就有足够好的mAP，69.8%，说明尺度和长宽比不是检测精度的disentangled维度。但我们仍会采取这两种维度来保持系统的灵活性。</p><p><img src="/archives/./1533084685081.png" alt="表8"></p><p>在表9中我们比较了等式1中不同的$\lambda$的作用。我们默认采用$\lambda = 10$让等式1中的两项权重相等。表9显示，当$\lambda$在两个量级间变动时（1~100），mAP会变动1%。这说明结果对很大范围的$\lambda$不敏感。</p><p><img src="/archives/./1533084700597.png" alt="表9"></p><p><strong>Analysis of Recall-to-IoU.</strong> 接下来我们计算不同IoU阈值的proposal召回率。需要注意Recall-to-IoU评估仅仅<em>松</em>相关$^{[19],[20],[21]}$于最终检测准确度。用来诊断proposal方法更合适，而不是评估它。</p><p><img src="/archives/./1533087019596.png" alt="图4"></p><p>在图4中，我们显示了使用300，1000，2000个proposal的结果。我们与SS和EB进行了比较，用基于方法给与的confidence选出的top-N proposal。图表显示当数量从2000降到300时，RPN方法同样优雅。这解释了为何使用300个proposal时mAP的优秀表现。如我们之前分析的，这一性质主要归功于RPN的cls项。SS和EB的recall降得快得多。</p><p><strong>One-Stage Detection vs. Two-Stage Proposal + Detection.</strong> OverFeat论文$^{[9]}$提出了回归和分类器在feature map上滑动窗口的检测方法。OverFeat是一个<em>单步，类相关</em>的检测流程，我们的是<em>两步，瀑布式</em>，由类无关的proposal和类相关的检测器组成。OverFeat中，region的特征来自尺度金字塔的一个特定的长宽比的滑动窗口。这些特征同时用于决定位置和物体类别。在RPN中，特征来自方形(3*3)滑动窗口，预测proposal与锚点框不同尺度和长宽比有关。尽管两种方法都使用滑动窗口，region proposal任务仅是Faster R-CNN的第一步——下游的FRCN检测器致力于改良proposal。第二步中，region-wise特征自适应地从proposal框中池化出，更如实地覆盖了region。我们相信这些特征会导致更精确的检测。</p><p>为了比较两个系统，我们通过单步FRCN评估了OverFeat系统。系统中的proposal是三个尺度（128，256，512）和三个长宽比（1:1,1:2,2:1）的密集滑动窗口。FRCN训练用来从这些窗口预测类相关得分，回归限位框位置。由于OverFeat系统采用图片金字塔，我们也用从5个尺度提取的卷积特征进行评估。</p><p><img src="/archives/./1533087053268.png" alt="表10"></p><p>表10比较了两步系统和两个单步系统的变种。使用ZF模型的单步系统mAP为53.9%。比双步系统低了4.8%。实验证明瀑布式region proposal和物体检测的有效性。类似的观察结果在[2],[39]都有报道，他们用滑动窗口替换SS，导致mAP下降了6%。</p><h3 id="4-2-Experiments-on-MS-COCO"><a href="#4-2-Experiments-on-MS-COCO" class="headerlink" title="4.2 Experiments on MS COCO"></a>4.2 Experiments on MS COCO</h3><p>我们提供了更多在MS COCO物体检测数据集上的结果。该数据集共有80种物体。我们的实验使用了80k张测试集，40k张验证集和20k张test-dev集。我们测试了$IoU \in [ 0.5:0.05:0.95 ]$下的mAP均值（COCO标准评估方法，简记为mAP@[.5,.95]）和mAP@0.5（VOC标准评估方法）</p><p>我们为此数据集建造的系统有一些小修改。我们在8-GPU实现上训练了我们的模型，RPN的有效mini-batch大小变为8（每个GPU1个），FRCN为16（每个GPU2个）。RPN步骤和FRCN步骤分别用0.003学习率训练了240k迭代，接着用0.0003训练了80k。我们修改了学习率（开始为0.001，改为0.003），因为mini-batch大小改变了。对于锚点框，我们用了3种长宽比和4种尺度（增加了$64^2$），主要是为了掌控数据集中的小物体。另外，在FRCN步骤中，负样本定义为与gt框的最大IoU在[0,0.5)之间的，而不是[0.1,0.5)。我们注意到在SPPnet系统中，在网络调优阶段使用[0.1,0.5)作为负样本，但在SVM阶段仍然使用[0,0.5)作为负样本，并进行hard negative mining。但FRCN丢弃了SVM步骤，因此[0,0.1)间的样本未使用。引入这部分样本在FRCN和Faster R-CNN上都提升了在COCO上的mAP@0.5（但在PASCAL VOC上是副作用）。</p><p>剩余实现细节与VOC上的一样。特别是我也仍用了300proposal和单尺度（s = 600）的测试设定。测试时间同样是200ms每张图片。</p><p><img src="/archives/./1533124317161.png" alt="表11"></p><p>在表11中我们首次报告了我们用本论文的FRCN实现的结果。这个基线在test-dev集上的mAP@0.5为39.3%，比[2]中报告的高。我们推测这一差距原因为负样本的定义不同和mini-batch大小的改变。我们也注意到mAP@[.5,.95]旗鼓相当。</p><p>接下来我们评估了我们的Faster R-CNN系统。使用COCO训练集进行训练，Faster R-CNN在test-dev上有着42.1%的mAP@0.5和21.5%的mAP@[.5,.95]。比起FRCN都高了2.几。这说明RPN在更高的IoU阈值下提升定位准确度出色。使用COCO训练验证集训练，结果为42.7%mAP@0.5和21.9%mAP@[.5,.95]。图6显示了一些COCO test-dev上的结果。</p><p><img src="/archives/./1533124977460.png" alt="图6"></p><p><strong>Faster R-CNN in ILSVRC &amp; COCO 2015 competitions</strong> 我们已论证了Faster R-CNN从更好的特征中收益——由于RPN完全通过神经网络学习propose region。当将深度大量提升至超过100层时$^{[18]}$，这一现象依然成立。仅仅是用ResNet-101替换VGG-16，就将mAP从41.5%/21.2%提升到48.4%/27.2%。使用了其它方式提升（other improvements orthogonal）Faster R-CNN后，He他们的在COCO test-dev集上单模型结果为55.7%/34.9%，组合模型达到59.0%/37.4%，赢得了COCO 2015物体检测竞赛第一名。同样的系统也取得了ILSVRC 2015物体检测竞赛的第一名，领先第二名8.5%。RPN也赢得了ILSVRC 2015定位和COCO 2015分割竞赛第一名。</p><h3 id="4-3-From-MS-COCO-to-PASCAL-VOC"><a href="#4-3-From-MS-COCO-to-PASCAL-VOC" class="headerlink" title="4.3 From MS COCO to PASCAL VOC"></a>4.3 From MS COCO to PASCAL VOC</h3><p>大量数据是提升深度神经网络的关键。接下来，我们调查了怎样用MS COCO数据集提升VOC上的检测表现。</p><p>我们直接评估了COCO检测模型在VOC数据集上的表现，作为基线，没有使用任何VOC数据调优。这一评估是可行的因为COCO数据集的种类是VOC的种类的超集。COCO独有的种类在本实验中忽略，softmax层表现为20个类加背景。这一设定下在VOC 07测试集上的mAP为76.1%（表12）。尽管没有利用VOC数据，这一结果都好过在VOC 07+12的结果（73.2%）很大一截。</p><p>接下来我们在VOC数据集上调优COCO模型。在这个实验中，我们用COCO模型替换了原有的用于初始化权重的ImageNet预训练模型，并按章节3.2那样调优Faster R-CNN系统。在07 测试集上结果为78.8%。来自COCO的数据提升了5.6%的mAP。表6显示用COCO+VOC训练的模型在VOC 12测试集上有着所有类上最高的AP。</p><p><img src="/archives/./1533124344258.png" alt="表12"></p><h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5. Conclusion"></a>5. Conclusion</h2><p>我们提出了用于高效准确地生成region proposal的RPN。通过与后续检测网络共享卷积特征，region proposal步骤近乎不耗时。我们的方法允许一个整体的、基于深度学习的物体检测系统以近实时的帧率运行。训练出的RPN同样提升了region proposal质量，以及全部的物体检测准确度。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling<br>in deep convolutional networks for visual recognition,” in<br>European Conference on Computer Vision (ECCV), 2014.<br>[2] R. Girshick, “Fast R-CNN,” in IEEE International Conference on<br>Computer Vision (ICCV), 2015.<br>[3] K. Simonyan and A. Zisserman, “Very deep convolutionalnetworks<br>for large-scale image recognition,” in International<br>Conference on Learning Representations (ICLR), 2015.<br>[4] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders,<br>“Selective search for object recognition,” International<br>Journal of Computer Vision (IJCV), 2013.<br>[5] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature<br>hierarchies for accurate object detection and semantic segmentation,”<br>in IEEE Conference on Computer Vision and Pattern<br>Recognition (CVPR), 2014.<br>[6] C. L. Zitnick and P. Dollar, “Edge boxes: Locating object ´<br>proposals from edges,” in European Conference on Computer<br>Vision (ECCV), 2014.<br>[7] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional<br>networks for semantic segmentation,” in IEEE Conference on<br>Computer Vision and Pattern Recognition (CVPR), 2015.<br>[8] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan,<br>“Object detection with discriminatively trained partbased<br>models,” IEEE Transactions on Pattern Analysis and Machine<br>Intelligence (TPAMI), 2010.<br>[9] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,<br>and Y. LeCun, “Overfeat: Integrated recognition, localization<br>and detection using convolutional networks,” in International<br>Conference on Learning Representations (ICLR), 2014.<br>[10] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards<br>real-time object detection with region proposal networks,” in<br>Neural Information Processing Systems (NIPS), 2015.<br>[11] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and<br>A. Zisserman, “The PASCAL Visual Object Classes Challenge<br>2007 (VOC2007) Results,” 2007.<br>[12] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,<br>P. Dollar, and C. L. Zitnick, “Microsoft COCO: Com- ´<br>mon Objects in Context,” in European Conference on Computer<br>Vision (ECCV), 2014.<br>[13] S. Song and J. Xiao, “Deep sliding shapes for amodal 3d object<br>detection in rgb-d images,” arXiv:1511.02300, 2015.<br>[14] J. Zhu, X. Chen, and A. L. Yuille, “DeePM: A deep part-based<br>model for object detection and semantic part localization,”<br>arXiv:1511.07131, 2015.<br>[15] J. Dai, K. He, and J. Sun, “Instance-aware semantic segmentation<br>via multi-task network cascades,” arXiv:1512.04412, 2015.<br>[16] J. Johnson, A. Karpathy, and L. Fei-Fei, “Densecap: Fully<br>convolutional localization networks for dense captioning,”<br>arXiv:1511.07571, 2015.<br>[17] D. Kislyuk, Y. Liu, D. Liu, E. Tzeng, and Y. Jing, “Human curation<br>and convnets: Powering item-to-item recommendations<br>on pinterest,” arXiv:1511.04003, 2015.<br>[18] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning<br>for image recognition,” arXiv:1512.03385, 2015.<br>[19] J. Hosang, R. Benenson, and B. Schiele, “How good are detection<br>proposals, really?” in British Machine Vision Conference<br>(BMVC), 2014.<br>[20] J. Hosang, R. Benenson, P. Dollar, and B. Schiele, “What makes ´<br>for effective detection proposals?” IEEE Transactions on Pattern<br>Analysis and Machine Intelligence (TPAMI), 2015.<br>[21] N. Chavali, H. Agrawal, A. Mahendru, and D. Batra,<br>“Object-Proposal Evaluation Protocol is ’Gameable’,” arXiv:<br>1505.05836, 2015.<br>[22] J. Carreira and C. Sminchisescu, “CPMC: Automatic object<br>segmentation using constrained parametric min-cuts,”<br>IEEE Transactions on Pattern Analysis and Machine Intelligence<br>(TPAMI), 2012.<br>[23] P. Arbelaez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik, ´<br>“Multiscale combinatorial grouping,” in IEEE Conference on<br>Computer Vision and Pattern Recognition (CVPR), 2014.<br>[24] B. Alexe, T. Deselaers, and V. Ferrari, “Measuring the objectness<br>of image windows,” IEEE Transactions on Pattern Analysis<br>and Machine Intelligence (TPAMI), 2012.<br>[25] C. Szegedy, A. Toshev, and D. Erhan, “Deep neural networks<br>for object detection,” in Neural Information Processing Systems<br>(NIPS), 2013.<br>[26] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov, “Scalable<br>object detection using deep neural networks,” in IEEE Conference<br>on Computer Vision and Pattern Recognition (CVPR), 2014.<br>[27] C. Szegedy, S. Reed, D. Erhan, and D. Anguelov, “Scalable,<br>high-quality object detection,” arXiv:1412.1441 (v1), 2015.<br>[28] P. O. Pinheiro, R. Collobert, and P. Dollar, “Learning to<br>segment object candidates,” in Neural Information Processing<br>Systems (NIPS), 2015.<br>[29] J. Dai, K. He, and J. Sun, “Convolutional feature masking<br>for joint object and stuff segmentation,” in IEEE Conference on<br>Computer Vision and Pattern Recognition (CVPR), 2015.<br>[30] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun, “Object<br>detection networks on convolutional feature maps,”<br>arXiv:1504.06066, 2015.<br>[31] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and<br>Y. Bengio, “Attention-based models for speech recognition,”<br>in Neural Information Processing Systems (NIPS), 2015.<br>[32] M. D. Zeiler and R. Fergus, “Visualizing and understanding<br>convolutional neural networks,” in European Conference on<br>Computer Vision (ECCV), 2014.<br>[33] V. Nair and G. E. Hinton, “Rectified linear units improve<br>restricted boltzmann machines,” in International Conference on<br>Machine Learning (ICML), 2010.<br>[34] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,<br>D. Erhan, and A. Rabinovich, “Going deeper with convolutions,”<br>in IEEE Conference on Computer Vision and Pattern<br>Recognition (CVPR), 2015.<br>[35] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,<br>W. Hubbard, and L. D. Jackel, “Backpropagation applied to<br>handwritten zip code recognition,” Neural computation, 1989.<br>[36] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,<br>Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg,<br>and L. Fei-Fei, “ImageNet Large Scale Visual Recognition<br>Challenge,” in International Journal of Computer Vision (IJCV),<br>2015.<br>[37] A. Krizhevsky, I. Sutskever, and G. Hinton, “Imagenet classification<br>with deep convolutional neural networks,” in Neural<br>Information Processing Systems (NIPS), 2012.<br>[38] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,<br>S. Guadarrama, and T. Darrell, “Caffe: Convolutional<br>architecture for fast feature embedding,” arXiv:1408.5093, 2014.<br>[39] K. Lenc and A. Vedaldi, “R-CNN minus R,” in British Machine<br>Vision Conference (BMVC), 2015.</p></div><div class="popular-posts-header">Related Posts</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/562809c1.html" rel="bookmark">YOLO</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/5beb21d0.html" rel="bookmark">R-CNN</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/8504aa6c.html" rel="bookmark">RetinaNet</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/423509a9.html" rel="bookmark">YOLOv2 and YOLO9000</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/719cc2e5.html" rel="bookmark">SSD</a></div></li></ul><div><ul class="post-copyright"><li class="post-copyright-author"><strong>Post author: </strong>慕湮</li><li class="post-copyright-link"><strong>Post link: </strong><a href="http://muyaan.com/archives/fc798de3.html" title="Faster R-CNN">http://muyaan.com/archives/fc798de3.html</a></li><li class="post-copyright-license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noopener noreferrer" target="_blank">CC BY-NC-SA 4.0</a> unless stating additionally.</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Computer-Vision/" rel="tag"><i class="fa fa-tag"></i> Computer Vision</a> <a href="/tags/Deep-Learning/" rel="tag"><i class="fa fa-tag"></i> Deep Learning</a> <a href="/tags/Paper/" rel="tag"><i class="fa fa-tag"></i> Paper</a> <a href="/tags/Object-Detection/" rel="tag"><i class="fa fa-tag"></i> Object Detection</a> <a href="/tags/Faster-R-CNN/" rel="tag"><i class="fa fa-tag"></i> Faster R-CNN</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/archives/9bab3bc6.html" rel="next" title="Fast R-CNN"><i class="fa fa-chevron-left"></i> Fast R-CNN</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/archives/562809c1.html" rel="prev" title="YOLO">YOLO <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div><div class="comments" id="comments"><div id="lv-container" data-id="city" data-uid="MTAyMC8zODEwMy8xNDYzMw=="></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">Table of Contents</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">Overview</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4" alt="慕湮"><p class="site-author-name" itemprop="name">慕湮</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">21</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">30</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/tycallen" target="_blank" title="GitHub" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-github"></i>GitHub</a></span><br><span class="links-of-author-item"><a href="mailto:tyc.allen@gmail.com" target="_blank" title="E-Mail" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span><br><span class="links-of-author-item"><a href="http://weibo.com/pojunallen" target="_blank" title="微博" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-weibo"></i>微博</a></span><br><span class="links-of-author-item"><a href="https://tuchong.com/1070837" target="_blank" title="图虫" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-globe"></i>图虫</a></span><br></div><div class="links-of-blogroll motion-element links-of-blogroll-block"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> 友情链接</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="https://dotrabbit.github.io/" title="dotrabbit" target="_blank" rel="external nofollow noopener noreferrer">dotrabbit</a></li></ul></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text"></span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Introduction"><span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Related-Work"><span class="nav-text">2. Related Work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Faster-R-CNN"><span class="nav-text">3. Faster R-CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Region-Proposal-Networks"><span class="nav-text">3.1 Region Proposal Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-1-Anchors"><span class="nav-text">3.1.1 Anchors</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-2-Loss-Function"><span class="nav-text">3.1.2 Loss Function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-3-Training-RPNs"><span class="nav-text">3.1.3 Training RPNs</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-Sharing-Features-for-RPN-and-Fast-R-CNN"><span class="nav-text">3.2 Sharing Features for RPN and Fast R-CNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-Implementation-Details"><span class="nav-text">3.3 Implementation Details</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Experiments"><span class="nav-text">4. Experiments</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-Experiments-on-PASCAL-VOC"><span class="nav-text">4.1 Experiments on PASCAL VOC</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-Experiments-on-MS-COCO"><span class="nav-text">4.2 Experiments on MS COCO</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-From-MS-COCO-to-PASCAL-VOC"><span class="nav-text">4.3 From MS COCO to PASCAL VOC</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Conclusion"><span class="nav-text">5. Conclusion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reference"><span class="nav-text">Reference</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; 2014 &mdash; <span itemprop="copyrightYear">2018</span> <span class="with-love" id="animate"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">慕湮</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="Symbols count total">312k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="Reading time total">9:27</span></div><div class="powered-by">Powered by <a class="theme-link" target="_blank" rel="external nofollow noopener noreferrer" href="https://hexo.io">Hexo</a></div><span class="post-meta-divider">|</span><div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" rel="external nofollow noopener noreferrer" href="https://theme-next.org">NexT.Muse</a></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/lib/reading_progress/reading_progress.js"></script><script type="text/javascript" src="/js/src/utils.js?v=6.3.0"></script><script type="text/javascript" src="/js/src/motion.js?v=6.3.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=6.3.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=6.3.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=6.3.0"></script><script type="text/javascript">!function(e,t){var n,c=e.getElementsByTagName(t)[0];"function"!=typeof LivereTower&&((n=e.createElement(t)).src="https://cdn-city.livere.com/js/embed.dist.js",n.async=!0,c.parentNode.insertBefore(n,c))}(document,"script")</script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script><script>AV.initialize("UWGXBMGbguCDhSCbC3NrUQtL-gzGzoHsz","ke1jrA5b6VyR89Kqqqwf2kPP")</script><script>function showTime(e){var t=new AV.Query(e),c=[],u=$(".leancloud_visitors");u.each(function(){c.push($(this).attr("id").trim())}),t.containedIn("url",c),t.find().done(function(e){var t=".leancloud-visitors-count";if(0!==e.length){for(var n=0;n<e.length;n++){var o=e[n],i=o.get("url"),s=o.get("time"),r=document.getElementById(i);$(r).find(t).text(s)}for(n=0;n<c.length;n++){i=c[n],r=document.getElementById(i);var l=$(r).find(t);""==l.text()&&l.text(0)}}else u.find(t).text(0)}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(i){var e=$(".leancloud_visitors"),s=e.attr("id").trim(),r=e.attr("data-flag-title").trim(),t=new AV.Query(i);t.equalTo("url",s),t.find({success:function(e){if(0<e.length){var t=e[0];t.fetchWhenSave(!0),t.increment("time"),$(document.getElementById(s)).find(".leancloud-visitors-count").text(t.get("time")),t.save(null,{success:function(e){},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var n=new i,o=new AV.ACL;o.setPublicReadAccess(!0),o.setPublicWriteAccess(!0),n.setACL(o),n.set("title",r),n.set("url",s),n.set("time",1),n.save(null,{success:function(e){$(document.getElementById(s)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):1<$(".post-title-link").length&&showTime(e)})</script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>