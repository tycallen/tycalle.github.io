<!DOCTYPE html><html class="theme-next muse use-motion" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="//cdn.staticfile.org/font-awesome/4.6.2/css/font-awesome.min.css" rel="stylesheet" type="text/css"><link href="/css/main.css?v=6.3.0" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.3.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.3.0"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.3.0"><link rel="mask-icon" href="/images/logo.svg?v=6.3.0" color="#222"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Muse",version:"6.3.0",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!1,fastclick:!1,lazyload:!1,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="description" content="GoogLeNet v1论文翻译。一个经典图片分类卷积网络架构。主要思想是通过现有密集架构，组合出一个稀疏空间架构，也就是Inception模块。GoogleNet: Going Deeper with ConvolutionsChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguel"><meta name="keywords" content="Computer Vision,Deep Learning,Paper,Image Classification,GoogLeNet,GoogLeNet v1"><meta property="og:type" content="article"><meta property="og:title" content="GoogLeNet v1"><meta property="og:url" content="http://muyaan.com/archives/d37bc1de.html"><meta property="og:site_name" content="慕湮"><meta property="og:description" content="GoogLeNet v1论文翻译。一个经典图片分类卷积网络架构。主要思想是通过现有密集架构，组合出一个稀疏空间架构，也就是Inception模块。GoogleNet: Going Deeper with ConvolutionsChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguel"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://muyaan.com/archives/1534239944950.png"><meta property="og:image" content="http://muyaan.com/archives/1534324174577.png"><meta property="og:image" content="http://muyaan.com/archives/1534338990191.png"><meta property="og:image" content="http://muyaan.com/archives/1534384707106.png"><meta property="og:image" content="http://muyaan.com/archives/1534398724873.png"><meta property="og:image" content="http://muyaan.com/archives/1534398808615.png"><meta property="og:image" content="http://muyaan.com/archives/1534410272912.png"><meta property="og:image" content="http://muyaan.com/archives/1534411060350.png"><meta property="og:updated_time" content="2018-09-12T03:22:25.027Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="GoogLeNet v1"><meta name="twitter:description" content="GoogLeNet v1论文翻译。一个经典图片分类卷积网络架构。主要思想是通过现有密集架构，组合出一个稀疏空间架构，也就是Inception模块。GoogleNet: Going Deeper with ConvolutionsChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguel"><meta name="twitter:image" content="http://muyaan.com/archives/1534239944950.png"><link rel="canonical" href="http://muyaan.com/archives/d37bc1de.html"><script type="text/javascript" id="page.configurations">CONFIG.page={sidebar:""}</script><meta name="google-site-verification" content="Ojd_HrL_PelaXvKK5IkbhLbjZ_sHt6IxRzP-XPaaTw4"><meta name="baidu-site-verification" content="skdT7mIEAb"><title>GoogLeNet v1 | 慕湮</title><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?ca5844321cfb80fdf6f12b4dcc326991";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style type="text/css">.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.logo-line-after i{right:initial}</style></noscript></head><body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">慕湮</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">白日放歌须纵酒</h1></div><div class="site-nav-toggle"><button aria-label="Toggle navigation bar"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-archives menu-item-active"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><div class="reading-progress-bar"></div><article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://muyaan.com/archives/d37bc1de.html"><span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="慕湮"><meta itemprop="description" content=""><meta itemprop="image" content="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4"></span><span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="慕湮"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">GoogLeNet v1</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2018-08-16 17:31:18" itemprop="dateCreated datePublished" datetime="2018-08-16T17:31:18+08:00">2018-08-16</time> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">Edited on</span> <time title="Modified: 2018-09-12 11:22:25" itemprop="dateModified" datetime="2018-09-12T11:22:25+08:00">2018-09-12</time> </span><span id="/archives/d37bc1de.html" class="leancloud_visitors" data-flag-title="GoogLeNet v1"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Views: </span><span class="leancloud-visitors-count"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数: </span><span title="Symbols count in article">12k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="Reading time">23 mins.</span></div></div></header><div class="post-body" itemprop="articleBody"><p>GoogLeNet v1论文翻译。一个经典图片分类卷积网络架构。主要思想是通过现有密集架构，组合出一个稀疏空间架构，也就是Inception模块。</p><blockquote><h1 id="GoogleNet-Going-Deeper-with-Convolutions"><a href="#GoogleNet-Going-Deeper-with-Convolutions" class="headerlink" title="GoogleNet: Going Deeper with Convolutions"></a><a href="https://arxiv.org/abs/1409.4842" rel="external nofollow noopener noreferrer" target="_blank">GoogleNet: Going Deeper with Convolutions</a></h1><p>Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, 2014.11</p><p>Google Inc. University of North Carolina. Chapel Hill. University of Michigan. Ann Arbor Magic Leap Inc.</p></blockquote><h2><a href="#" class="headerlink"></a><a id="more"></a></h2><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>在过去三年，我们的物体分类和检测能力因深度学习和卷积网络的提升有大幅进步。一个振奋人心的消息是大部分进展不是基于更强大的硬件、更大的数据集和更大的模型，而是一系列新想法、算法和网络架构提升。没有使用新的数据源。我们的提交模型GoogLeNet比两年前的胜者架构参数少12倍，同时还要精确得多。在物体检测前沿，最大的提升不是来自更大、更深的网络，而是深度架构和经典计算机视觉的协同，如Girshick等人的R-CNN算法$^{[6]}$。</p><p>另一个不可忽视的因素是移动设备和嵌入式设备的牵引，我们算法的效率——特别是电力和内存使用——变得更重要。需要注意的是这一考虑使本论文的深度架构设计包含了这一因素，而不是固定在精确的数字上。大部分实验中，模型都设计为保持计算量为15亿次，因此它不仅仅是学术玩具，而是在大数据集上也能以合理花费实际运用的。</p><p>在本论文中我们着力于一个用于计算机视觉的高效神经网络架构，代号“Inception（没错就是盗梦空间的名字）”，它的名字来自Lin等人的NIN论文$^{[12]}$中的一个梗。通常来说可以把Inception模块看做[12]的逻辑顶点logical culmination，同时得到了来自Arora等人的理论研究$^{[2]}$的激发和指导。架构的好处在ILSVRC 2014分类和检测竞赛上进行了实验验证，显著地超越了前沿方法。</p><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p>自LeNet-5$^{[10]}$起，卷积神经网络就一直有一个标准结构—层叠的卷积层（选择性后缀归一化层和max-pooling层），后缀一个或多个全连接层。这一基础设计的变种流行于图片分类领域，并在MNIST，CIFAR和ImageNet分类上得到了最佳结果。对于大型数据集如ImageNet，近期潮流是增加层数、增大层的尺寸，同时使用dropout$^{[7]}$来解决过拟合的问题。</p><p>尽管存在对于max-pooling层对于空间信息精度损失的担忧，同样的卷积网络结构$^{[9]}$已成功用于定位$^{[9,14]}$，物体检测$^{[6,14,18,5]}$和人类姿态估计$^{[19]}$。</p><p>被灵长类视觉皮层的神经学模型所激发，Serre等人$^{[15]}$使用一系列固定的不同尺寸的Gabor filters来处理不同的尺度。我们使用了类似的策略。但与他们固定的两层深的模型不同，所有Inception结构中的filter都是需要训练的。而且Inception层重复了多次，得到GoogLeNet模型中的22层深度。</p><p>NIN是Lin等人$^{[12]}$提出的用于增加神经网络表达力的方法。在它们的模型中，加入了额外的$1\times1$卷积层以增加深度。我们在自己的架构中大量使用了该方法。不过我们设定中的$1\times1$卷积层有两个目的：它们很大程度上用做缩减维度模块，以移除计算量瓶颈，否则它会限制我们网络的大小。这不仅允许增大深度，还可以在没有明显性能损耗的情况下增大网络宽度。</p><p>最后，当下最前沿的物体检测算法是Girshick等人的R-CNN。R-CNN将检测问题分解为两个子问题：以领域不可知的形式（category-agnostic）使用低级线索如颜色和纹理来生成物体位置候选，用CNN分类器来在那些区域中识别物体类别。这一方法同时利用了使用低级线索的限位框分割的精确度和前沿CNN的强大分类能力。在我们的检测提交中采用了类似的流程，但在两个步骤中都找到了加强方法，如使用multi-box$^{[5]}$预测来获得更高的物体限位框召回率，组合方法来获得更佳的分类效果。</p><h2 id="3-Motivation-and-High-Level-Considerations"><a href="#3-Motivation-and-High-Level-Considerations" class="headerlink" title="3. Motivation and High Level Considerations"></a>3. Motivation and High Level Considerations</h2><p>提升神经网络性能最直接的方法就是增加它们的尺寸。这包括增加深度——网络的层数——以及宽度：每层单元数量。这是训练一个更高质量模型的简单又安全的方法，特别是有大量标注训练数据的情况下。但这一方法有两个主要弊端。</p><p>更大的尺寸通常意味着更多的参数，使得增大后的网络倾向于过拟合，特别是训练样本数量受限的情况下。这是主要的瓶颈，因为强力标注数据集非常耗时，也很昂贵，通常需要专家来进行区别视觉种类，如图1。</p><p><img src="/archives/./1534239944950.png" alt="图1"></p><p>另一个一致地增大网络尺寸的缺点是所需计算资源的增长。比如在一个深度视觉网络中，如果两个卷积层是相连的，任何对它们卷积核尺寸的增加都会导致计算量平方级的增加。如果增加的容量没有好好利用（如大部分权重最终为0），那么大部分计算都浪费掉了。通常计算资源都是有限的，因此计算资源的有效分布需要不随意增大尺寸，就算主要目标是提升分类质量。</p><p>一个解决这些问题的根本方法是稀疏性的引入和用更稀疏的层替换全连接层，甚至那些卷积中的。除了模仿生物系统，依据Arora等人的开创性理论工作$^{[2]}$，这还有坚固的理论基础。他们的主要成果说明，如果数据集的概率分布能通过一个大的、极稀疏的深度网络来表达，那么最优的网络拓扑可以通过分析之前层的激活值的相关统计和有着高度相关输出的神经元的聚类，一层一层地构建出来。尽管严格数学证明需要非常强的条件，但这一陈诉与我们熟知的Hebbian principle——同时激活的神经元，它们连在一起——的共振resonates，说明即使在实际应用中不那么严格的条件下，其想法也是合适的。</p><p>不幸的是，现在的计算架构对于未归一化的稀疏数据结构非常低效。就算算数运算数量缩减100倍，缓存查找和未命中的额外开销同样能占统治地位：切换到稀疏矩阵可能不会成功。稳步提升、高度调优的数值库的使用通过开发CPU和GPU硬件细节允许了极快极密集的矩阵乘法，进一步扩大了差距。而且，未归一化的稀疏模型需要更复杂的工程和计算架构。目前大部分面向视觉的机器学习系统仅把稀疏性用来利用卷积。不过，卷积是更早层的块的密集连接集合。卷积网络有在特征维度中使用随机、稀疏连接表的传统$^{[11]}$，为了打破对称性并提升训练效果，不过近年为了进一步并行优化，这一潮流变为了全连接$^{[9]}$。目前前沿的计算机视觉架构有着同样的结构。更大的filter数量和batch大小允许密集计算的高效。</p><p>这就引出一个问题，是否有一个中间步骤存在的希望：一个能利用filter级稀疏度的架构，如理论建议的那样，但通过利用密集矩阵的计算来开发我们目前硬件的潜力。大量稀疏矩阵计算的文献（如[3]）建议将稀疏矩阵聚集为相对密集的子矩阵能为稀疏矩阵乘法带来有竞争力的性能。看起来在不久的将来类似的方法就会用于未归一化深度学习架构的自动构建。</p><p>Inception架构起源于对复杂网络拓扑结构算法的假定输出评定，一个用于模拟[2]隐含的用于视觉网络的稀疏架构，并覆盖了现成的密集组件的假定输出。尽管是个高度投机的事业，与参照网络[12]相比，很早就发现了少许优势。经过一点调优后，差距进一步扩大，Inception被证明在定位和物体检测中用作[5]和[6]的基础网络特别有用。有趣的是，尽管大部分原始架构选择都分别被怀疑，被彻底测试过，但结果它们都接近于局部最优。有一点需要十分小心：尽管Inception架构在计算机视觉获得了成功，它对于导致它的构建的指导原则能否有贡献仍值得怀疑。弄清楚这个需要更彻底的分析和验证。</p><h2 id="4-Architecture-Details"><a href="#4-Architecture-Details" class="headerlink" title="4. Architecture Details"></a>4. Architecture Details</h2><p>Inception架构的主要思想是一个卷积视觉网络的最优局部稀疏结构能通过现成密集组件近似并覆盖到什么程度。需要注意平移不变性需要我们网络使用卷积结构构建。我们需要做的是找到最优局部结构，并在空间上不断重复它。Arora等人$^{[2]}$建议采用一层层的结构，每一层需要分析上一层的相关性统计，并把高度相关的单元聚集到一起。这些簇由下一层的单元组成，连接至前一层的单元。我们假定前一层的每个单元都对应于输入图片的一些区域，它们聚集起来就成了滤波器组filter banks。在更低的层中（更接近输入图片）相关的单元会关注局部区域。因此我们最终会得到许多关注到一个区域的簇，它们能通过下一层的$1\times1$卷积覆盖cover，如[12]所提出的那样。不过仍有少量的空间分布得更开的簇，它们可以被更大块patch的卷积覆盖，随着区域变大，patch的数量会减小。为了避免块对齐patch-alignment问题，目前的Inception架构被限制为卷积核大小：$1\times1,3\times3,5\times5$，这一决策的考虑更多是基于方便，而不是必要。这也意味着建议结构是把所有层将它们的滤波器组连接为一个输出向量，作为下一步的输入。另外，由于池化操作对于当前的卷积网络的成功非常关键，说明在每个这样的步骤中增加一个额外的并行池化路径应该能有额外的收益（见图2(a)）。</p><p>所有这些Inception模块都一个个堆叠起来，它们输出的相关性统计会有很大差异：因为高度抽象的特征由更高层捕获，它们的空间关注度会减小。这也说明随着层数变高，$3\times3$和$5\times5$占比应该变高。</p><p>以上模块的一个大问题就是，就算$5\times5$卷积数量很少，它如果放置于有大量卷积核的卷积层之上，也会过于昂贵。一旦加入了池化单元，这一问题更严重了：输出的卷积核数量等于之前步骤的卷积核数量。加入了池化层后，逐步增加的输出数量不可逆转。尽管这一架构也许能覆盖最优空间结构，它也是非常低效的，导致几层之内计算量就会爆炸。</p><p><img src="/archives/./1534324174577.png" alt="图2"></p><p>这导致了第二版Inception架构：只要计算量会增大太多，就降低维度。这是基于嵌入式embedding的成功：就算低维的嵌入也能包含一个相对大的图片块的许多信息。不过嵌入式以一种密集、压缩的形式表达信息，而压缩过的信息更难处理。这一表达需要在大部分地方保持稀疏（如[2]的条件所需），并在信号不得不被全部聚集时才压缩它们。也就是说，$1\times1$卷积用在昂贵的$3\times3$和$5\times5$之前以降低维度。除了用作维度缩减，它们还包含了ReLU激活。最终结果是图2(b)。</p><p>通常来说，一个Inception网络是一个由以上类型模块互相堆叠，间或有步长为2的max-pooling层来减半分辨率所组成的。出于技术上的原因（训练中的内存效率），似乎保持底层传统卷积形式，在更高的层才使用Incepition也能收益。这不是严格必须的，仅是反应了我们目前实现上的一些基础结构的低效。</p><p>这一架构有个有用的方面，它允许每个阶段的单元数量显著增长，而不会导致之后阶段的计算量复杂度不受控制地增长。这是通过有着更大尺寸的昂贵卷积之前的无处不在的维度缩减的使用所获得的。而且这一设计遵从了实践直觉：视觉信息应在不同的尺度处理并聚合使得下一阶段能从不同尺度同时抽象特征。</p><p>这一计算资源使用的提升允许同时增大每一阶段的宽度和阶段数量，而不用面临计算困难。任何人都能使用Inception结构来创建稍低级但计算量更少的版本。我们已经发现对计算资源的平衡控制使网络比同样性能的无Inception架构的网络快3-10倍，不过这需要小心的手工设计。</p><h2 id="5-GoogLeNet"><a href="#5-GoogLeNet" class="headerlink" title="5. GoogLeNet"></a>5. GoogLeNet</h2><p>GooLeNet这个名字特指我们的ILSVRC 2014竞赛的提交所用的Inception架构。我们也用了一个更深、更宽的Inception网络，质量稍高，但将其加入模型组合只提高了一点点结果。我们省略了那个网络的细节，因为经验证明额外的建筑参数影响相对小。表1解释了竞赛中用得最多的Inception实例。这个网络（用不同的图片块采样方法训练）占了我们7个组合模型中的6个。</p><p><img src="/archives/./1534338990191.png" alt="表1"></p><p>所有的卷积，包括Inception中的，使用ReLU激活。我们网络的输入是RGB空间的$224\times224$，均值为0。“#$3\times3$”和“#$5\times5$”指$3\times3$和$5\times5$卷积层前$1\times1$卷积核数量。也可以看到pool proj列中内建max-pooling后的projection层的$1\times1$卷积核数量。所有的reduction/projection层也同样使用ReLU激活。</p><p>网络在设计时就考虑了计算效率和实用性，它能在各种设备上运行，甚至是计算资源有限的设备，特别是内存较低的那些。只计算有参数的层的话，网络共22层（如果计入池化层，则是27）。网络结构中所有的层数（独立建筑块）约100个。具体数字取决于如何对层计数。基于[12]，我们在分类器前使用了平均池化，但在我们的实现中增加了额外的线性层。线性层允许我们轻松地将网络适配到其他标注数据集，但它的使用主要出于方便，并未期待有何影响。我们发现将全连接层替换为平均池化层获得了0.6%的top-1准确度提升，但就算移除了全连接层，dropout层依然至关重要。</p><p>当网络深度相对大时，能高效地在这些层中反向传播就重要起来了。更浅网络在这个任务上的强劲表现说明网络中间的层产生的特征非常有识别力。通过为这些中间层连接额外的分类器，可以期待这些更低阶段的分类器有辨识力。这是出于对梯度消失问题的考虑，同时提供了正则化。分类器以更小的卷积网络形式放置于Inception(4a)和(4b)模块输出之上。在训练中，它们的loss会以有折扣的权重加入整个网络的loss中（额外的分类器loss权重为0.3）。在应用时，这些额外的网络会被丢弃。稍后的控制实验显示额外的网络效果相对小（约0.5%），而只需一个那样的网络就能获得同样的效果。</p><p>网络额外结构如下：</p><ul><li>一个尺寸为$5\times5$的步长为3的平均池化层。用作了(4a)的一个$4\times4\times512$的输出和(4d)的$4\times4\times528$。</li><li>一个有128个$1\times1$卷积核的层，进行维度缩减和ReLU</li><li>一个有1024个单元和ReLU的全连接层</li><li>一个70%的比例的dropout层</li><li>一个使用softmax做为分类器的线性层（与主分类器预测同样的1000类，但使用时移除）</li></ul><p>整个网络如图3。<br><img src="/archives/./1534384707106.png" alt="图3"></p><h2 id="6-Training-Methodology"><a href="#6-Training-Methodology" class="headerlink" title="6. Training Methodology"></a>6. Training Methodology</h2><p>GoogLeNet是使用分布式机器学习系统DistBelief$^{[4]}$训练的，使用了适度的模型和数据并行。尽管我们只使用了CPU实现，粗略估计GoogLeNet能使用少量高端GPU在一周内训练至收敛，主要限制在于内存使用。我们的训练使用了异步SGD，动量为0.9，固定的学习率计划（每8个epoch降低4%的学习率）。使用了Polyak averaging$^{[13]}$来得到测试使用的最终模型。</p><p>近几个月图片采样方法已充分改变，已收敛的模型是用其它选项训练的，有时还改变了超参数，如dropout和学习率。因此很难给出一个决定性的最有效的训练指导。受[8]激发，有些模型主要在相对小的裁切上训练的，其余在更大的。竞赛后验证了一个管理非常有用，包括以不同的patch size采样，从8%到100%，长宽比保持在$[\frac 3 4, \frac 4 3]$。我们同样发现Andrew Howard的广度扰动$^{[8]}$也很能对抗过拟合。</p><h2 id="7-ILSVRC-2014-Classification-Challenge-Setup-and-Results"><a href="#7-ILSVRC-2014-Classification-Challenge-Setup-and-Results" class="headerlink" title="7. ILSVRC 2014 Classification Challenge Setup and Results"></a>7. ILSVRC 2014 Classification Challenge Setup and Results</h2><p>ILSVRC 2014分类挑战是将图片分类到ImageNet层级中的1000个叶子种类。共约120万张训练图片，5万验证和10万测试。每个图片与一个gt类别关联，性能是基于最高得分的预测测量的。通常报告两个数据：top-1准确度，把gt类与最高预测相比；和top-5错误率，把gt类与前5预测相比，如果gt在这5个类中就认为分类正确。这一挑战使用了top-5错误率作为排名依据。</p><p>我们没有使用额外的数据来训练我们的参赛模型。除了本论文提到的训练技术，我们还采用了一系列测试技术来获得更高性能：</p><ol><li>我们独立训练了同样模型的GoogLeNet的7个版本（包括一个更宽的版本），并用它们组合预测。这些模型以同样的初始化设置和学习率策略训练。它们仅有采样方法和图片输入顺序随机化的不同。</li><li>在测试时我们采用了Krizhevsky等人的更激进的切图方法$^{[9]}$。特别的是，我们将图片缩放到4个尺度，让短边分别是256，288，320和352，从缩放后的图片取左、中、右三个方块（对于肖像照就是上、中、下方块）。对于每个方块我们取四个角和中心的$224\times224$块，连同整块缩放，共6种，以及它们的镜像版本。这就为每张图片带来$4\times3\times6\times2 = 144$个方块。Andrew Howard$^{[8]}$曾在去年的提交中使用了类似的方法，我们靠经验推断比这一方法要稍差。需要提到的是实际应用也许不需要如此激进的切块，因为随着切块数量足够之后，更多块的收益变得很小（我们随后会展示）。</li><li>softmax概率在多个块上平均。我们在验证数据集上分析了不同的方法，如在每个块上max pooling，并在分类器平均。但比简单的平均性能低。</li></ol><p>在剩余的论文中，我们分析了对于最终提交总体性能有帮助的多个因素。</p><p>我们的最终提交在验证和测试集上top-5误差为6.67%，排名第一。相对2012的SuperVIsion有近56.5%的相对提升，相比去年的最佳方法（Clarifai）有40%的相对提升，它们还都使用了额外数据来训练。表2是过去3年的顶级方法统计。</p><p><img src="/archives/./1534398724873.png" alt="表2"></p><p>我们也分析并报告了多个测试选择的性能，通过测试时使用不同的模型数量和切块数量，见表3。当使用一个模型时，选用了在验证集上错误率最低的模型。所有数据都基于验证集，以避免过拟合测试数据统计。</p><p><img src="/archives/./1534398808615.png" alt="表3"></p><h2 id="8-ILSVRC-2014-Detection-Challenge-Setup-and-Results"><a href="#8-ILSVRC-2014-Detection-Challenge-Setup-and-Results" class="headerlink" title="8. ILSVRC 2014 Detection Challenge Setup and Results"></a>8. ILSVRC 2014 Detection Challenge Setup and Results</h2><p>ILSVRC检测任务是为图片中的可能为200种的物体产生限位框。当类匹配且与gt框Jaccard交集大于50%，就认为检测正确。无关检测被视作false positive，将被惩罚。与分类任务不同的是，图片中可能没有物体，或很多物体，而它们尺度相差巨大。结果以mAP的形式报告。我们采用了与R-CNN类似的方法，但将Inception模型用作了区域分类器。另外，候选区域步骤使用multibox和SS的合并以获得更高的限位框召回率。为了降低false positive数量，超像素superpixel尺寸增大了两倍。这将SS算法产出的候选区域数量减半。我们加入了来自multibox的200个候选区域，其中共60%的候选被[6]使用，同时将覆盖率从92%提升至93%。这一区域数量裁剪和覆盖率的增加，一起为单模型增加了1%的mAP。最后，我们使用了6个GoogLeNet的组合来为每个候选区域分类。需要注意与R-CNN不同的是，由于时间关系我们没有使用限位框回归。</p><p>我们首先报告了最近检测结果，并展示了自第一届检测竞赛依赖的进展。与2013年的结果相比，精确度接近翻倍。最佳性能均使用了CNN。我们在表4列出了各队伍的官方得分和共同策略：额外数据，组合模型或上下文模型。额外数据通常是ILSVRC12分类数据，用于预训练一个用于分类的模型。有的队伍也提到了定位数据的使用。因为检测集中并未包含较好的定位数据，他们可以预训练一个通用的限位框回归器。GoogLeNet没使用额外的定位数据进行预训练。</p><p><img src="/archives/./1534410272912.png" alt="表4"></p><p>在表5中，我们比较了单模型的效果。最佳模型是Deep Insight，但它使用3个的组合模型时，只提升了0.3，而GoogLeNet组合效果强得多。</p><p><img src="/archives/./1534411060350.png" alt="表5"></p><h2 id="9-Conclusion"><a href="#9-Conclusion" class="headerlink" title="9.Conclusion"></a>9.Conclusion</h2><p>我们的结果成为了通过现成的密集建筑块来逼近期望的最优空间结构是可行的提升用于计算机视觉的神经网络的方法的强力证明。这一方法的主要优点是比起更浅更窄的架构，仅需合理的计算量增长就能获得很大的质量增长。</p><p>我们的物体检测研究具有竞争力，尽管没有使用上下文或进行限位框回归，说明Inception架构仍有潜力。</p><p>对于分类和检测，可以预期有着近似深度和宽度的，更昂贵的非Inception网络也能获得近似的质量。但我们的方法证明了向稀疏架构发展是有用且可行的想法。这建议了未来研究基于[2]的基础，自动地创建稀疏、精炼的结构，以及将Inception架构的思想应用于其它领域。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] Know your meme: We need to go deeper.<br><a href="http://knowyourmeme.com/memes/we-need-to-go-deeper" rel="external nofollow noopener noreferrer" target="_blank">http://knowyourmeme.com/memes/we-need-to-go-deeper</a>.<br>Accessed: 2014-09-15.<br>[2] S. Arora, A. Bhaskara, R. Ge, and T. Ma. Provable<br>bounds for learning some deep representations. CoRR,<br>abs/1310.6343, 2013.<br>[3] U. V. C¸ atalyurek, C. Aykanat, and B. Uc¸ar. On ¨<br>two-dimensional sparse matrix partitioning: Models,<br>methods, and a recipe. SIAM J. Sci. Comput.,<br>32(2):656–683, Feb. 2010.<br>[4] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin,<br>M. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang,<br>Q. V. Le, and A. Y. Ng. Large scale distributed deep<br>networks. In P. Bartlett, F. Pereira, C. Burges, L. Bottou,<br>and K. Weinberger, editors, NIPS, pages 1232–1240. 2012.<br>[5] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov.<br>Scalable object detection using deep neural networks.<br>In CVPR, 2014.<br>[6] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik.<br>Rich feature hierarchies for accurate object detection<br>and semantic segmentation. In Computer Vision and<br>Pattern Recognition, 2014. CVPR 2014. IEEE Conference<br>on, 2014.<br>[7] G. E. Hinton, N. Srivastava, A. Krizhevsky,<br>I. Sutskever, and R. Salakhutdinov. Improving neural<br>networks by preventing co-adaptation of feature detectors.<br>CoRR, abs/1207.0580, 2012.<br>[8] A. G. Howard. Some improvements on deep convolutional<br>neural network based image classification.<br>CoRR, abs/1312.5402, 2013.<br>[9] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet<br>classification with deep convolutional neural<br>networks. In Advances in Neural Information Processing<br>Systems 25, pages 1106–1114, 2012.<br>[10] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.<br>Howard, W. Hubbard, and L. D. Jackel. Backpropagation<br>applied to handwritten zip code recognition.<br>Neural Comput., 1(4):541–551, Dec. 1989.<br>[11] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner.<br>Gradient-based learning applied to document recognition.<br>Proceedings of the IEEE, 86(11):2278–2324,1998.<br>[12] M. Lin, Q. Chen, and S. Yan. Network in network.<br>CoRR, abs/1312.4400, 2013.<br>[13] B. T. Polyak and A. B. Juditsky. Acceleration of<br>stochastic approximation by averaging. SIAM J. Control<br>Optim., 30(4):838–855, July 1992.<br>[14] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,<br>and Y. LeCun. Overfeat: Integrated recognition,<br>localization and detection using convolutional<br>networks. CoRR, abs/1312.6229, 2013.<br>[15] T. Serre, L. Wolf, S. M. Bileschi, M. Riesenhuber, and<br>T. Poggio. Robust object recognition with cortex-like<br>mechanisms. IEEE Trans. Pattern Anal. Mach. Intell.,<br>29(3):411–426, 2007.<br>[16] F. Song and J. Dongarra. Scaling up matrix computations<br>on shared-memory manycore systems with 1000<br>cpu cores. In Proceedings of the 28th ACM International<br>Conference on Supercomputing, ICS ’14, pages<br>333–342, New York, NY, USA, 2014. ACM.<br>[17] I. Sutskever, J. Martens, G. E. Dahl, and G. E. Hinton.<br>On the importance of initialization and momentum in<br>deep learning. In ICML, volume 28 of JMLR Proceedings,<br>pages 1139–1147. JMLR.org, 2013.<br>[18] C. Szegedy, A. Toshev, and D. Erhan. Deep neural networks<br>for object detection. In C. J. C. Burges, L. Bottou,<br>Z. Ghahramani, and K. Q. Weinberger, editors,<br>NIPS, pages 2553–2561, 2013.<br>[19] A. Toshev and C. Szegedy. Deeppose: Human<br>pose estimation via deep neural networks. CoRR,<br>abs/1312.4659, 2013.<br>[20] K. E. A. van de Sande, J. R. R. Uijlings, T. Gevers,<br>and A. W. M. Smeulders. Segmentation as selective<br>search for object recognition. In Proceedings of the<br>2011 International Conference on Computer Vision,<br>ICCV ’11, pages 1879–1886, Washington, DC, USA,2011. IEEE Computer Society.<br>[21] M. D. Zeiler and R. Fergus. Visualizing and understanding<br>convolutional networks. In D. J. Fleet, T. Pajdla,<br>B. Schiele, and T. Tuytelaars, editors, ECCV,<br>volume 8689 of Lecture Notes in Computer Science,<br>pages 818–833. Springer, 2014.</p></div><div class="popular-posts-header">Related Posts</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/b2e32499.html" rel="bookmark">VGGNet</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/719cc2e5.html" rel="bookmark">SSD</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/ae74f67c.html" rel="bookmark">Aligned ReID</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/ed3a751d.html" rel="bookmark">ResNet</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/423509a9.html" rel="bookmark">YOLOv2 and YOLO9000</a></div></li></ul><div><ul class="post-copyright"><li class="post-copyright-author"><strong>Post author: </strong>慕湮</li><li class="post-copyright-link"><strong>Post link: </strong><a href="http://muyaan.com/archives/d37bc1de.html" title="GoogLeNet v1">http://muyaan.com/archives/d37bc1de.html</a></li><li class="post-copyright-license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noopener noreferrer" target="_blank">CC BY-NC-SA 4.0</a> unless stating additionally.</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Computer-Vision/" rel="tag"><i class="fa fa-tag"></i> Computer Vision</a> <a href="/tags/Deep-Learning/" rel="tag"><i class="fa fa-tag"></i> Deep Learning</a> <a href="/tags/Paper/" rel="tag"><i class="fa fa-tag"></i> Paper</a> <a href="/tags/Image-Classification/" rel="tag"><i class="fa fa-tag"></i> Image Classification</a> <a href="/tags/GoogLeNet/" rel="tag"><i class="fa fa-tag"></i> GoogLeNet</a> <a href="/tags/GoogLeNet-v1/" rel="tag"><i class="fa fa-tag"></i> GoogLeNet v1</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/archives/2c51a51a.html" rel="next" title="YOLOv3"><i class="fa fa-chevron-left"></i> YOLOv3</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/archives/49cd3ce.html" rel="prev" title="Batch Normalization">Batch Normalization <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">Table of Contents</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">Overview</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4" alt="慕湮"><p class="site-author-name" itemprop="name">慕湮</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">21</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">30</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/tycallen" target="_blank" title="GitHub" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-github"></i>GitHub</a></span><br><span class="links-of-author-item"><a href="mailto:tyc.allen@gmail.com" target="_blank" title="E-Mail" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span><br><span class="links-of-author-item"><a href="http://weibo.com/pojunallen" target="_blank" title="微博" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-weibo"></i>微博</a></span><br><span class="links-of-author-item"><a href="https://tuchong.com/1070837" target="_blank" title="图虫" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-globe"></i>图虫</a></span><br></div><div class="links-of-blogroll motion-element links-of-blogroll-block"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> 友情链接</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="https://dotrabbit.github.io/" title="dotrabbit" target="_blank" rel="external nofollow noopener noreferrer">dotrabbit</a></li></ul></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#GoogleNet-Going-Deeper-with-Convolutions"><span class="nav-text">GoogleNet: Going Deeper with Convolutions</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-text"></span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Introduction"><span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Related-Work"><span class="nav-text">2. Related Work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Motivation-and-High-Level-Considerations"><span class="nav-text">3. Motivation and High Level Considerations</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Architecture-Details"><span class="nav-text">4. Architecture Details</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-GoogLeNet"><span class="nav-text">5. GoogLeNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-Training-Methodology"><span class="nav-text">6. Training Methodology</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-ILSVRC-2014-Classification-Challenge-Setup-and-Results"><span class="nav-text">7. ILSVRC 2014 Classification Challenge Setup and Results</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-ILSVRC-2014-Detection-Challenge-Setup-and-Results"><span class="nav-text">8. ILSVRC 2014 Detection Challenge Setup and Results</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-Conclusion"><span class="nav-text">9.Conclusion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-text">References</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; 2015 &mdash; <span itemprop="copyrightYear">2018</span> <span class="with-love" id="animate"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">慕湮</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="Symbols count total">312k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="Reading time total">9:27</span></div><div class="powered-by">Powered by <a class="theme-link" target="_blank" rel="external nofollow noopener noreferrer" href="https://hexo.io">Hexo</a></div><span class="post-meta-divider">|</span><div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" rel="external nofollow noopener noreferrer" href="https://theme-next.org">NexT.Muse</a></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="//cdn.staticfile.org/jquery/2.1.3/jquery.min.js"></script><script type="text/javascript" src="//cdn.staticfile.org/velocity/1.2.1/velocity.min.js"></script><script type="text/javascript" src="//cdn.staticfile.org/velocity/1.2.1/velocity.ui.min.js"></script><script type="text/javascript" src="/lib/reading_progress/reading_progress.js"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=6.3.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=6.3.0"></script><script defer type="text/javascript" src="/js/src/onload.js?v=6.3.0"></script><script src="https://cdn.staticfile.org/pace/1.0.2/pace.min.js"></script><link href="https://cdn.staticfile.org/pace/1.0.2/themes/blue/pace-theme-big-counter.min.css" rel="stylesheet"><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script><script>AV.initialize("UWGXBMGbguCDhSCbC3NrUQtL-gzGzoHsz","ke1jrA5b6VyR89Kqqqwf2kPP")</script><script>function showTime(e){var t=new AV.Query(e),c=[],u=$(".leancloud_visitors");u.each(function(){c.push($(this).attr("id").trim())}),t.containedIn("url",c),t.find().done(function(e){var t=".leancloud-visitors-count";if(0!==e.length){for(var n=0;n<e.length;n++){var o=e[n],i=o.get("url"),s=o.get("time"),r=document.getElementById(i);$(r).find(t).text(s)}for(n=0;n<c.length;n++){i=c[n],r=document.getElementById(i);var l=$(r).find(t);""==l.text()&&l.text(0)}}else u.find(t).text(0)}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(i){var e=$(".leancloud_visitors"),s=e.attr("id").trim(),r=e.attr("data-flag-title").trim(),t=new AV.Query(i);t.equalTo("url",s),t.find({success:function(e){if(0<e.length){var t=e[0];t.fetchWhenSave(!0),t.increment("time"),$(document.getElementById(s)).find(".leancloud-visitors-count").text(t.get("time")),t.save(null,{success:function(e){},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var n=new i,o=new AV.ACL;o.setPublicReadAccess(!0),o.setPublicWriteAccess(!0),n.setACL(o),n.set("title",r),n.set("url",s),n.set("time",1),n.save(null,{success:function(e){$(document.getElementById(s)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):1<$(".post-title-link").length&&showTime(e)})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script async type="text/javascript" color="99,184,255" opacity="0.5" zindex="-2" count="99" src="//cdn.staticfile.org/canvas-nest.js/1.0.0/canvas-nest.js"></script></body></html>