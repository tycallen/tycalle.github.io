<!DOCTYPE html><html class="theme-next muse use-motion" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><script src="https://cdn.staticfile.org/pace/1.0.2/pace.min.js"></script><link href="https://cdn.staticfile.org/pace/1.0.2/themes/blue/pace-theme-big-counter.min.css" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="//cdn.staticfile.org/font-awesome/4.6.2/css/font-awesome.min.css" rel="stylesheet" type="text/css"><link href="/css/main.css?v=6.3.0" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.3.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.3.0"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.3.0"><link rel="mask-icon" href="/images/logo.svg?v=6.3.0" color="#222"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Muse",version:"6.3.0",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!1,fastclick:!1,lazyload:!1,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="description" content="这周读完的第二篇论文，对当前状态挺满意的。当然今后工作时不会有这样多的时间读论文，不过一周至少一篇还是需要保持的。  Fast R-CNNRoss Girshick  2015UC Berkeley"><meta name="keywords" content="Computer Vision,Deep Learning,Paper,Object Detection,Fast R-CNN"><meta property="og:type" content="article"><meta property="og:title" content="Fast R-CNN"><meta property="og:url" content="http://muyaan.com/archives/9bab3bc6.html"><meta property="og:site_name" content="慕湮"><meta property="og:description" content="这周读完的第二篇论文，对当前状态挺满意的。当然今后工作时不会有这样多的时间读论文，不过一周至少一篇还是需要保持的。  Fast R-CNNRoss Girshick  2015UC Berkeley"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://muyaan.com/archives/1532588038900.png"><meta property="og:image" content="http://muyaan.com/archives/1532671669590.png"><meta property="og:image" content="http://muyaan.com/archives/1532671687443.png"><meta property="og:image" content="http://muyaan.com/archives/1532671651694.png"><meta property="og:image" content="http://muyaan.com/archives/1532681584378.png"><meta property="og:image" content="http://muyaan.com/archives/1532683809866.png"><meta property="og:image" content="http://muyaan.com/archives/1532685413759.png"><meta property="og:image" content="http://muyaan.com/archives/1532687627713.png"><meta property="og:image" content="http://muyaan.com/archives/1532745654999.png"><meta property="og:image" content="http://muyaan.com/archives/1532745896409.png"><meta property="og:updated_time" content="2018-09-12T03:22:25.024Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Fast R-CNN"><meta name="twitter:description" content="这周读完的第二篇论文，对当前状态挺满意的。当然今后工作时不会有这样多的时间读论文，不过一周至少一篇还是需要保持的。  Fast R-CNNRoss Girshick  2015UC Berkeley"><meta name="twitter:image" content="http://muyaan.com/archives/1532588038900.png"><link rel="canonical" href="http://muyaan.com/archives/9bab3bc6.html"><script type="text/javascript" id="page.configurations">CONFIG.page={sidebar:""}</script><meta name="google-site-verification" content="Ojd_HrL_PelaXvKK5IkbhLbjZ_sHt6IxRzP-XPaaTw4"><meta name="baidu-site-verification" content="skdT7mIEAb"><title>Fast R-CNN | 慕湮</title><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?ca5844321cfb80fdf6f12b4dcc326991";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style type="text/css">.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.logo-line-after i{right:initial}</style></noscript></head><body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">慕湮</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">白日放歌须纵酒</h1></div><div class="site-nav-toggle"><button aria-label="Toggle navigation bar"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-archives menu-item-active"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li></ul></nav><script type="text/javascript" color="99,184,255" opacity="0.5" zindex="-2" count="99" src="//cdn.staticfile.org/canvas-nest.js/1.0.0/canvas-nest.js"></script></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><div class="reading-progress-bar"></div><article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://muyaan.com/archives/9bab3bc6.html"><span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="慕湮"><meta itemprop="description" content=""><meta itemprop="image" content="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4"></span><span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="慕湮"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">Fast R-CNN</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2018-07-28 11:29:52" itemprop="dateCreated datePublished" datetime="2018-07-28T11:29:52+08:00">2018-07-28</time> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">Edited on</span> <time title="Modified: 2018-09-12 11:22:25" itemprop="dateModified" datetime="2018-09-12T11:22:25+08:00">2018-09-12</time> </span><span id="/archives/9bab3bc6.html" class="leancloud_visitors" data-flag-title="Fast R-CNN"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Views: </span><span class="leancloud-visitors-count"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数: </span><span title="Symbols count in article">16k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="Reading time">30 mins.</span></div></div></header><div class="post-body" itemprop="articleBody"><p>这周读完的第二篇论文，对当前状态挺满意的。当然今后工作时不会有这样多的时间读论文，不过一周至少一篇还是需要保持的。</p><blockquote><p><a href="https://arxiv.org/pdf/1504.08083.pdf" rel="external nofollow noopener noreferrer" target="_blank">Fast R-CNN</a><br>Ross Girshick 2015<br>UC Berkeley</p></blockquote><a id="more"></a><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>本论文提出了一种用于物体检测的基于Fast Region的卷积神经网络（Fast R-CNN）。Fast R-CNN是基于先前的工作。与前作相比，Fast R-CNN引入了一些创新来提升训练、测试速度，还提升了检测准确率。基于VGG16来说，相比R-CNN，Fast R-CNN训练速度快9倍，测试速度快213倍，在PASCAL VOC 2012上获得了更高的mAP。与SPPnet相比，Fast R-CNN训练VGG16速度快3倍，测试快10倍，也更精确。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>近年来，深度卷积网络显著地提高了图片分类和物体检测准确度。与图片分类相比，物体检测因其所需的更复杂的解决方法，要更具挑战。因为复杂度的原因，当前的多阶段训练模型的方法既慢又不优雅。</p><p>因为检测需要准确定位物体，复杂度自然上升，这也产生了两个主要挑战。首先，需要处理大量的候选物体位置（通常称为proposal）。这些候选仅仅提供了粗略的定位，需要改善以获得精确位置。这些问题的解决方案通常需要在速度、精确度或简洁度上妥协。</p><p>在本论文中，我们将时下最新的基于卷积网络的物体检测器训练过程流水线化了。我们提出了一个单步训练算法，将proposal分类和精确空间定位训练结合了起来。</p><p>结果是本方法能训练极深的检测网络（VGG16），比R-CNN快9倍，比SPPnet快3倍。在运行时，检测网络处理一张图片只需0.3秒（出去生成proposal的时间）。同时在PASCAL VOC 2012上取得了最佳的66%的mAP（R-CNN为62%）。</p><h3 id="1-1-R-CNN-and-SPPnet"><a href="#1-1-R-CNN-and-SPPnet" class="headerlink" title="1.1 R-CNN and SPPnet"></a>1.1 R-CNN and SPPnet</h3><p>R-CNN在使用卷积网络对proposal分类上取得了不错的成效，但它仍有显著的缺点：</p><ol><li><strong>训练是多阶段的</strong> R-CNN首先在proposal上用log loss调优，再用CNN特征拟合SVM，最后训练一个限位框回归器</li><li><strong>训练耗时、耗空间</strong> 对于SVM和限位框回归器的训练，特征是对每张图片的每个proposal计算并存在硬盘上的。对于深度卷积网络如VGG16，需要花费2.5块GPU才能处理完VOC 07训练验证集的5000张图片。这些特征需要数百GB的存储空间。</li><li><strong>物体检测慢</strong> 在测试时，特征从每张图片的每个proposal上提取，用VGG16进行检测需要47秒才能处理一张图片（单GPU）</li></ol><p>R-CNN慢是因为它对每个proposal都进行了一次CNN前向传播，没有使用共享计算。Spatial pyramid pooling networks(SPPnet)$^{[11]}$就提出来通过共享技术加速R-CNN。SPPnet即是为整个输入图片计算一个feature map，并基于从共享的feature map提取的proposal的特征向量来进行分类。一个proposal的特征是通过对feature map所属部分max pooling到固定大小（如6*6）输出得到的。多个不同大小的输出、池化再连接，就是SPP，空间金字塔池化$^{[15]}$。SPPnet为R-CNN提速了10~100倍。训练时间也因为更快的proposal特征抽取缩减了3倍。</p><p>SPPnet也有其缺点。像R-CNN一样，它的训练也有多个阶段，包括抽取特征，用log loss调优网络，训练SVM，最后拟合一个限位框回归器。特征也是写入磁盘的。不过不像R-CNN，[11]中提出的调优算法无法更新在SPP之前的卷积层。那么这一限制（固定的卷积层）限制了更深的网络精确度就不奇怪了。</p><h3 id="1-2-Contributions"><a href="#1-2-Contributions" class="headerlink" title="1.2 Contributions"></a>1.2 Contributions</h3><p>我们提出了一个新的训练算法，解决了R-CNN和SPPnet的缺点，同时提升了速度和准确度。Fast R-CNN方法有如下优点：</p><ol><li>比R-CNN和SPPnet更高的检测质量（mAP）</li><li>训练是单过程的，使用多任务loss(multi-task loss)</li><li>训练能更新网络中的所有层</li><li>不需要硬盘来缓存特征</li></ol><p>Fast R-CNN用python和C++实现，基于MIT协议<a href="https://github.com/rbgirshick/fast-rcnn" rel="external nofollow noopener noreferrer" target="_blank">开源</a>。</p><h2 id="2-Fast-R-CNN-architecture-and-training"><a href="#2-Fast-R-CNN-architecture-and-training" class="headerlink" title="2. Fast R-CNN architecture and training"></a>2. Fast R-CNN architecture and training</h2><p>图1说明了Fast R-CNN的结构。Fast R-CNN接收一张图片和一系列proposal作为输入。网络首先用数个卷积层和池化层处理图片，生成一个feature map。接着一个region of interest（RoI）层为每个proposal从feature map中提取一个固定长度的特征向量。每个特征向量输入到一系列全连接层，最终分为两个兄弟输出层：一个针对所有K类物体和一个背景类的输出softmax概率，另一层为每个类输出4个实数值。每一组4值编码都是对K类中的一个限位框位置改进。</p><p><img src="/archives/./1532588038900.png" alt="图1"></p><h3 id="2-1-The-RoI-pooling-layer"><a href="#2-1-The-RoI-pooling-layer" class="headerlink" title="2.1 The RoI pooling layer"></a>2.1 The RoI pooling layer</h3><p>兴趣区池化层使用max pooling来将任意有效兴趣区中的特征转换为一个有着固定$H \times W $（如7*7）的小的feature map。其中H和W是超参数，与特定的RoI无关。在本论文中，一个RoI是一个卷积特征网络的矩形窗口。每个RoI用一个四元组$(r,c,h,w)$定义，其中$(r,c)$定义了左上角坐标，$(h,w)$定义了高宽。</p><p>RoI max pooling层工作如下：将h * w的RoI窗口分割为大小约为$h/H \times w/W$的H * W个子窗口，并将每个子窗口的max pooling值输出到输出矩阵的对应位置。像标准max pooling一样，pooing是各channel独立进行的。RoI层是SPPnet中spatial pyramid pooling的特种形式，即只有一个pyramid level。我们使用了[11]中提出的pooling subwindow 计算方法。</p><h3 id="2-2-Initializing-from-pre-trained-networks"><a href="#2-2-Initializing-from-pre-trained-networks" class="headerlink" title="2.2 Initializing from pre-trained networks"></a>2.2 Initializing from pre-trained networks</h3><p>我们用了3个在ImageNet上预训练的网络，每个有5个max pooling层和5到13个卷积层。当一个预训练的网络用作Fast R-CNN时，有三个变换需要进行。</p><ol><li>最后一个max pooling层用RoI层代替，设置H和W大小匹配网络的第一个fc层（对于VGG16是H=W=7）</li><li>将网络的最后一个fc和softmax层（训练作ImageNet的1000类分类问题）用两个前面描述过的兄弟层（一个fc加softmax进行K+1分类和一个领域相关的限位框回归）</li><li>网络修改为接收两个输入，图片列表和图片中的RoI列表</li></ol><h3 id="2-3-Fine-tuning-for-detection"><a href="#2-3-Fine-tuning-for-detection" class="headerlink" title="2.3 Fine-tuning for detection"></a>2.3 Fine-tuning for detection</h3><p>用反向传播训练所有网络权重是Fast R-CNN的一个重要能力。首先，让我们解释为何SPPnet不能更新spatial pyramid pooling层以下的卷积层权重（caffe中网络自底向上）。</p><p>根本原因是在每个训练样本（如RoI）来自不同图片时，进行SPP反向传播效率极低。而这正是R-CNN和SPPnet训练的方式。这种低效源于每个RoI也许有个很大的感受野，常常横跨整个图片。因为前向传播需要处理整个感受野，训练输入就很大（常常是整个图片）。</p><p>我们提出了一个更高效的方法，在训练时共享特征。在Fast R-CNN训练过程中，SGD mini batch是结构化hierarchically采样的，首先采样N个图片，并在每个图片采样R/N个RoI。来自同一图片的RoI在前向和反向过程中很大程度地共享了内存和计算。降低N会降低mini-batch的计算量。举例来说，当N=2，R=128时，整个训练计划约快过从128张图片各取一个RoI（即R-CNN和SPPnet的方法）的64倍。</p><p>本方法的顾虑之一是由于来自同一图片的RoI有相互作用，训练收敛可能会很慢。这一顾虑并没有成为实际问题。当我们用N=2，R=128时，用比R-CNN更少的SGD迭代次数就能获得更好的结果。</p><p>除了结构化采样，Fast R-CNN使用了一个单步端到端的训练过程，将优化分类器和训练限位框回归整合到了一起，而不是分别进行。流程的构成（loss，mini-batch采样策略，通过RoI pooling层的反向传播和SGD超参数）将在下文描述。</p><p><strong>Multi-task loss</strong> 一个Fast R-CNN有两个兄弟输出层。第一个层为每个RoI输出一个对K+1个类的离散概率分布，$p=(p_0,…,P_K)$。通常，用全连接层输出的K+1进行softmax计算出p。第二个兄弟层输出限位框回归偏移，$t^k=(t^k_x,t^k_y,t^k_w,t^k_h)$，k指K个类中的第k个。我们用[9]中的参数化处理$t^k$，$t^k$是对一个proposal的大小不变平移和log空间的宽高变换。</p><p>每一个训练的RoI用一个gt类$u$和一个gt限位框回归目标$v$标注。我们用一个多任务loss L，对每一个标注RoI联合地训练分类和限位框回归：</p><script type="math/tex;mode=display">L(p,u,t^u,v) = L_{cls}(p,u) + \lambda  [ u \ge 1 ]L_{loc}(t^u,v) \qquad (1)</script><p>其中$ L_{cls}(p,u) = -\log p_u$是对于真类u的log loss。</p><p>第二个loss $L_{loc}$是由对类u对应的限位框回归目标v组成的元组$v=(v_x,v_y,v_w,v_h)$，以及预测值$t^u=(t^u_x,t^u_y,t^u_w,t^u_h)$组成。艾弗森括号Iverson bracket指当$u\ge 1$时为1，否则为0。为了方便，背景类标记为$u = 0$。因为背景类RoI没有gt的概念，所以将$L_{loc}$忽略掉。我们使用的loss函数是：</p><script type="math/tex;mode=display">L_{loc}(t^u,v)=\sum _{i \in \{ x,y,w,h\}} smooth_{L_1}(t^u_i-v_i) \qquad (2)</script><p>其中</p><script type="math/tex;mode=display">smooth_{L_1}(x) = \begin{cases} 
0.5x^2,  & \mbox{if } |x| \mbox{ < 1} \\
|x|-0.5, & \mbox{otherwise}\mbox{ }
\end{cases} \qquad (3)</script><p>是鲁棒的$L_1$ loss，它相比R-CNN和SPPnet使用的$L_2$loss，对异常值更不敏感。如果回归目标极大unbounded，用$L_2$loss训练需要小心地调优学习率来防止梯度爆炸。等式3就消除了这一过敏问题。</p><p>等式1中的超参数$\lambda$控制了两个loss任务之间的平衡。我们将gt回归目标$v_i$归一化到均值为1，具有单位方差unit variance。在我们所有实验中$\lambda = 1$。</p><p>我们注意到[6]用了一个关联related loss来训练一个类别无知的物体proposal网络。与我们的方法不同，它们主张一个双网系统，分别进行定位和分类。OverFeat，R-CNN和SPPnet也都训练分类和定位器，不过它们的方法是分阶段训练，我们会在5.1部分证明其对于Fast R-CNN不是最优解。</p><p><strong>Mini-batch sampling</strong> 在调优过程中，每个SGD mini-batch随机地取样N = 2张图片（在实际应用中，我们通常遍历数据集中的排列）。我们使用mini-batches R = 128，从每张图片中采样64个RoI。我们从所有与gt限位框IoU不少于0.5的RoI中取了25%。这些RoI组成了标注为一个前景物体类labeled with a foreground object class，如$u \ge 1$。剩余的RoU是从和gt框的最大IoU为[0.1,0.5)的proposal中采样。这些是背景样本并标注为$u = 0$。剩余低于0.1阈值的被用作hard example mining 的启发。在训练过程中，图片有0.5的概率水平翻转，没有使用其他的数据增广方法。</p><p><strong>Back-propagation through RoI pooling layers</strong> 反向传播导数derivative通过RoI pooling层。为了简洁，我们设定每个mini-batch为一张图片（N = 1），不过扩展到N &gt; 1也很直接，因为前向传播是独立处理每张图片的。</p><p>设$x_i \in \mathbb {R}$是输入到RoI pooling layer的第i个激活输入，$y_{rj}$为第r个RoI的第j个输出。RoI pooling层计算$y_{rj}=x_{i<em>(r,j)}$，其中$i^</em>(r,j) = \arg \max _{i’ \in \mathcal R(r,j)} x_{i’}$。$\mathcal R(r,j)$是对于输出单元$y_{rj}$的sub-window在输入中的坐标。一个$x_i$可能被赋值给多个不同的输出$y_{rj}$。</p><p>在RoI pooling层反向传输函数中对每个输入$x_i$通过argmax开关计算偏导：</p><script type="math/tex;mode=display">\frac {\partial L} {\partial x_i} = \sum_r \sum_j [i = i^*(r,j)] \frac {\partial L} {\partial y_{rj}} \qquad (4)</script><p>用语言描述，对于每个mini-batch RoI $r$和每个pooling输出单元$y_{rj}$ ，偏导$\partial L / \partial y_{rj}$只在i是通过max pooling选出的让$y_{rj}$最大时累积。在反向传播中偏导$\partial L / \partial y_{rj}$已被RoI pooling 层上的层的backwards函数计算。</p><p><strong>SGD hyper-parameters</strong> 用于softmax分类和限位框回归的全连接层由有着0均值、标准差0.01和0.001的高斯分布分别初始化。Baises初始化为0。每层的逐层学习率weight为1，biases为2，全局学习率为0.001。在训练VOC07和VOC12训练验证集时，我们进行了3万次mini-batch SGD迭代，在调整学习率为0.0001后再训练了1万次。我们在更大的数据集上训练时，我们会运行更多次迭代，这将在后续说明。momentum设为0.9，decay设为0.0005.</p><h3 id="2-4-Scale-invariance"><a href="#2-4-Scale-invariance" class="headerlink" title="2.4 Scale invariance"></a>2.4 Scale invariance</h3><p>我们探索了两条实现尺度不变性的物体检测的策略：通过蛮力学习和使用图片金字塔。两种策略都按照[11]中的方法进行。在蛮力法中，每张图片在训练和测试时都处理为预定义的像素大小。网络必须直接从数据中训练出尺度不变性物体检测。</p><p>多尺度方法与其相反，通过为网络提供图片金字塔，近似地得到尺度不变性。在测试时，图片金字塔用于将每个proposal近似地尺度归一化 scale-normalize。在训练阶段，我们按照[11]的方法，为每一个抽样到的图片随机采样一个金字塔尺度。由于GPU内存限制，我们只在小网络上实验了多尺度训练。</p><h2 id="3-Fast-R-CNN-detection"><a href="#3-Fast-R-CNN-detection" class="headerlink" title="3. Fast R-CNN detection"></a>3. Fast R-CNN detection</h2><p>一旦Fast R-CNN网络调优完成，检测会比前向传播计算量稍大。网络接收一个图片（或一个图片金字塔，编码为一系列图片）和一系列需要评分的R object proposal。在测试时, R通常接近2000，未来我们会考虑更大的情况（约4.5万）。使用图片金字塔时，每个RoI使用让它变形后最接近$224^2$像素的scale。</p><p>对于每个被测RoI $r$，前向传播输出一个后验概率分布$p$和为$r$预测的限位框偏移集合（K类中每类都得到自己的修正后限位框预测）。我们为每个物体类$k$使用估计概率$Pr(class = k | r) \triangleq p_k$来为$r$赋值检测置信度。我们再用一个和R-CNN算法、设置一样的非极大值抑制对每个类分别计算</p><h3 id="3-1-Truncated-SVD-for-faster-detection"><a href="#3-1-Truncated-SVD-for-faster-detection" class="headerlink" title="3.1 Truncated SVD for faster detection"></a>3.1 Truncated SVD for faster detection</h3><p>对于整图分类任务来说，全连接层比卷积层计算所需时间段。而在需要检测的RoI数量很大时，情况相反，近半数前向传播时间都花在全连接层计算（见图2）。大的全连接层很容易通过使用截短奇异值分解（truncated SVD）压缩来加速。</p><p>在这个技术中，一层的$u*v$权重矩阵W可近似地使用如下SVD因式分解：</p><script type="math/tex;mode=display">W \approx U\Sigma_tV^T \qquad (5)</script><p>在这个分解中，U是$u*t$的矩阵，包含了W的前t个singular，$\Sigma_t$是一个长宽为t的对角方阵，包含W的前t个right-singular向量。Truncated SVD将参数量从uv降到t(u+v)，当t比$\min (u,v)$都小得多时，这将很有效。为了压缩网络，W对应的全连接层用两个全连接层代替，其中不包含非线性关系（non-linearity）。第一个矩阵的参数使用$\Sigma_tV^T$且不包含biases，第二个使用U作为权重，以及原W对应的biases。这一简单的压缩方法在RoI数量大时非常有效。</p><h2 id="4-Main-results"><a href="#4-Main-results" class="headerlink" title="4. Main results"></a>4. Main results</h2><p>本论文有三个主要贡献：</p><ol><li>VOC 07,10,12上的顶级mAP</li><li>比起R-CNN和SPPnet，更快的训练和测试</li><li>对VGG16中的卷积层调优来提升mAP</li></ol><h3 id="4-1-Experimental-setup"><a href="#4-1-Experimental-setup" class="headerlink" title="4.1 Experimental setup"></a>4.1 Experimental setup</h3><p>我们实验中使用的三个使用ImageNet预训练的模型可以在网络上获得。第一个是来自R-CNN的CaffeNet（本质是AlexNet）。因为它小，我们将其称为模型<strong>S</strong>。第二个是来自[3]的VGG_CNN_M_1024，与<strong>S</strong>有着同样的深度，但更宽，我们称它为<strong>M</strong>，中等的简写。最后一个是来自[20]的极深模型VGG16，称为<strong>L</strong>。本节中，所有实验都是用单scale进行训练和测试（s=600，章节5.2里有细节）。</p><h3 id="4-2-VOC-2010-and-2012-results"><a href="#4-2-VOC-2010-and-2012-results" class="headerlink" title="4.2 VOC 2010 and 2012 results"></a>4.2 VOC 2010 and 2012 results</h3><p>我们把FRCN与公开榜上的最好方法（来自表2报表3）在这些数据集上比较。当前NUS_NIN_c2000和BabyLearning方法没有相关发表，所以我们无法找到其使用的卷积网络架构的具体信息，只知道使用的Network-in-Network NiN$$^{[17]}的变体。所有其它的方法都是初始化自同样的预训练VGG16网络。</p><p><img src="/archives/./1532671669590.png" alt="表2"></p><p><img src="/archives/./1532671687443.png" alt="表3"></p><p>FRCN以65.7%（通过额外数据达到68.4%）获得了VOC12的最好成绩。它同样比其它方法快上两个量级，它们都基于慢的R-CNN流程。在VOC10上，SegDeepM的mAP比FRCN更高（67.2% vs 66.1%）。SegDeepM使用附加了分段标注的VOC12训练验证集，它设计为通过使用马尔科夫随机场（Markov random field）提升R-CNN准确率，比R-CNN检测和O2P的语义分割方法更合理。FRCN可以替换掉SegDeepM中的R-CNN，也许会有更好的结果。当使用扩充的07++12训练集（见表2表头）是，FRCN的mAP提升到68.8%，超过了SegDeepM。</p><h3 id="4-3-VOC-2007-result"><a href="#4-3-VOC-2007-result" class="headerlink" title="4.3 VOC 2007 result"></a>4.3 VOC 2007 result</h3><p>我们把FRCN与R-CNN和SPPnet在VOC07上进行了对比。所有的方法都始于一个预训练的VGG16且都使用限位框回归。VGG16 SPPnet的结果来自[11]的作者。在训练和测试中，SPPnet都使用了5个scale。FRCN相比SPPnet的提升说明尽管FRCN只使用单scale进行训练测试，对于卷积层的调优还是让它获得了mAP的大幅提升（63.1%到66.9%）。R-CNN的mAP达到了66.0%。作为最小值，SPPnet没用使用PASCAL中标注为“难”的样本训练。移除那些样本后，FRCN的mAP达到68.1%。所有其它实验都包含了difficult样本。</p><p><img src="/archives/./1532671651694.png" alt="表1"></p><h3 id="4-4-Training-and-testing-time"><a href="#4-4-Training-and-testing-time" class="headerlink" title="4.4 Training and testing time"></a>4.4 Training and testing time</h3><p>更快的训练和测试时我们的第二大成果。表4在VOC07上比较了FRCN，R-CNN和SPPnet的训练时间（小时），测试率（秒/每张图）和mAP。使用VGG16时，FRCN处理一张图片比R-CNN快146倍（不带truncated SVD），快213倍（带SVD）。训练时间缩短了9倍，从84个小时降低到9.5个小时，与SPPnet相比，在VGG16上训练快2.7倍（9.5 vs 25.5 h），测试快7倍（无SVD）到10倍（有SVD）。因为它不需要缓存特征，FRCN也节省了数百G的磁盘空间。</p><p><strong>Truncated SVD</strong> 使用截短奇异值分解可以在仅损失很少（0.3%）的mAP且不需在模型压缩后额外调优的前提下，缩短30%的检测时间。图2显示了如何使用VGG16的fc6层25088<em>4096矩阵中的前1024个奇异值，和fc7的4096</em>4096的前256个奇异值，在只降低一点mAP的前提下减少运行时间。如果在压缩后再调优，也许mAP下降会更少。</p><p><img src="/archives/./1532681584378.png" alt="图2"></p><h3 id="4-5-Which-layers-to-fine-tune"><a href="#4-5-Which-layers-to-fine-tune" class="headerlink" title="4.5 Which layers to fine-tune?"></a>4.5 Which layers to fine-tune?</h3><p>对于SPPnet里的更浅的深度网络，看来只调优全连接层已经足够获得好的准确率了。但我们猜测对于更深的网络，这个结论不能成立。为了验证在VGG16上，调优卷积层很重要这一观点，我们使用FRCN来调优，但冻结13个卷积层从而只训练全连接层。这一消融模拟了单scale SPPnet，发现mAP从66.9%降至61.4%（表5）。这一实验验证了我们的猜想。</p><p><img src="/archives/./1532683809866.png" alt="表5"></p><p>那么是否说明所有卷积层都应该被调优呢？简单的说，不是。在较小的网络（<strong>S</strong>和<strong>M</strong>），我们发现conv1是泛化的且任务无关的（众所周知的事实$^{[14]}$）。允许conv1学习与否对于mAP没有任何有意义的影响。对于VGG16，我们发现仅有必要更新conv3_1及以上的层（13个卷积层中的9个）。这一发现很实用：</p><ol><li>从conv2_1开始学习比从conv3_1慢1.3倍（12.5 vs 9.5 h）。</li><li>从conv1_1开始学习超过了GPU显存限制。</li></ol><p>从conv2_1开始学习仅提高了0.3%的mAP（表5）。本论文中所有基于VGG16的FRCN的结果都是从conv3_1开始调优的。所有用<strong>S</strong>和<strong>M</strong>的实验都是从conv2开始调优的。</p><h2 id="5-Design-evaluation"><a href="#5-Design-evaluation" class="headerlink" title="5. Design evaluation"></a>5. Design evaluation</h2><p>我们进行了一系列实验来理解FRCN和R-CNN以及SPPnet的比较，同时评估我们的设计决策。我们在PASCAL VOC07上进行了如下实验。</p><h3 id="5-1-Does-multi-task-training-help"><a href="#5-1-Does-multi-task-training-help" class="headerlink" title="5.1 Does multi-task training help?"></a>5.1 Does multi-task training help?</h3><p>多任务训练因为不用管理顺序训练的任务而比较方便。但它同样有提升结果的空间，因为任务间通过共享的表达（如CNN）互相影响$^{[2]}$。那么FRCN中的多任务训练是否提升了物体检测准确度呢？</p><p>为了测试此问题，我们训练了一个只是用分类loss——等式1中的$L_{cls}$（如设置$\lambda = 0$）的基线网络。见表6中每组第一列。注意所有模型都未进行限位框回归。接下来（每组第二列），我们使用通过多任务loss训练的网络（如$\lambda = 1$），但我们在测试时禁止限位框回归。这将网络的分类准确率孤立出来，允许其和基线比较。</p><p>对于所有的三个网络我们都发现比起只训练分类的模型，多任务训练的纯分类准确率要高。mAP提升从0.8到1.1不等，显示出来自多任务训练的持续正效果。</p><p>最终，我们使用基线模型（只训练了分类loss），组装上限位框回归层，用$L_{loc}$进行训练并冻结其它参数。每组的第三列显示了这种多阶段的训练结果：mAP胜过第一列，但不如多任务训练（第四列）。</p><p><img src="/archives/./1532685413759.png" alt="表6"></p><h3 id="5-2-Scale-invariance-to-brute-force-or-finesse"><a href="#5-2-Scale-invariance-to-brute-force-or-finesse" class="headerlink" title="5.2 Scale invariance: to brute force or finesse"></a>5.2 Scale invariance: to brute force or finesse</h3><p>我们比较了两种获得尺度不变性的方法：暴力训练（单scale）和图片金字塔（多scale）。我们定义图片的scale s为它的短边长度。</p><p>所有单scale实验都使用s=600，有的图片s可能小于600，因为我们在保持长宽比的前提下将长边变形到1000像素。设定为这些值使调优时VGG16能装入GPU显存。更小的模型因为没有内存限制所以可以享受更大的s带来的好处；不过，我们的主要目的不是为了每个模型找到最优的s。我们注意到PASCAL图片平均大小为384*473，因此我们的单scale设定通常对样本放大了1.6倍。在RoI pooling层的平均有效步长约为10像素。</p><p>在多scale设置里，我们为了让与SPPnet的比较更容易，我们是使用了同样的五个scale（$s \in {480,576,688,864,1200}$）。不过我们把长边限制在了2000像素，来避免超出GPU存储限制。</p><p><img src="/archives/./1532687627713.png" alt="表7"></p><p>表7显示了模型S和M在scale为1和5时的训练测试效果。也许在[11]中最惊人的发现就是单scale检测几乎与多scale检测一样好。我们的发现证实了它们的结论：深度卷积网络擅长直接学习尺度不变性。多scale方式仅提升了一点mAP，却耗费了很多时间。在VGG16（模型L），因为实习细节的原因我们只能使用单scale。但它仍达到了66.9%的mAP，比R-CNN的66.0%稍高，尽管R-CNN用了无限的scale。</p><p>因为单scale处理提供了速度和准确率之间的最佳平衡，特别是深度网络模型上。除此节之外的所有实验都使用了s=600像素的单scale进行训练和测试。</p><h3 id="5-3-Do-we-need-more-training-data"><a href="#5-3-Do-we-need-more-training-data" class="headerlink" title="5.3 Do we need more training data?"></a>5.3 Do we need more training data?</h3><p>一个好的物体检测系统应该在获得更多训练数据时提升其性能。Zhu及其他人$^{[24]}$发现DPM$^{[8]}$的mAP在数千个样本训练后就饱和了。我们将VOC07训练验证集用VOC12训练验证集进行增广，将图片数量大致提升了3倍，达到165000，用来评估FRCN。扩充训练样本让它在VOC07测试集上的mAP从66.9%提升到70.0%（表1）。在这个数据集上训练时，我们使用60000的mini-batch迭代次数，而不是40000。</p><p>我们也在VOC10和VOC12上进行了相似的实验。构建了一个215000张的图片集。我们使用了100000的迭代次数，并在每40000次迭代（而不是每30000）后将学习率降低十倍。对于VOC 10和12，mAP分别从66.1%到68.8%，和65.7%到68.4%。</p><h3 id="5-4-Do-SVMs-outperform-softmax"><a href="#5-4-Do-SVMs-outperform-softmax" class="headerlink" title="5.4 Do SVMs outperform softmax?"></a>5.4 Do SVMs outperform softmax?</h3><p>FRCN使用在调优阶段训练出的softmax分类器代替了像R-CNN和SPPnet中那样，在事后训练的one-vs-rest 线性SVMs分类器。为了理解这一选择的影响，我们用FRCN实现了带hard negative mining的事后训练SVM。我们用了和R-CNN一样的训练算法和超参数。<br><img src="/archives/./1532745654999.png" alt="表8"></p><p>表8显示了softmax在三个网络上都略微胜过了SVM，约提升了0.1~0.8%的mAP。效果不大，但是说明了单次调优足够与之前的多阶段训练方法比较。我们注意到softmax不像one-vs-rest SVM，在为RoI评分时引入了类间的竞争。</p><h3 id="5-5-Are-more-proposals-always-better"><a href="#5-5-Are-more-proposals-always-better" class="headerlink" title="5.5 Are more proposals always better?"></a>5.5 Are more proposals always better?</h3><p>宽泛地说，一共有两种类型的物体检测器：使用稀疏集合的proposal（如Selective search）和使用密集集合的（如DPM）。对稀疏proposal分类是一种串联了拒绝大量候选proposal机制的方法。这一串联在用于DPM检测时提升了检测准确率$^{[21]}。我们找到了这一串联同样能提升FRCN的证据。</p><p>使用Selective search的quality模式。我们在每次重训练和重测试模型<strong>M</strong>时，从一张图片扫出1000到10000个proposal。如果proposal仅仅作为纯粹的计算角色，提升每张图片的proposal数量不应该损害mAP。</p><p><img src="/archives/./1532745896409.png" alt="图3"></p><p>我们发现随着proposal数量提升，mAP轻微地提升并回落（图3蓝色实线）。这一实验说明用更多的proposal淹没深度分类器毫无作用，甚至会轻微损害准确度。</p><p>不实际运行这一实验室很难预测结果的。最新的度量物体proposal质量的方式是平均召回率（Average Recall AR）。在每张图片的proposal数量固定时，AR和mAP在许多使用R-CNN的proposal方法上有不错的相关性。图3显示由于每张图片的proposal数量不一致，AR（红实线）与mAP没有相关性。必须小心地使用AR；由于更多的proposal导致的AR提升不意味着mAP也会提升。幸运的是，用模型M训练和测试少于2.5小时。FRCN允许使用有效的、直接的mAP评估。</p><p>我们也调查了使用密集生成的框的FRCN（覆盖大小、位置和长宽比），每张图片约45000个框。这一proposal密集到，将所有Selective search框用与其最近（IoU）的密集框替换，mAP也仅仅降低1%（到57.7%，图3中蓝色三角）。</p><p>密集框的统计不同于Selective search。从2000个Selective search框开始，我们测试了添加随机采样的$1000 \times \{2,4,6,8,10,32,45\}$个dense框时的mAP。对于每个实验，我们都重训练、测试模型M。随着框越多，mAP跌落得越快，最终达到53.0%。</p><p>我们也只用dense框（45000个每张图）训练和测试FRCN。得到52.9%的mAP（蓝色钻石）。最后我们检查了这一dense版本是否需要带hard negative mining的SVM。但SVM更差：49.3%（蓝色圆）。</p><h3 id="5-6-Preliminary-MS-COCO-results"><a href="#5-6-Preliminary-MS-COCO-results" class="headerlink" title="5.6 Preliminary MS COCO results"></a>5.6 Preliminary MS COCO results</h3><p>我们也将使用VGG16的FRCN应用在了MS COCO数据集，得到了一个初步的基线。我们在80000张训练集上训练了240000次迭代，并用测试服务器上的”test-dev”数据集进行了评估。PASCAL风格的mAP是35.9%，COCO风格的AP（会对不同的IoU阈值进行平均）是19.7%。</p><h2 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6. Conclusion"></a>6. Conclusion</h2><p>本论文提出了FRCN，一个R-CNN和SPPnet的简单快速的升级版。除了报告最新的检测结果，我们也呈现了详细的实验，希望能带来新的洞察力。看起来稀疏的proposal能提升检测器质量。在过去，这一实验太过费时，但FRCN让其变得可行。当然，也许仍有未发现的能让密集框同样优秀的技术。如果研发出这一技术，也许会帮助未来物体检测的加速。</p><p><strong>References</strong><br>[1] J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. Semantic<br>segmentation with second-order pooling. In ECCV,2012. 5<br>[2] R. Caruana. Multitask learning. Machine learning, 28(1),1997. 6<br>[3] K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman.<br>Return of the devil in the details: Delving deep into convolutional<br>nets. In BMVC, 2014. 5<br>[4] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei.<br>ImageNet: A large-scale hierarchical image database.<br>In CVPR, 2009. 2<br>[5] E. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus.<br>Exploiting linear structure within convolutional networks for<br>efficient evaluation. In NIPS, 2014. 4<br>[6] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable<br>object detection using deep neural networks. In CVPR, 2014.3<br>[7] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and<br>A. Zisserman. The PASCAL Visual Object Classes (VOC)<br>Challenge. IJCV, 2010. 1<br>[8] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan.<br>Object detection with discriminatively trained part<br>based models. TPAMI, 2010. 3, 7, 8<br>[9] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature<br>hierarchies for accurate object detection and semantic<br>segmentation. In CVPR, 2014. 1, 3, 4, 8<br>[10] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Regionbased<br>convolutional networks for accurate object detection<br>and segmentation. TPAMI, 2015. 5, 7, 8<br>[11] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling<br>in deep convolutional networks for visual recognition. In<br>ECCV, 2014. 1, 2, 3, 4, 5, 6, 7<br>[12] J. H. Hosang, R. Benenson, P. Dollar, and B. Schiele. What ´<br>makes for effective detection proposals? arXiv preprint<br>arXiv:1502.05082, 2015. 8<br>[13] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,<br>S. Guadarrama, and T. Darrell. Caffe: Convolutional<br>architecture for fast feature embedding. In Proc. of the ACM<br>International Conf. on Multimedia, 2014. 2<br>[14] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet classification<br>with deep convolutional neural networks. In NIPS,2012. 1, 4, 6<br>[15] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of<br>features: Spatial pyramid matching for recognizing natural<br>scene categories. In CVPR, 2006. 1<br>[16] Y. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard,<br>W. Hubbard, and L. Jackel. Backpropagation applied to<br>handwritten zip code recognition. Neural Comp., 1989. 1<br>[17] M. Lin, Q. Chen, and S. Yan. Network in network. In ICLR,2014. 5<br>[18] T. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick,<br>J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zit- ´<br>nick. Microsoft COCO: common objects in context. arXiv<br>e-prints, arXiv:1405.0312 [cs.CV], 2014. 8<br>[19] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,<br>and Y. LeCun. OverFeat: Integrated Recognition, Localization<br>and Detection using Convolutional Networks. In ICLR,2014. 1, 3<br>[20] K. Simonyan and A. Zisserman. Very deep convolutional<br>networks for large-scale image recognition. In ICLR, 2015.1, 5<br>[21] J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders.<br>Selective search for object recognition. IJCV, 2013. 8<br>[22] P. Viola and M. Jones. Rapid object detection using a boosted<br>cascade of simple features. In CVPR, 2001. 8<br>[23] J. Xue, J. Li, and Y. Gong. Restructuring of deep neural<br>network acoustic models with singular value decomposition.<br>In Interspeech, 2013. 4<br>[24] X. Zhu, C. Vondrick, D. Ramanan, and C. Fowlkes. Do we<br>need more training data or better models for object detection?<br>In BMVC, 2012. 7<br>[25] Y. Zhu, R. Urtasun, R. Salakhutdinov, and S. Fidler.<br>segDeepM: Exploiting segmentation and context in deep<br>neural networks for object detection. In CVPR, 2015. 1, 5</p></div><div class="popular-posts-header">Related Posts</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/719cc2e5.html" rel="bookmark">SSD</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/423509a9.html" rel="bookmark">YOLOv2 and YOLO9000</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/5beb21d0.html" rel="bookmark">R-CNN</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/8504aa6c.html" rel="bookmark">RetinaNet</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/fc798de3.html" rel="bookmark">Faster R-CNN</a></div></li></ul><div><ul class="post-copyright"><li class="post-copyright-author"><strong>Post author: </strong>慕湮</li><li class="post-copyright-link"><strong>Post link: </strong><a href="http://muyaan.com/archives/9bab3bc6.html" title="Fast R-CNN">http://muyaan.com/archives/9bab3bc6.html</a></li><li class="post-copyright-license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noopener noreferrer" target="_blank">CC BY-NC-SA 4.0</a> unless stating additionally.</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Computer-Vision/" rel="tag"><i class="fa fa-tag"></i> Computer Vision</a> <a href="/tags/Deep-Learning/" rel="tag"><i class="fa fa-tag"></i> Deep Learning</a> <a href="/tags/Paper/" rel="tag"><i class="fa fa-tag"></i> Paper</a> <a href="/tags/Object-Detection/" rel="tag"><i class="fa fa-tag"></i> Object Detection</a> <a href="/tags/Fast-R-CNN/" rel="tag"><i class="fa fa-tag"></i> Fast R-CNN</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/archives/5beb21d0.html" rel="next" title="R-CNN"><i class="fa fa-chevron-left"></i> R-CNN</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/archives/fc798de3.html" rel="prev" title="Faster R-CNN">Faster R-CNN <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div><div class="comments" id="comments"><div id="lv-container" data-id="city" data-uid="MTAyMC8zODEwMy8xNDYzMw=="></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">Table of Contents</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">Overview</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4" alt="慕湮"><p class="site-author-name" itemprop="name">慕湮</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">21</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">30</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/tycallen" target="_blank" title="GitHub" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-github"></i>GitHub</a></span><br><span class="links-of-author-item"><a href="mailto:tyc.allen@gmail.com" target="_blank" title="E-Mail" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span><br><span class="links-of-author-item"><a href="http://weibo.com/pojunallen" target="_blank" title="微博" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-weibo"></i>微博</a></span><br><span class="links-of-author-item"><a href="https://tuchong.com/1070837" target="_blank" title="图虫" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-globe"></i>图虫</a></span><br></div><div class="links-of-blogroll motion-element links-of-blogroll-block"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> 友情链接</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="https://dotrabbit.github.io/" title="dotrabbit" target="_blank" rel="external nofollow noopener noreferrer">dotrabbit</a></li></ul></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Introduction"><span class="nav-text">1. Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-R-CNN-and-SPPnet"><span class="nav-text">1.1 R-CNN and SPPnet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-Contributions"><span class="nav-text">1.2 Contributions</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Fast-R-CNN-architecture-and-training"><span class="nav-text">2. Fast R-CNN architecture and training</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-The-RoI-pooling-layer"><span class="nav-text">2.1 The RoI pooling layer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Initializing-from-pre-trained-networks"><span class="nav-text">2.2 Initializing from pre-trained networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-Fine-tuning-for-detection"><span class="nav-text">2.3 Fine-tuning for detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-Scale-invariance"><span class="nav-text">2.4 Scale invariance</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Fast-R-CNN-detection"><span class="nav-text">3. Fast R-CNN detection</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Truncated-SVD-for-faster-detection"><span class="nav-text">3.1 Truncated SVD for faster detection</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Main-results"><span class="nav-text">4. Main results</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-Experimental-setup"><span class="nav-text">4.1 Experimental setup</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-VOC-2010-and-2012-results"><span class="nav-text">4.2 VOC 2010 and 2012 results</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-VOC-2007-result"><span class="nav-text">4.3 VOC 2007 result</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-Training-and-testing-time"><span class="nav-text">4.4 Training and testing time</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-5-Which-layers-to-fine-tune"><span class="nav-text">4.5 Which layers to fine-tune?</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Design-evaluation"><span class="nav-text">5. Design evaluation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-Does-multi-task-training-help"><span class="nav-text">5.1 Does multi-task training help?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-Scale-invariance-to-brute-force-or-finesse"><span class="nav-text">5.2 Scale invariance: to brute force or finesse</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-Do-we-need-more-training-data"><span class="nav-text">5.3 Do we need more training data?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-Do-SVMs-outperform-softmax"><span class="nav-text">5.4 Do SVMs outperform softmax?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-5-Are-more-proposals-always-better"><span class="nav-text">5.5 Are more proposals always better?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-6-Preliminary-MS-COCO-results"><span class="nav-text">5.6 Preliminary MS COCO results</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-Conclusion"><span class="nav-text">6. Conclusion</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; 2015 &mdash; <span itemprop="copyrightYear">2018</span> <span class="with-love" id="animate"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">慕湮</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="Symbols count total">313k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="Reading time total">9:30</span></div><div class="powered-by">Powered by <a class="theme-link" target="_blank" rel="external nofollow noopener noreferrer" href="https://hexo.io">Hexo</a></div><span class="post-meta-divider">|</span><div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" rel="external nofollow noopener noreferrer" href="https://theme-next.org">NexT.Muse</a></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="//cdn.staticfile.org/jquery/2.1.3/jquery.min.js"></script><script type="text/javascript" src="//cdn.staticfile.org/velocity/1.2.1/velocity.min.js"></script><script type="text/javascript" src="//cdn.staticfile.org/velocity/1.2.1/velocity.ui.min.js"></script><script type="text/javascript" src="/lib/reading_progress/reading_progress.js"></script><script type="text/javascript" src="/js/src/utils.js?v=6.3.0"></script><script type="text/javascript" src="/js/src/motion.js?v=6.3.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=6.3.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=6.3.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=6.3.0"></script><script type="text/javascript">!function(e,t){var n,c=e.getElementsByTagName(t)[0];"function"!=typeof LivereTower&&((n=e.createElement(t)).src="https://cdn-city.livere.com/js/embed.dist.js",n.async=!0,c.parentNode.insertBefore(n,c))}(document,"script")</script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script><script>AV.initialize("UWGXBMGbguCDhSCbC3NrUQtL-gzGzoHsz","ke1jrA5b6VyR89Kqqqwf2kPP")</script><script>function showTime(e){var t=new AV.Query(e),c=[],u=$(".leancloud_visitors");u.each(function(){c.push($(this).attr("id").trim())}),t.containedIn("url",c),t.find().done(function(e){var t=".leancloud-visitors-count";if(0!==e.length){for(var n=0;n<e.length;n++){var o=e[n],i=o.get("url"),s=o.get("time"),r=document.getElementById(i);$(r).find(t).text(s)}for(n=0;n<c.length;n++){i=c[n],r=document.getElementById(i);var l=$(r).find(t);""==l.text()&&l.text(0)}}else u.find(t).text(0)}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(i){var e=$(".leancloud_visitors"),s=e.attr("id").trim(),r=e.attr("data-flag-title").trim(),t=new AV.Query(i);t.equalTo("url",s),t.find({success:function(e){if(0<e.length){var t=e[0];t.fetchWhenSave(!0),t.increment("time"),$(document.getElementById(s)).find(".leancloud-visitors-count").text(t.get("time")),t.save(null,{success:function(e){},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var n=new i,o=new AV.ACL;o.setPublicReadAccess(!0),o.setPublicWriteAccess(!0),n.setACL(o),n.set("title",r),n.set("url",s),n.set("time",1),n.save(null,{success:function(e){$(document.getElementById(s)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):1<$(".post-title-link").length&&showTime(e)})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script></body></html>