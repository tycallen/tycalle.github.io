<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">






  
  
    
      
    
    
      
    
  <script src="https://cdn.staticfile.org/pace/1.0.2/pace.min.js"></script>
  <link href="https://cdn.staticfile.org/pace/1.0.2/themes/blue/pace-theme-big-counter.min.css" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">






















  

<link href="//cdn.staticfile.org/font-awesome/4.6.2/css/font-awesome.min.css" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.4.1" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.4.1">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.4.1">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.4.1">


  <link rel="mask-icon" href="/images/logo.svg?v=6.4.1" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.4.1',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="YOLO论文翻译。一个有着很酷名字的单步物体检测算法。  YOLO : You Only Look Once: Unified, Real-Time Object DetectionJoseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi  2015University of Washington, Allen Institute for">
<meta name="keywords" content="Computer Vision,Deep Learning,Paper,Object Detection,YOLO">
<meta property="og:type" content="article">
<meta property="og:title" content="YOLO">
<meta property="og:url" content="http://muyaan.com/archives/562809c1.html">
<meta property="og:site_name" content="慕湮">
<meta property="og:description" content="YOLO论文翻译。一个有着很酷名字的单步物体检测算法。  YOLO : You Only Look Once: Unified, Real-Time Object DetectionJoseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi  2015University of Washington, Allen Institute for">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://muyaan.com/archives/1533172629942.png">
<meta property="og:image" content="http://muyaan.com/archives/1533203960848.png">
<meta property="og:image" content="http://muyaan.com/archives/1533209110200.png">
<meta property="og:image" content="http://muyaan.com/archives/1533274570698.png">
<meta property="og:image" content="http://muyaan.com/archives/1533281060562.png">
<meta property="og:image" content="http://muyaan.com/archives/1533281269019.png">
<meta property="og:image" content="http://muyaan.com/archives/1533283556398.png">
<meta property="og:image" content="http://muyaan.com/archives/1533283582281.png">
<meta property="og:image" content="http://muyaan.com/archives/1533273390802.png">
<meta property="og:updated_time" content="2018-09-12T03:22:25.034Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="YOLO">
<meta name="twitter:description" content="YOLO论文翻译。一个有着很酷名字的单步物体检测算法。  YOLO : You Only Look Once: Unified, Real-Time Object DetectionJoseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi  2015University of Washington, Allen Institute for">
<meta name="twitter:image" content="http://muyaan.com/archives/1533172629942.png">






  <link rel="canonical" href="http://muyaan.com/archives/562809c1.html">



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>YOLO | 慕湮</title>
  






  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ca5844321cfb80fdf6f12b4dcc326991";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">慕湮</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <h1 class="site-subtitle" itemprop="description">白日放歌须纵酒</h1>
      
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives menu-item-active">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>
  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  
    <div class="reading-progress-bar"></div>
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://muyaan.com/archives/562809c1.html">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="慕湮">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="慕湮">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">YOLO
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-08-03 16:33:08" itemprop="dateCreated datePublished" datetime="2018-08-03T16:33:08+08:00">2018-08-03</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-09-12 11:22:25" itemprop="dateModified" datetime="2018-09-12T11:22:25+08:00">2018-09-12</time>
              
            
          </span>

          

          
            
          

          
          
             <span id="/archives/562809c1.html" class="leancloud_visitors" data-flag-title="YOLO">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数：</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">本文字数：</span>
                
                <span title="本文字数">16k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">30 分钟</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>YOLO论文翻译。一个有着很酷名字的单步物体检测算法。</p>
<blockquote>
<p><a href="https://arxiv.org/abs/1506.02640" rel="external nofollow noopener noreferrer" target="_blank">YOLO : You Only Look Once: Unified, Real-Time Object Detection</a><br>Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi  2015<br>University of Washington, Allen Institute for AI, Facebook AI Research<br><a href="http://pjreddie.com/yolo/" rel="external nofollow noopener noreferrer" target="_blank">http://pjreddie.com/yolo/</a></p>
</blockquote>
<a id="more"></a>
<hr>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>人类只需瞥一眼图片就立刻知道图片里有哪些物体，它们在哪儿，它们怎么互相作用的。人类视觉系统快速又准确，允许我们进行复杂的任务如只用一点注意力开车。快速、准确的物体检测算法允许计算机无需特殊传感器即可开车，允许辅助驾驶将实时信息传递给人类用户，解锁通用响应式机器人的潜力。</p>
<p>目前的检测系统将分类器用作检测。为了检测物体，系统使用一个分类器来评估它在图片中的所有可能位置和尺度。像DPM这样的系统使用一个滑动窗口，它会在整个图片的平均空间位置中运行。</p>
<p>近来的方法如R-CNN使用候选区域方法来先产生图片中可能的限位框，再用分类器运行它们。分类后，用回归去提升限位框精度，消除重复检测，并基于场景中其它物体为它重打分。这样复杂的流程又慢又难以优化，因为每个独立的组件都需要分别训练。</p>
<p>我们将物体检测问题重构为一个单独的回归问题，直接从像素到限位框坐标和分类概率。使用我们的系统，You Only Look Once(YOLO) ，你只需看一次图片就能预测出有哪些物体、它们在哪儿。</p>
<p><img src="/archives/562809c1/1533172629942.png" alt="图1"></p>
<p>YOLO相当简单：见图1。一个卷积网络同时预测多个限位框和这些框的类概率。YOLO在完整的图片上训练，并直接优化检测性能。这一统一的模型相对传统物体检测方法有多种好处。</p>
<p>首先，YOLO非常快。因为我们将问题重构为回归问题，因此流程很简单。测试时我们只需要用我们的神经网络运行图片即可预测。我们的基础网络fps为45，快速版为150（无多进程，单Titan X）。这意味着我们能以低于25ms的延迟实时处理视频流。不仅如此，YOLO相比其他实时系统mAP高两倍。一个我们系统在一个webcam上实时运行的<a href="http://pjreddie.com/yolo/" rel="external nofollow noopener noreferrer" target="_blank">Demo</a></p>
<p>其次，YOLO在预测时是基于整个图片推理的。不像基于滑动窗口和候选区域的技术，YOLO训练测试时有整个图片的视野，因此它隐式地将类的上下文信息和外观编码。FRCN，一个顶级检测方法，常将图片中的背景块误认为物体，因为它无法看到更大的上下文。相比FRCN，YOLO的背景错误要少一半。</p>
<p>最后，YOLO能学到物体的泛化表达。当在自然图片上训练而在艺术品上测试时，YOLO领先其它检测方法如DPM和R-CNN一大截。因为YOLO的高度泛化性，当应用在新领域和不同的输入时，更不容易失败。</p>
<p>YOLO与前沿检测系统相比，准确率仍有落后。它能快速辨识出图片中的物体，但对一些物体，特别是小的物体，难以精确定位其位置。我们在实验了研究了这一权衡。</p>
<p>我们所有的训练、测试代码均已开源。一系列预训练的模型也供下载。</p>
<h2 id="2-Unified-Detection"><a href="#2-Unified-Detection" class="headerlink" title="2. Unified Detection"></a>2. Unified Detection</h2><p>我们把物体检测的多个组件统一为一个神经网络。我们的网络使用整个图片的特征来预测每个限位框。它同时为该图片的所有限位框针对所有类型进行预测。这意味着我们的方法对图片中的所有信息和所有物体知情。这样的设计让YOLO能保持高mAP的情况下，实时地端到端运行。</p>
<p>我们的系统将输入的图片分割为 S * S个格子。如果一个物体的中心落在一个格子中，该格就负责这个物体的检测。</p>
<p>每个格子预测一系列限位框<strong><em>B</em></strong>和限位框信心分。信心分反应出了模型对于这个框包含物体的自信程度和它觉得这个框的预测精确程度。通常我们把信心定义为$Pr(object) * {IoU}_{pred}^{truth}$。如果该格子不含物体，信心应为0。否则我们希望信心分与IoU相等。</p>
<p>每个限位框由5个预测值组成：x,y,w,h和confidence。(x,y)代表限位框中心相对格子边界的坐标。宽高预测为相对整个图片的比例。信心是预测框和任意gt框的IoU。</p>
<p>每个格子同样预测<strong><em>C</em></strong> 条件类概率，$Pr(Class_i|Object)$。这些概率是基于格子包含一个物体的条件概率。我们只为一个格子预测一个类概率集，不管其中有多少个限位框。</p>
<p>在测试时，我们将条件类概率和每个框的信心分相乘，</p>
<script type="math/tex; mode=display">Pr(Class_i|Object) * Pr(Object) *IoU_{pred}^{truth} = Pr(Class_i) * IoU_{pred}^{truth} \qquad (1)</script><p>就得到了每个框的类相关的信心分。这些分中不仅有类出现在框中的概率，也有框与物体拟合得有多好的信息。<br><img src="/archives/562809c1/1533203960848.png" alt="图2"></p>
<p> 在VOC上测试YOLO时，我们使用S = 7, B = 2。VOC有着20个类因此C = 20。我们最终预测是一个7 * 7 * 30的张量tensor。</p>
<h3 id="2-1-Network-Design"><a href="#2-1-Network-Design" class="headerlink" title="2.1 Network Design"></a>2.1 Network Design</h3><p>我们将模型实现为卷积神经网络，并在VOC验证集上测试$^{[9]}$。卷积层从图片提取特征，全连接层输出概率和坐标。</p>
<p>我们的网络结构灵感来自用于图片分类的GoogLeNet。我们的网络有24个卷积层后跟着2个全连接层。我们用1 * 1的reduction简化层后跟3 * 3卷积层的结果代替了GoogLeNet的inception模块。图3是整个网络。</p>
<p><img src="/archives/562809c1/1533209110200.png" alt="图3"></p>
<p>我们也训练了一个Fast YOLO，用于推进物体检测的速度边界。Fast YOLO使用更少的卷积层（9）以及层中更少的filter的网络。除了大小不同，其余训练测试参数和正常版本没有区别。</p>
<h3 id="2-2-Training"><a href="#2-2-Training" class="headerlink" title="2.2 Training"></a>2.2 Training</h3><p>我们用ImageNet 1000类竞赛数据集预训练我们的卷积层$^{[30]}$。在预训练我们用图3中的前20个卷积层，后跟一个平均池化层和全连接层。我们训练这个网络花了接近一周，在INLSVC 12验证集上的single top-5准确度为88%，与Caffe中的GoogLeNet模型$^{[24]}$相近。我们训练和推论都使用了Darknet框架$^{[26]}$。</p>
<p>接下来我们将模型转变为进行检测。Ren等人发现将卷积和全连接层添加到预训练好的网络能提升性能$^{[29]}$。按照它们的例子，我们增加了4个卷积层和2个全连接层，权重随机初始化。通常检测都需要更优质的视觉信息，因此我们将网络输入分辨率从224*224增加到了448*448。</p>
<p>我们的最终层同时预测类概率和限位框坐标。我们用图片的宽高归一化限位框宽高，使之落入0，1区间。我们把限位框的x,y坐标参数化为相对所在格子特定位置的偏移，这样它们也限定在0，1间。</p>
<p>我们用如下的leaky rectified linear activation作为最终层和其它所有层的激活函数：</p>
<script type="math/tex; mode=display">\phi (x) = \begin{cases}
x, & \mbox{if }x\mbox{ > 0} \\
0.1x , &  {otherwise}
\end{cases} \qquad (2)</script><p>我们优化我们模型输出的平方和(? sum-squard)误差。我们用平方和误差的原因是它易于优化，不过它并不是完美符合我们最大化mAP的目标。它为定位误差和分类误差给与了相等的权重，这可能不是最佳选择。同样，在每个图片的许多格子中并不包含任何物体。这让信心分向0推进，常常盖过来自有物体的格子的梯度。这会导致模型的不稳定性，使训练在一开始就偏离。</p>
<p>为了纠正这一问题，我们提升了限位框坐标预测的loss，降低了不含物体的限位框的信心loss。我们用了两个参数$\lambda_{coord} = 5$和 $\lambda_{noobj} = .5$来实现。</p>
<p>平方和误差同样为大框和小框给与了同样的权重。我们的loss方法应该反映为大框中的小偏差重要性不如在小框。为了着重强调这一点，我们预测限位框宽高的平方根。</p>
<p>YOLO为每个格子预测多个限位框。在训练时，我们仅希望为每个物体有一个负责预测的限位框预测器predictor。我们把预测值与gt有着最高IoU的预测器标记为负责预测该物体。这让限位框预测器之间出现了专精（specialization）。每个预测器在预测特定大小、长宽比或物体类别时都变得更好，提升了整体召回率。</p>
<p>在训练过程中，我们优化如下的多部分loss函数：</p>
<script type="math/tex; mode=display">\begin{split}
 \lambda _{coord} &\sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb 1_{ij}^{obj} [(x_i&-\hat x_i)^2+(y_i-\hat y_i)^2] \\
&+\lambda_{coord}  \sum_{i=0}^{S^2} \sum_{j=0}^{B} & \mathbb 1_{ij}^{obj} [(\sqrt {w_i}-\sqrt{\hat w_i})^2+(\sqrt {h_i}-\sqrt{\hat h_i})^2] \\
& &+ \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb 1_{ij}^{obj} (C_i+\hat C_i)^2\\
&&+\lambda_{noobj}  \mathbb 1_{ij}^{noobj} (C_i+\hat C_i)^2\\
&&+\sum_{i=0}^{S^2} \mathbb 1_{i}^{obj} \sum _{c \in classes} (p_i(c) - \hat p_i(c))^2 \qquad (3)
\end{split}</script><p>其中$\mathbb 1_{i}^{obj} $记做如果物体在格子$i$中出现， $\mathbb 1_{ij}^{obj} $记做如果格子i中的第j个限位框预测器负责该预测。</p>
<p>要注意loss函数只惩罚了当物体在那个格子出现时（由于早先讨论的条件概率）的分类误差。它同样只惩罚了当预测器负责该gt框时限位框坐标误差。</p>
<p>我们在VOC 07 12的训练验证集上训练了135个epoch。在12上测试时，我们还引入了07测试集进行训练。在整个训练中，mini-batch大小为64，动量为0.9，decay为0.0005。</p>
<p>我们训练步骤如下：在第一个epoch，我们缓慢地将学习率从$10^{-3}$提升到$10^{-2}$。如果从高学习率开始，模型常常因为不稳定的梯度而偏离。接着用$10^{-2}$训练75个epoch，接着$10^{-3}$进行30个epoch，最终30个为$10^{-4}$。</p>
<p>为了避免过拟合，我们使用了dropout和数据增广。在第一个连接层后使用了rate=0.5的dropout层，防止它们互相适应$^{[18]}$。数据增广我们使用了随机缩放和平移，最高20%的原图像尺寸。我们还在HSV色彩空间中，用因子最高1.5，随机的调整图像的曝光和饱和度。</p>
<h3 id="2-3-Inference"><a href="#2-3-Inference" class="headerlink" title="2.3 Inference"></a>2.3 Inference</h3><p>和训练时一样，测试图片也只需在一个网络上进行检测。在VOC上该网络为每张图片预测98个限位框和每个限位框的类概率。不像基于分类器的方法，YOLO测试时极度地快，因为它只有一个网络。</p>
<p>格子设计强制了限位框预测的空间多样性。通常能很清楚地判断一个物体落入哪个格子，网络仅为每个物体预测一个框。不过，一些大物体或靠近多个格子边界的物体能被多个格子很好地定位。非极大值抑制能用于修正这些多个检测。尽管对于R-CNN和DPM，该方法对性能影响不大，但为YOLO提升了2~3%的mAP。</p>
<h3 id="2-4-Limitations-of-YOLO"><a href="#2-4-Limitations-of-YOLO" class="headerlink" title="2.4 Limitations of YOLO"></a>2.4 Limitations of YOLO</h3><p>YOLO为限位框预测加以强空间约束：每个格子只能预测两个框且只能有一个类。这一空间约束限制了我们的模型能预测的临近物体数量。我们的模型面对成群出现的小物体时很挣扎，如一群鸟。</p>
<p>由于我们的模型从数据中学习预测限位框，它很难泛化到有新的或不常见的长宽比或配置的物体。因为我们的网络架构从输入图片开始，有多个下采样层，我们的模型也使用的相对粗糙的特征来预测限位框。</p>
<p>最后，我们的loss函数同等对待大小限位框里的误差。大限位框的小误差通常是无害的，但小框里的小误差有着对IoU的较大影响。我们主要的错误源即是定位错误。</p>
<h2 id="3-Comparison-to-Other-Detection-Systems"><a href="#3-Comparison-to-Other-Detection-Systems" class="headerlink" title="3. Comparison to Other Detection Systems"></a>3. Comparison to Other Detection Systems</h2><p>物体检测是计算机视觉里的核心问题。检测流程通常由从输入图片中抽取一个健壮的特征集开始（Haar$^{[25]}$，SIFT$^{[23]}$，HOG$^{[4]}$，卷积特征$^{[6]}$）。接着，分类器$^{[36,21,14,10]}$或定位器$^{[1,32]}$被用于从特征空间中找出物体。这些分类器和定位器不是在整个图片上以滑动窗口运行，就是在图片的一些子区域上检测。我们把YOLO与一些顶级检测框架相比，突出关键的相似和不同。</p>
<p><strong>Deformable parts models.</strong> DPM使用滑动窗口来检测物体$^{[10]}$。DPM用分离的流程进行抽取静态特征，区域分类，为高评分区域预测限位框等。我们的系统把这些不相干的部分用一整个卷积网络代替。网络同时进行特征抽取、限位框预测、非极大值抑制和上下文推理。不是使用静态特征，网络内嵌地训练特征，并为检测任务优化它们。我们统一的结构得到了一个比DPM更快、更精确的模型。</p>
<p><strong>R-CNN</strong> R-CNN及其变体使用候选区域来代替滑动窗口进行图片中的物体寻找。SS$^{[35]}$生成候选限位框，一个卷积网络来提取特征，一个SVM来为限位框评分，一个线性模型来调整限位框，一个非极大值抑制来消除重复检测。这一复杂流程的每一步都需要独立精确调优，系统很慢，需要40s处理一张图片。</p>
<p>YOLO与R-CNN有一些相似处。每个格子提出候选限位框并用卷积特征为它们打分。不过，我们的系统在格子候选上加入了空间约束，缓和了对同一物体的多次检测。我们的系统提出的候选框数量也少得多，每张图片约98个，而SS是2000个。最后，我们是一个整体的同时优化的模型。</p>
<p><strong>Other Fast Detectors</strong> Fast 和 Faster R-CNN专注于通过共享计算来加速R-CNN框架，并用神经网络替代SS来提出区域$^{[14,28]}$。它们相对R-CNN都有速度和精度上的提升，但在实时性能上仍落后。</p>
<p>许多研究致力于加速DPM流程$^{[31,38,5]}$。它们加速HOG计算，使用级联cascades，使用GPU进行运算。但只有30Hz DPM能实时运行。</p>
<p>与其试图优化一个大的检测流程的独立组件，YOLO完全抛开了流程并通过设计来变快。</p>
<p>对单类如脸或人进行检测可以高度调优，因为它们只需面对少得多的变化$^{[37]}$。YOLO是一个通用的检测器，训练来同时检测许多物体。</p>
<p><strong>Deep MultiBox</strong> 不像R-CNN，Szegedy等人训练了一个卷积网络来预测RoI$^{[8]}$而不是使用SS。MultiBox也能通过将confidence prediction替换为单类预测进行单物体预测。但是MultiBox不能进行泛化的物体检测，而且只是一个更大检测流程中的一部分，需要进一步的图片块分类。YOLO和MultiBox都使用一个卷积网络来在图片上预测限位框，但YOLO是一个完整的检测系统。</p>
<p><strong>OverFeat</strong> Sermanet等人将卷积网络训练用于定位，并将其用于检测$^{[32]}$。OverFeat能高效地运用滑动窗口来检测，但其仍是个分离的系统。OverFeat向定位优化，并不是检测性能。和DPM一样，定位器决策时只能看见局部信息。OverFeat不能对全局上下文进行推理，因此需要有效的后处理来产生清晰的检测。</p>
<p><strong>MultiGrasp</strong> 我们的工作与Redmon他们的设计类似$^{[27]}$。我们用于预测限位框的格子法是基于MultiGrsp系统的对grasps的回归。MultiGrasp仅需为一个包含一个物体的图片预测一个graspable区域。它不需要估计其大小、位置、边界或类别，仅需找出一个grasping所需的区域。YOLO为图片中多类、多物体预测限位框和类概率。</p>
<h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h2><p>首先我们把YOLO和其它实时检测系统在VOC 07上进行了比较。为了理解YOLO和R-CNN各版本的不同，我们调查了YOLO和FRCN（一个R-CNN的高性能版本）$^{[14]}$各自的错误。基于不同错误的侧写profile，我们证明YOLO能用于对FRCN检测结果的重打分，能减少其中的背景错误正预测，达到一个显著的性能提升。我们也呈现了VOC 12的结果，并与前沿方法比较了mAP。最终，我们在两个艺术数据集上证明了YOLO能比其余检测器有更佳的新领域泛化表现。</p>
<h3 id="4-1-Comparison-to-Other-Real-Time-Systems"><a href="#4-1-Comparison-to-Other-Real-Time-Systems" class="headerlink" title="4.1 Comparison to Other Real-Time Systems"></a>4.1 Comparison to Other Real-Time Systems</h3><p>许多物体检测研究致力于加速标准检测流程$^{[5,38,31,14,17,28]}$。不过，Sadeghi等人确实创建了一个实时系统（30fps或更高）$^{[31]}$。我们把YOLO和他妈的GPU版的DPM（30Hz或100Hz）比较。尽管其余系统没达到实时标准，我们也比较了它们的相对mAP和速度，来检验物体检测系统中的精度、速度平衡。</p>
<p>Fast YOLO是VOC上最快的；目前据我们所知，它是现存最快的物体检测器。它有52.7%的mAP，比其余的实时检测系统高两倍。YOLO的mAP则达到了63.4%，仍有实时的速度性能。</p>
<p>我们也使用了VGG-16训练YOLO，这个模型更准确，但比YOLO慢很多。将其用于与其余基于VGG-16的系统进行比较很有用，但因为它的性能不是实时的，论文其余部分都着重与更快的模型。</p>
<p>Faster DPM相比DPM速度提高了很多，还没有牺牲太多mAP。但它仍低于实时两倍$^{[38]}$。相比神经网络的方法，它也受制于DPM相对较低的检测精度。</p>
<p>R-CNN minus R用静态限位框替代SS$^{[20]}$。尽管它比R-CNN快很多，但仍没达到实时级，并因没有好的proposal而精度下降。</p>
<p>FRCN加速了R-CNN的分类阶段，但它仍依赖于SS，该阶段需要在每张图片上花费2秒来生成候选限位框。尽管它mAP很高，但仍不是实时的。</p>
<p>近期的Faster R-CNN用神经网络替换SS来预测限位框，类似Szegedy他们。在我们的测试中，它们的最精确模型达到了7fps，精度更低版本的为18fps。VGG-16版本的Faster R-CNN比YOLO 的mAP高10，当然也要慢上6倍。Zeiler-Fergus的Faster R-CNN仅比YOLO慢2.5倍，但它的精度也更低。</p>
<h3 id="4-2-VOC-2007-Error-Analysis"><a href="#4-2-VOC-2007-Error-Analysis" class="headerlink" title="4.2 VOC 2007 Error Analysis"></a>4.2 VOC 2007 Error Analysis</h3><p>为了进一步检验YOLO和前沿检测器的差别，我们查看了VOC 07结果的详细分类。我们使用FRCN进行比较，因为它是最好的之一，而且检测结果公开。</p>
<p>我们使用了Hoiem等人的方法论和工具$^{[19]}$。对于每个测试时的类别，我们查看了该类的top-N预测。每个预测不是正确的就是基于以下几类错误：</p>
<ul>
<li>正确：正确的类，IoU &gt; .5</li>
<li>定位：正确的类，.1 &lt; IoU &lt; .5</li>
<li>相似：相似的类，IoU &gt; .1</li>
<li>其它：错误的类，IoU &gt; .1</li>
<li>背景：对任何物体 IoU &lt; .1</li>
</ul>
<p><img src="/archives/562809c1/1533274570698.png" alt="图4"></p>
<p>图4显示了各类错误的平均值。</p>
<p>YOLO很难准确定位物体。YOLO的定位错误比其余错误类型加起来都多。FRCN的定位错误少得多，但有更多的背景错误。它的检测里13.6%都是false positive，不含任何物体。</p>
<h3 id="4-3-Combining-Fast-R-CNN-and-YOLO"><a href="#4-3-Combining-Fast-R-CNN-and-YOLO" class="headerlink" title="4.3 Combining Fast R-CNN and YOLO"></a>4.3 Combining Fast R-CNN and YOLO</h3><p>YOLO比起FRCN，背景错误少很多。使用YOLO消除FRCN的背景检测结果，我们的得到了一个很大的性能提升。对于每个FRCN预测的限位框，我们都使用YOLO检查是否它也预测了一个近似的框。如果是的话，我们就基于YOLO预测的概率和两个框重叠程度，给与这个预测一个增加boost。</p>
<p>在VOC 07测试集上，最好的FRCN模型mAP为71.8%。当与YOLO结合后，提升了3.2%的mAP，达到75%。我们同样尝试了将FRCN模型与多个FRCN变种的结合，这些结合的提升都很低，约.3~.6%，详见表2。</p>
<p><img src="/archives/562809c1/1533281060562.png" alt="表2"></p>
<p>来自YOLO的提升绝不仅仅是组合模型的副产品。而是因为YOLO的错误类型不一样，故能用于提升FRCN的性能。</p>
<p>不幸的是，这一组合不能享受YOLO的速度优势，因为两个模型需要分别运行并组合其结果。不过，因为YOLO的快速，这比起FRCN，并没有显著的计算时间增加。</p>
<h3 id="4-4-VOC-2012-Results"><a href="#4-4-VOC-2012-Results" class="headerlink" title="4.4 VOC 2012 Results"></a>4.4 VOC 2012 Results</h3><p>在VOC 12测试集上，YOLO得到了57.9%的mAP。这比前沿水平要低，接近使用VGG-16的原始R-CNN，见表3。与其相似的竞争者比，YOLO对小物体很挣扎。在如瓶子、绵羊和显示器这些类别上，YOLO得分低于R-CNN或Feature Edit 8-10%。不过在其他类型如猫和火车，YOLO的性能更好。</p>
<p><img src="/archives/562809c1/1533281269019.png" alt="表3"></p>
<p>我们的FRCN+YOLO的组合模型是最高性能之一。加上YOLO后FRCN的mAP提高了2.3%，排名上升了5位。</p>
<h3 id="4-5-Generalizability-Person-Detection-in-Artwork"><a href="#4-5-Generalizability-Person-Detection-in-Artwork" class="headerlink" title="4.5 Generalizability: Person Detection in Artwork"></a>4.5 Generalizability: Person Detection in Artwork</h3><p>用于物体检测学术研究的数据集，训练和测试数据来自同一分布。在实际应用中，难以预测所有用例而系统面对的测试数据可能会偏离它见过的很多$^{[3]}$。我们把YOLO与其他检测系统在毕加索数据集$^{[12]}$和People-Art数据集$^{[3]}$上比较，用于检测艺术品里的人。</p>
<p>图5显示了YOLO与其它方法的性能比较。作为参考，我们给出了VOC 07的人类检测AP（仅使用VOC 07数据训练）。在毕加索测试的模型在VOC 12上训练，在People-Art测试的在VOC 10上训练。</p>
<p>R-CNN在VOC 07上的AP很高，但它应用于艺术品时降低很快。它所使用的用来提出限位框的SS是为自然图片调优的。R-CNN中的分类阶段只能看见小区域，所以需要好的候选。</p>
<p>在应用于艺术品时，DPM保持了它的AP。前面的工作能推理出这是因为DPM有着物体的形状和布局的强空间模型。尽管DPM没有像R-CNN那样退化，但AP还是有降低。</p>
<p>YOLO在VOC 07上性能不错，同时应用于艺术品时，AP下降也最少。与DPM一样，YOLO对物体的大小和形状以及物体与其常见同时出现的物体间关系建模。艺术品和自然图片在像素级差别很大，但在物体的大小形状等形式上很相似，因此YOLO仍能做出不错的限位框预测和检测。</p>
<p><img src="/archives/562809c1/1533283556398.png" alt="图5"></p>
<h2 id="5-Real-Time-Detection-In-The-Wild"><a href="#5-Real-Time-Detection-In-The-Wild" class="headerlink" title="5. Real-Time Detection In The Wild"></a>5. Real-Time Detection In The Wild</h2><p>YOLO快速、准确，是计算机视觉应用的理想模型。我们将YOLO与一个网络摄像头连接，验证其实时性能，包括获取图片和展示检测结果的时间。</p>
<p>系统结果是交互式且吸引人的。虽然YOLO独立的处理图片，但当它附着于摄像头后，它表现得像一个跟踪系统，随着物体移动、变化外形而检测它们。Demo和一些源码可以去我们的<a href="https://pjreddie.com/yolo/" rel="external nofollow noopener noreferrer" target="_blank">项目网站</a></p>
<h2 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6. Conclusion"></a>6. Conclusion</h2><p>我们提出了YOLO，一个统一的物体检测模型。我们的模型易于构建并能从完整图片直接训练。不像基于分类器的方法，训练YOLO的loss函数与检测性能直接相关且整个模型是共同训练的。</p>
<p>Fast YOLO是目前最快的通用物体检测器，YOLO推进了前沿的实时物体检测。YOLO也能很好的泛化到新领域中，让它非常适合需要快速、健壮的物体检测的应用。</p>
<p><img src="/archives/562809c1/1533283582281.png" alt="图6"></p>
<p><img src="/archives/562809c1/1533273390802.png" alt="表1"></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] M. B. Blaschko and C. H. Lampert. Learning to localize objects<br>with structured output regression. In Computer Vision–<br>ECCV 2008, pages 2–15. Springer, 2008. 4<br>[2] L. Bourdev and J. Malik. Poselets: Body part detectors<br>trained using 3d human pose annotations. In International<br>Conference on Computer Vision (ICCV), 2009. 8<br>[3] H. Cai, Q. Wu, T. Corradi, and P. Hall. The crossdepiction<br>problem: Computer vision algorithms for recognising<br>objects in artwork and in photographs. arXiv preprint<br>arXiv:1505.00110, 2015. 7<br>[4] N. Dalal and B. Triggs. Histograms of oriented gradients for<br>human detection. In Computer Vision and Pattern Recognition,2005. CVPR 2005. IEEE Computer Society Conference<br>on, volume 1, pages 886–893. IEEE, 2005. 4, 8<br>[5] T. Dean, M. Ruzon, M. Segal, J. Shlens, S. Vijayanarasimhan,<br>J. Yagnik, et al. Fast, accurate detection of<br>100,000 object classes on a single machine. In Computer<br>Vision and Pattern Recognition (CVPR), 2013 IEEE Conference<br>on, pages 1814–1821. IEEE, 2013. 5<br>[6] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,<br>E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation<br>feature for generic visual recognition. arXiv preprint<br>arXiv:1310.1531, 2013. 4<br>[7] J. Dong, Q. Chen, S. Yan, and A. Yuille. Towards unified<br>object detection and semantic segmentation. In Computer<br>Vision–ECCV 2014, pages 299–314. Springer, 2014. 7<br>[8] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable<br>object detection using deep neural networks. In Computer<br>Vision and Pattern Recognition (CVPR), 2014 IEEE Conference<br>on, pages 2155–2162. IEEE, 2014. 5, 6<br>[9] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I.<br>Williams, J. Winn, and A. Zisserman. The pascal visual object<br>classes challenge: A retrospective. International Journal<br>of Computer Vision, 111(1):98–136, Jan. 2015. 2<br>[10] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan.<br>Object detection with discriminatively trained part<br>based models. IEEE Transactions on Pattern Analysis and<br>Machine Intelligence, 32(9):1627–1645, 2010. 1, 4<br>[11] S. Gidaris and N. Komodakis. Object detection via a multiregion<br>&amp; semantic segmentation-aware CNN model. CoRR,<br>abs/1505.01749, 2015. 7<br>[12] S. Ginosar, D. Haas, T. Brown, and J. Malik. Detecting people<br>in cubist art. In Computer Vision-ECCV 2014 Workshops,<br>pages 101–116. Springer, 2014. 7<br>[13] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature<br>hierarchies for accurate object detection and semantic<br>segmentation. In Computer Vision and Pattern Recognition<br>(CVPR), 2014 IEEE Conference on, pages 580–587. IEEE,2014. 1, 4, 7<br>[14] R. B. Girshick. Fast R-CNN. CoRR, abs/1504.08083, 2015.<br>2, 5, 6, 7<br>[15] S. Gould, T. Gao, and D. Koller. Region-based segmentation<br>and object detection. In Advances in neural information<br>processing systems, pages 655–663, 2009. 4<br>[16] B. Hariharan, P. Arbelaez, R. Girshick, and J. Malik. Simul- ´<br>taneous detection and segmentation. In Computer Vision–<br>ECCV 2014, pages 297–312. Springer, 2014. 7<br>[17] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling<br>in deep convolutional networks for visual recognition. arXiv<br>preprint arXiv:1406.4729, 2014. 5<br>[18] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and<br>R. R. Salakhutdinov. Improving neural networks by preventing<br>co-adaptation of feature detectors. arXiv preprint<br>arXiv:1207.0580, 2012. 4<br>[19] D. Hoiem, Y. Chodpathumwan, and Q. Dai. Diagnosing error<br>in object detectors. In Computer Vision–ECCV 2012, pages<br>340–353. Springer, 2012. 6<br>[20] K. Lenc and A. Vedaldi. R-cnn minus r. arXiv preprint<br>arXiv:1506.06981, 2015. 5, 6<br>[21] R. Lienhart and J. Maydt. An extended set of haar-like features<br>for rapid object detection. In Image Processing. 2002.<br>Proceedings. 2002 International Conference on, volume 1,<br>pages I–900. IEEE, 2002. 4<br>[22] M. Lin, Q. Chen, and S. Yan. Network in network. CoRR,<br>abs/1312.4400, 2013. 2<br>[23] D. G. Lowe. Object recognition from local scale-invariant<br>features. In Computer vision, 1999. The proceedings of the<br>seventh IEEE international conference on, volume 2, pages<br>1150–1157. Ieee, 1999. 4<br>[24] D. Mishkin. Models accuracy on imagenet 2012<br>val. <a href="https://github.com/BVLC/caffe/wiki/" rel="external nofollow noopener noreferrer" target="_blank">https://github.com/BVLC/caffe/wiki/</a><br>Models-accuracy-on-ImageNet-2012-val. Accessed:<br>2015-10-2. 3<br>[25] C. P. Papageorgiou, M. Oren, and T. Poggio. A general<br>framework for object detection. In Computer vision, 1998.<br>sixth international conference on, pages 555–562. IEEE,1998. 4<br>[26] J. Redmon. Darknet: Open source neural networks in c.<br><a href="http://pjreddie.com/darknet/" rel="external nofollow noopener noreferrer" target="_blank">http://pjreddie.com/darknet/</a>, 2013–2016. 3<br>[27] J. Redmon and A. Angelova. Real-time grasp detection using<br>convolutional neural networks. CoRR, abs/1412.3128, 2014.<br>5<br>[28] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards<br>real-time object detection with region proposal networks.<br>arXiv preprint arXiv:1506.01497, 2015. 5, 6, 7<br>[29] S. Ren, K. He, R. B. Girshick, X. Zhang, and J. Sun. Object<br>detection networks on convolutional feature maps. CoRR,<br>abs/1504.06066, 2015. 3, 7<br>[30] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,<br>S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,<br>A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual<br>Recognition Challenge. International Journal of Computer<br>Vision (IJCV), 2015. 3<br>[31] M. A. Sadeghi and D. Forsyth. 30hz object detection with<br>dpm v5. In Computer Vision–ECCV 2014, pages 65–79.<br>Springer, 2014. 5, 6<br>[32] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,<br>and Y. LeCun. Overfeat: Integrated recognition, localization<br>and detection using convolutional networks. CoRR,<br>abs/1312.6229, 2013. 4, 5<br>[33] Z. Shen and X. Xue. Do more dropouts in pool5 feature maps<br>for better object detection. arXiv preprint arXiv:1409.6911,2014. 7<br>[34] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,<br>D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.<br>Going deeper with convolutions. CoRR, abs/1409.4842,2014. 2<br>[35] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W.<br>Smeulders. Selective search for object recognition. International<br>journal of computer vision, 104(2):154–171, 2013.<br>4<br>[36] P. Viola and M. Jones. Robust real-time object detection.<br>International Journal of Computer Vision, 4:34–47, 2001. 4<br>[37] P. Viola and M. J. Jones. Robust real-time face detection.<br>International journal of computer vision, 57(2):137–154,2004. 5<br>[38] J. Yan, Z. Lei, L. Wen, and S. Z. Li. The fastest deformable<br>part model for object detection. In Computer Vision and Pattern<br>Recognition (CVPR), 2014 IEEE Conference on, pages<br>2497–2504. IEEE, 2014. 5, 6<br>[39] C. L. Zitnick and P. Dollar. Edge boxes: Locating object pro- ´<br>posals from edges. In Computer Vision–ECCV 2014, pages<br>391–405. Springer, 2014. 4</p>

      
    </div>

    
      

  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="/archives/423509a9.html" rel="bookmark">YOLOv2 and YOLO9000</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="/archives/2c51a51a.html" rel="bookmark">YOLOv3</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="/archives/8504aa6c.html" rel="bookmark">RetinaNet</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="/archives/fc798de3.html" rel="bookmark">Faster R-CNN</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="/archives/719cc2e5.html" rel="bookmark">SSD</a></div>
      
    </li>
  
  </ul>


    

    
    
    

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>慕湮</li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://muyaan.com/archives/562809c1.html" title="YOLO">http://muyaan.com/archives/562809c1.html</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noopener noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！</li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Computer-Vision/" rel="tag"># Computer Vision</a>
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
            <a href="/tags/Paper/" rel="tag"># Paper</a>
          
            <a href="/tags/Object-Detection/" rel="tag"># Object Detection</a>
          
            <a href="/tags/YOLO/" rel="tag"># YOLO</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/archives/fc798de3.html" rel="next" title="Faster R-CNN">
                <i class="fa fa-chevron-left"></i> Faster R-CNN
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/archives/8504aa6c.html" rel="prev" title="RetinaNet">
                RetinaNet <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4" alt="慕湮">
            
              <p class="site-author-name" itemprop="name">慕湮</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">21</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">30</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/tycallen" target="_blank" title="GitHub" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:tyc.allen@gmail.com" target="_blank" title="E-Mail" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="http://weibo.com/pojunallen" target="_blank" title="微博" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-weibo"></i>微博</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://tuchong.com/1070837" target="_blank" title="图虫" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-globe"></i>图虫</a>
                  
                </span>
              
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友情链接
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://dotrabbit.github.io/" title="dotrabbit" target="_blank" rel="external nofollow noopener noreferrer">dotrabbit</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Introduction"><span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Unified-Detection"><span class="nav-text">2. Unified Detection</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-Network-Design"><span class="nav-text">2.1 Network Design</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Training"><span class="nav-text">2.2 Training</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-Inference"><span class="nav-text">2.3 Inference</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-Limitations-of-YOLO"><span class="nav-text">2.4 Limitations of YOLO</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Comparison-to-Other-Detection-Systems"><span class="nav-text">3. Comparison to Other Detection Systems</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Experiments"><span class="nav-text">4. Experiments</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-Comparison-to-Other-Real-Time-Systems"><span class="nav-text">4.1 Comparison to Other Real-Time Systems</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-VOC-2007-Error-Analysis"><span class="nav-text">4.2 VOC 2007 Error Analysis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-Combining-Fast-R-CNN-and-YOLO"><span class="nav-text">4.3 Combining Fast R-CNN and YOLO</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-VOC-2012-Results"><span class="nav-text">4.4 VOC 2012 Results</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-5-Generalizability-Person-Detection-in-Artwork"><span class="nav-text">4.5 Generalizability: Person Detection in Artwork</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Real-Time-Detection-In-The-Wild"><span class="nav-text">5. Real-Time Detection In The Wild</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-Conclusion"><span class="nav-text">6. Conclusion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-text">References</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2015 – <span itemprop="copyrightYear">2018</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">慕湮</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
    <span title="站点总字数">313k</span>
  

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    
    <span title="站点阅读时长">9:30</span>
  
</div>


  



  <div class="powered-by">由 <a class="theme-link" target="_blank" rel="external nofollow noopener noreferrer" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a class="theme-link" target="_blank" rel="external nofollow noopener noreferrer" href="https://theme-next.org">NexT.Muse</a></div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>














  











  



  
  
    <script type="text/javascript" src="//cdn.staticfile.org/jquery/2.1.3/jquery.min.js"></script>
  

  
  
    <script type="text/javascript" src="//cdn.staticfile.org/velocity/1.2.1/velocity.min.js"></script>
  

  
  
    <script type="text/javascript" src="//cdn.staticfile.org/velocity/1.2.1/velocity.ui.min.js"></script>
  

  
  
    <script type="text/javascript" src="//cdn.staticfile.org/canvas-nest.js/1.0.0/canvas-nest.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/reading_progress/reading_progress.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.4.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.4.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.4.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.4.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.1"></script>



  



  










  





  

  
  <script>
    
    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();

      Counter('get', '/classes/Counter', { where: JSON.stringify({ url }) })
        .done(function ({ results }) {
          if (results.length > 0) {
            var counter = results[0];
            
              var $element = $(document.getElementById(url));
              $element.find('.leancloud-visitors-count').text(counter.time + 1);
            
            Counter('put', `/classes/Counter/${counter.objectId}`, JSON.stringify({ time: { "__op":"Increment", "amount":1 } }))
            
            .fail(function ({ responseJSON }) {
                console.log('Failed to save Visitor num, with error message: ' + responseJSON.error);
            })
          } else {
            
              Counter('post', '/classes/Counter', JSON.stringify({ title: title, url: url, time: 1}))
                .done(function () {
                  var $element = $(document.getElementById(url));
                  $element.find('.leancloud-visitors-count').text(1);
                })
                .fail(function () {
                  console.log('Failed to create');
                });
            
          }
        })
      .fail(function ({ responseJSON }) {
        console.log('LeanCloud Counter Error:' + responseJSON.code + " " + responseJSON.error);
      });
    }
    

    $(function() {
      $.get('https://app-router.leancloud.cn/2/route?appId=' + "UWGXBMGbguCDhSCbC3NrUQtL-gzGzoHsz")
        .done(function ({ api_server }) {
          var Counter = function (method, url, data) {
            return $.ajax({
              method: method,
              url: `https://${api_server}/1.1${url}`,
              headers: {
                'X-LC-Id': "UWGXBMGbguCDhSCbC3NrUQtL-gzGzoHsz",
                'X-LC-Key': "ke1jrA5b6VyR89Kqqqwf2kPP",
                'Content-Type': 'application/json',
              },
              data: data,
            });
          };
          
          addCount(Counter);
          
        })
    });
  </script>



  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  
  

  

  

  

  

  

</body>
</html>
