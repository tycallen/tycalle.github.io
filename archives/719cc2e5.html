<!DOCTYPE html><html class="theme-next muse use-motion" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="//cdn.staticfile.org/font-awesome/4.6.2/css/font-awesome.min.css" rel="stylesheet" type="text/css"><link href="/css/main.css?v=6.3.0" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.3.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.3.0"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.3.0"><link rel="mask-icon" href="/images/logo.svg?v=6.3.0" color="#222"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Muse",version:"6.3.0",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!1,fastclick:!1,lazyload:!1,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="description" content="SSD论文翻译SSD: Single Shot MultiBox DetectorWei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, ChengYang Fu, Alexander C. Berg 2015UNC Chapel Hill, Zoox Inc. Google Inc. University"><meta name="keywords" content="Computer Vision,Deep Learning,Paper,Object Detection,SSD"><meta property="og:type" content="article"><meta property="og:title" content="SSD"><meta property="og:url" content="http://muyaan.com/archives/719cc2e5.html"><meta property="og:site_name" content="慕湮"><meta property="og:description" content="SSD论文翻译SSD: Single Shot MultiBox DetectorWei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, ChengYang Fu, Alexander C. Berg 2015UNC Chapel Hill, Zoox Inc. Google Inc. University"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://muyaan.com/archives/1533537500132.png"><meta property="og:image" content="http://muyaan.com/archives/1533536532106.png"><meta property="og:image" content="http://muyaan.com/archives/1533556398807.png"><meta property="og:image" content="http://muyaan.com/archives/1533557474247.png"><meta property="og:image" content="http://muyaan.com/archives/1533557506757.png"><meta property="og:image" content="http://muyaan.com/archives/1533556834116.png"><meta property="og:image" content="http://muyaan.com/archives/1533558678487.png"><meta property="og:image" content="http://muyaan.com/archives/1533560323532.png"><meta property="og:image" content="http://muyaan.com/archives/1533603434721.png"><meta property="og:image" content="http://muyaan.com/archives/1533607827717.png"><meta property="og:updated_time" content="2018-09-12T03:22:25.033Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="SSD"><meta name="twitter:description" content="SSD论文翻译SSD: Single Shot MultiBox DetectorWei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, ChengYang Fu, Alexander C. Berg 2015UNC Chapel Hill, Zoox Inc. Google Inc. University"><meta name="twitter:image" content="http://muyaan.com/archives/1533537500132.png"><link rel="canonical" href="http://muyaan.com/archives/719cc2e5.html"><script type="text/javascript" id="page.configurations">CONFIG.page={sidebar:""}</script><meta name="google-site-verification" content="Ojd_HrL_PelaXvKK5IkbhLbjZ_sHt6IxRzP-XPaaTw4"><meta name="baidu-site-verification" content="skdT7mIEAb"><title>SSD | 慕湮</title><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?ca5844321cfb80fdf6f12b4dcc326991";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style type="text/css">.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.logo-line-after i{right:initial}</style></noscript></head><body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">慕湮</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">白日放歌须纵酒</h1></div><div class="site-nav-toggle"><button aria-label="Toggle navigation bar"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-archives menu-item-active"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><div class="reading-progress-bar"></div><article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://muyaan.com/archives/719cc2e5.html"><span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="慕湮"><meta itemprop="description" content=""><meta itemprop="image" content="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4"></span><span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="慕湮"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">SSD</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2018-08-07 10:11:28" itemprop="dateCreated datePublished" datetime="2018-08-07T10:11:28+08:00">2018-08-07</time> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">Edited on</span> <time title="Modified: 2018-09-12 11:22:25" itemprop="dateModified" datetime="2018-09-12T11:22:25+08:00">2018-09-12</time> </span><span id="/archives/719cc2e5.html" class="leancloud_visitors" data-flag-title="SSD"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Views: </span><span class="leancloud-visitors-count"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数: </span><span title="Symbols count in article">13k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="Reading time">23 mins.</span></div></div></header><div class="post-body" itemprop="articleBody"><p>SSD论文翻译</p><blockquote><p><a href="https://arxiv.org/abs/1512.02325v2" rel="external nofollow noopener noreferrer" target="_blank">SSD: Single Shot MultiBox Detector</a><br>Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, ChengYang Fu, Alexander C. Berg 2015<br>UNC Chapel Hill, Zoox Inc. Google Inc. University of Michigan, Ann-Arbor</p></blockquote><a id="more"></a><hr><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>前沿的物体检测系统都是以下方法的变体：猜测限位框，为每个限位框重采样像素或特征，并应用一个高质量分类器。自SS起到现在所有如Faster R-CNN等前沿方法，这一流程一直流行。尽管很精确，但对于嵌入式系统计算消耗难以承受，而且就算使用高端硬件，也难以进行实时甚至类实时的应用。通常，这些方法处理一帧的时间以秒记，而最高精度的检测器如基础Faster R-CNN，fps仅为7。业界通过对检测流程各阶段的攻关（见章节4，相关工作），广泛地尝试了构建更快的检测器。但截至目前，有效的提速只来自于显著的检测准确度牺牲。</p><p>本论文提出了首个不为限位框重采样像素或特征的基于深度网络的物体检测器，同时也是个准确的方法。这一结果显著地提升了高精度方法的速度（VOC 07测试集上 有58fps和72.1%mAP vs Faster R-CNN 7fps 73.2%map vs YOLO 45fps 63.4%mAP）。速度提升来自消除了限位框proposal步骤和后续像素或特征重采样步骤。本论文不是首个进行这些工作的$^{[4,5]}$，但我们通过增加一系列提升，我们成功地相对先前尝试显著地提升了准确度。我们的提升包括使用一个小卷积filter来预测物体类别和限位框偏移，为不同宽高比检测使用不同的预测器（filter），并在网络最后一层将这些filters应用到多个Feature map上，以实现多尺度检测。通过这些修改，我们能使用相对较低的输入分辨率获得较高的准确度。尽管这些修改的帮助分开来看很小，但最终系统却能在高速检测下提升精确度。相比近期在残差网络上的高度调优的工作，这是一个更大的准确度提升。</p><p>我们总结本论文贡献如下：</p><ul><li>我们提出了SSD，一个用于多种类的单步检测器，比前沿单步检测器（YOLO）快，同时更加精确，甚至接近了那些使用明确候选区域方法和池化的更慢方法的准确度（包括Faster R-CNN）。</li><li>SSD方法的核心是为默认限位框固定集用一个小卷积filter，对feature map预测类得分和限位框偏移。</li><li>为了获得更高的准确度，我们从不同尺度的feature map来预测，并显式地通过不同的长宽比区分。</li><li>这些设计特征结合起来得到了一个简单的端到端训练的，尽管使用相对低输入分辨率的高精度的系统，进一步提升了速度、精度权衡。</li><li>在VOC，COOC和ILSVRC上使用了各种输入大小测试时间和精度，并与一系列前沿方法进行了比较。</li></ul><h2 id="2-The-Single-Shot-Detector-SSD"><a href="#2-The-Single-Shot-Detector-SSD" class="headerlink" title="2. The Single Shot Detector(SSD)"></a>2. The Single Shot Detector(SSD)</h2><p>本节描述了我们提出的用于检测的SSD框架（章节2.1）和相关训练方法论（章节2.2）。后续章节2.3呈现了数据集相关模型细节和实验结果。</p><h3 id="2-1-Model"><a href="#2-1-Model" class="headerlink" title="2.1 Model"></a>2.1 Model</h3><p>SSD是建立在产生固定数量的限位框和物体类出现在那些框中的得分，接着用一个非极大值抑制来产生最终结果的卷积网络之上。网络前几层基于标准的图片分类结构(在任何分类层前截断)，我们将其称为基网。接下来我们为网络增加额外的结构，根据以下关键特征输出决策：</p><p><strong>Multi-scale feature maps for detection </strong>我们在基网末加入卷积特征层。这些层的尺寸逐渐减小，允许在多个尺度进行检测。每一个特征层使用的预测卷积模型都不一样（OverFeat 和YOLO在单尺度feature map上操作）。</p><p><strong>Convolutional predictors for detection</strong> 每个加入的特征层（或来自基网的原有的特征层）能用卷积filter集生成固定检测集。对于p个通道，尺寸为$m \times n$的特征层，可能性检测的预测参数的基础元素是一个$3 \times 3 \times p$的小kernel，生成一个类的得分或相对默认框坐标的形状偏移。在$m \times n$中的每个kernel使用位置，生成一个输出值。限位框偏移输出值是相对每个feature map默认限位框位置测算的（YOLO结构在这一步使用中间全连接层代替了本论文的卷积filter）</p><p><img src="/archives/./1533537500132.png" alt="图2"></p><p><strong>Default boxes and aspect ratios</strong> 我们为网络顶部的多个feature map中的每个格子与一个限位框集关联了起来。默认限位框以卷积的形式覆盖feature map，因此每个限位框与其对应格子固定下来。在每个feature map格子，我们预测与该格子对应限位框形状的偏移，以及在每个格子中的每类得分，指示该类物体出现在该框中的概率。在给定位置的k个格子中的每一个，我们计算类得分$c$和相对原限位框的4个偏移值。结果是feature map每个位置应用$(c = 4)k$个filter，一个$m \times n$的feature map最终得到$(c +4)kmn$的输出。图例见图1.。我们的默认框类似Faster R-CNN使用的锚点框，不过我们将其应用于不同分辨率的多个feature map。允许多个feature map中不同的box形状，允许我们高效地将限位框可能的输出形状空间离散化。</p><p><img src="/archives/./1533536532106.png" alt="图1"></p><h3 id="2-2-Training"><a href="#2-2-Training" class="headerlink" title="2.2 Training"></a>2.2 Training</h3><p>SSD和典型检测器训练过程，在最终分类器前使用候选区域和池化的，的区别是gt信息需要在固定集合的检测器输出中赋予特定的输出。有的训练过程中如YOLO$^{[5]}$或候选区域过程中如Faster R-CNN$^{[2]}$和MultiBox$^{[7]}$中也有这一步骤的其它版本。一旦这一赋值确定后，loss函数和反向传播就能端到端地进行了。训练同样牵涉到用于检测的默认限位框和尺寸的设定选择，以及HEM和数据增广策略。</p><p><strong>Matching strategy</strong> 在训练时我们需要建立起gt和默认框的对应关系。注意我们为每个gt框选择的默认框都是来自不同位置、长宽比和尺度范围中的。一开始根据jaccard overlap来匹配每个gt框与默认框。这是原始MultiBox$^{[7]}$使用的匹配方法，它保证了每个gt框都正好有一个匹配的默认框。与MultiBox不同的是，我们接着把默认框与任意jaccard overlap高于一个阈值（0.5）的gt框匹配起来。这些匹配的加入简化了训练问题：它允许网络为多个重叠的默认框预测高信心分而不是要求它找出有最大重叠的那一个。</p><p><strong>Training objective</strong> SSD的训练目标源自MultiBox，但将其扩展为支持多类物体。让我们将$x_{ij}^p=1$记做第i个默认框匹配了类p的第j个gt框，等于0则相反。至于上面描述的匹配策略，我们有$\sum_i x^p_{ij} \gt 1$，说明匹配第j个gt框的默认框数量可以大于1。整体loss函数是一个定位loss loc和信心loss conf的加权和：</p><script type="math/tex;mode=display">L(x,c,l,g) = \frac 1 N (L_{conf} (x,c) + \alpha L_{loc} (x,l,g)) \tag{1}</script><p>其中N是匹配的默认框数量，loc loss是预测框l和gt框g之间的Smooth L1 loss$^{[6]}$。类似Faster R-CNN，我们对其余限位框中心的偏移和其宽高回归。conf loss 是对多类信心(c)和权重$\alpha$（通过交叉验证，设为1）的softmax loss。</p><p><strong>Choosing scales and aspect ratios for default boxes</strong> 大部分卷积网络在其较深的层降低其feature map大小。这不仅能缩减计算量和内存消耗，它同样能提供一定程度的平移和尺度不变性。为了处理不同的物体尺度，一些方法$^{[4,9]}$建议将图片转换为不同大小并独立处理，最后合并结果。不过通过利用来自单网络中的多个不同层的feature map来预测，我们也能达到同样效果，同时还在所有尺度间共享了参数。之前的论文[10,11]显示了使用来自较低层的feature map能提升语义分割质量，因为底层能获取更多来自输入物体的细小细节。类似的，[12]显示为最顶层的feature map添加全局上下文池化global context pooled能有助于分割结果的平滑。被这些方法所激发，我们把高、低feature map都用于检测。图1显示了网络所用的两个样例feature map（$8 \times 8 \ and\ 4 \times 4$），当然实际中我们会使用更多，仅需相对很小的计算量增加。</p><p>众所周知，来自一个网络中不同层的feature map有着不同大小的感受野$^{[13]}$。幸运的是，在SSD网络中，默认框不需要对应到各层的实际感受野。我们能设计覆盖方式使特定feature map位置能学到对图片特定区域和特定物体尺度响应。假如我们希望使用m个feature map进行预测。每个feature map的默认框尺度计算如下：</p><script type="math/tex;mode=display">s_k = s_{min} + \frac {s_{max} - s_{min}} {m -1} (k -1), k \in [1,m] \tag{2}</script><p>当$s_{min} = 0.2$和$s_{max} = 0.95$时，意味着最低层尺度为0.2，最高层尺度为0.95，之间的层规则分布。我们为默认限位框提出不同的长宽比，并将它们记做$a_r \in \{ 1,2,3, \frac 1 2, \frac 1 3 \}$。我们能为每个默认框计算宽（$w_k^a=s_k\sqrt {a_r} $）和高（$h_k^a=s_k/\sqrt {a_r}$）。对于长宽比为1的，我们也增加一个尺度为$s’_k = \sqrt {s_k s_{k+1}}$的默认框。最终每个feature map位置有6个默认框。我们将每个默认框中心设为$(\frac {(i+0.5)} {|f_k|} , \frac {j+0.5} {|f_k|})$，其中$|f_k|$是第k个方形feature map的尺寸，$i,j \in [0,|f_k|)$。我们截取默认框的坐标，让其永远落入[0,1]。</p><p>将对许多feature map所有位置的不同尺度和宽高比的所有默认框预测集合起来，我们有了多种预测，覆盖不同的输入物体尺寸和形状。例如，如图1，狗被匹配到了$4 \times 4$feature map中的一个默认框，而在$8\times8$feature map中没有任何匹配。这是因为那些限位框有着不同的尺度且不能匹配狗的gt框，因此都在训练时当做负样本。</p><p><strong>Hard negative mining</strong> 在匹配之后，大部分默认框都成为负样本，特别是当候选框数量很大时。这导致了正负训练样本的极度不平衡。与其使用所有负样本，我们为每个默认框的最高信心分排序，并取最高的使正负样本比例约1:3。我们发现这会让优化更快，训练更稳定。</p><p><strong>Date augmentation</strong> 为了让模型面对各种输入物体的大小和形状更健壮，每个训练图片都随机使用如下选项抽样：</p><ul><li>使用整个原始输入图片</li><li>抽样一个图片块使其与物体的最小jaccard overlap 为0.1,0.3,0.5,0.7或0.9</li><li>随机采样一个图片块</li></ul><p>每个采样的图片块都是原图大小的[0.1, 1]，长宽比为0.5到2。如果gt框中心在采样块内部，我们会保留其超出区域。在上述采样步骤后，每个块都缩放至固定大小，并以0.5的概率翻转。</p><h2 id="3-Experimental-Results"><a href="#3-Experimental-Results" class="headerlink" title="3 Experimental Results"></a>3 Experimental Results</h2><p><strong>Base network</strong> 我们的实验均基于VGG-16，一个在ILSVRC CLS-LOC数据集上预训练的模型。类似DeepLab-LargeFOV$^{[16]}$，我们把fc6和fc7转换为卷积层，对fc6和fc7进行二次采样，将pool5从$2 \times 2 -s2$改为$3\times 3-s1$，并使用atrous算法填补”hole”。我们移除了所有dropout层和fc8层。我们用SGD对模型进行调优，初始学习率为0.001，momentum为0.9，decay为0.0005，batch-size为32。学习率衰减策略在各数据集上略有不同，我们将在后续描述细节。所有基于Caffe的训练和测试代码已<a href="https://github.com/weiliu89/caffe/tree/ssd" rel="external nofollow noopener noreferrer" target="_blank">开源</a></p><h3 id="3-1-PASCAL-VOC-2007"><a href="#3-1-PASCAL-VOC-2007" class="headerlink" title="3.1 PASCAL VOC 2007"></a>3.1 PASCAL VOC 2007</h3><p>我们在这一数据集上与FRCN$^{[6]}$和Faster R-CNN$^{[2]}$进行了比较。所有的方法都使用同样的训练数据和同一个预训练的VGG16网络。我们在VOC 07 12 训练验证集（16551张图片）上训练，在VOC 07测试集（4952张图片）上测试。</p><p>图2显示了SSD300模型的架构细节。我们使用了conv4_3, conv7(fc7), conv8_2,conv9_2, conv10_2和pool11来预测位置和信心。我们用xavier方法$^{[18]}$来为所有新添加的卷积层初始化参数。由于conv4_3尺寸较大($38 \times 38$)，我们仅为其放置3个默认框——一个scale为0.1，另两个长宽比为1:2和2。对于所有其它层，我们如在章节2.2中描述的一样放置6个默认限位框。因为如[12]指出的那样，conv4_3有与其余层不一样的特征尺度，我们使用[12]提出的L2归一化方法来将feature map中的每个位置的feature norm缩放到20，并在反向传播中训练缩放程度。前40k迭代我们使用0.001的学习率，接着0.0001进行20k。表1显示我们的SSD300模型已经比FRCN更精确。当我们用$500 \times 500$的图片训练SSD时精度更高，超越了Faster R-CNN 1.9%的mAP。</p><p><img src="/archives/./1533556398807.png" alt="表1"></p><p>为了从细节上理解我们两个SSD模型的性能，我们使用了来自[19]的检测分析工具。图3显示SSD能检测高质量（空白多）的不同物体类别。其有信心的检测的大部分都是正确的。召回率约85-90%，比起弱条件高得多。与R-CNN相比，SSD的定位错误更少，说明SSD由于直接学习而不是通过两步来回归物体形状和分类，它能更好地定位物体。但是SSD在处理多个相似物体类别（特别是动物）时很容易弄混，部分原因是因为我们在多个类别中共享了位置。</p><p><img src="/archives/./1533557474247.png" alt="图3"></p><p>图4显示SSD对限位框大小极其敏感。换句话说，它在小物体上的性能要比大物体差得多。这并不令人惊讶，因为小物体在最上层也许几乎没有任何信息。提升输入大小（从$300 \times 300$到$500 \times 500$）能帮助小物体的检测，但仍有很大提升空间。正面地讲，我们已经确认SSD在大物体上表现相当好。同时因为我们在每个feature map位置使用的默认框有着各种宽高比，模型对于不同宽高比的物体非常健壮。</p><p><img src="/archives/./1533557506757.png" alt="图4"></p><h3 id="3-2-Model-analysis"><a href="#3-2-Model-analysis" class="headerlink" title="3.2 Model analysis"></a>3.2 Model analysis</h3><p>为了更好地理解SSD，我们也进行了多个控制实验来检验各个组件对最终性能的影响。随后所有实验，我们都使用同样的设置和输入尺寸($300 \times 300$)，除了变量组件。</p><p><img src="/archives/./1533556834116.png" alt="表2"></p><p><strong>Data augmentation is crucial</strong> Fast and Faster R-CNN使用原始图片和水平翻转（0.5概率翻转）来训练。我们使用了更复杂的采样策略，类似YOLO，但他们使用了我们未使用的测光扰动。表2显示我们能通过这一采样策略提升6.7%的mAP。我们并不清楚我们的采样策略能否帮助FRCN和Faster R-CNN，但它们应该提升不会这么大，因为在分类阶段他们使用了特征池化，该设计会带来相对的物体平移健壮性。</p><p><strong>More feature maps is better</strong> 被许多语义分割的研究$^{[10,11,12]}$激发，我们也用了相对低级的feature map来预测限位框位置。我们比较了两个模型，一个使用conv4_3，另一个不使用。从表2中我们可以看出，通过增加conv4_3用于预测，得到了一个更好的结果（72.1% vs 68.1%）。这也同样符合我们对于conv4_3能捕捉更好物体细节，尤其是小物体的预期。</p><p><strong>More default box shapes is better</strong> 如章节2.2中描述那样，我们默认在每个位置使用6个限位框。如果我们移除宽高比为1:3和3:1的框，性能会下降0.9%。如果我们继续移除1:2和2:1的默认框，性能会再次下降2%。使用多种默认限位框形状能使网络预测框的任务简单。</p><p><strong>Atrous is better and faster</strong> 如章节3中描述的那样，我们使用了来自DeepLab-LargeFOV的atrous版本VGG16。如果我们使用完整VGG16，将pool5保持为$2\times 2-s2$，不对fc6和fc7参数再次采样，并增加conv5_3作为预测，结果略差（0.7%）且速度要慢50%。</p><h3 id="3-3-PASCAL-VOC2012"><a href="#3-3-PASCAL-VOC2012" class="headerlink" title="3.3 PASCAL VOC2012"></a>3.3 PASCAL VOC2012</h3><p>我们使用与VOC 07测试同样的设定。这次我们使用VOC 12训练验证集和VOC 07训练验证集以及测试集（21503张）来训练，在VOC 12测试集（10991张）上测试。由于训练集变大了，我们用0.001的学习率训练了60k，并用0.0001训练了20k。</p><p>表3显示了我们的SSD300和SSD500模型的结果。我们看到了与VOC07同样的性能趋势。我们的SSD300已经超过FRCN，非常接近Faster R-CNN(0.1%的差距)。通过将训练、测试图片尺寸增加至$500 \times 500$，我们比Faster R-CNN高2.7%。与YOLO相比，SSD要好得多，可能是因为我们训练中使用的多feature map 卷积化默认框和匹配策略。</p><p><img src="/archives/./1533558678487.png" alt="表3"></p><h3 id="3-4-MS-COCO"><a href="#3-4-MS-COCO" class="headerlink" title="3.4 MS COCO"></a>3.4 MS COCO</h3><p>为了进一步验证我们的SSD框架，我们在COCO数据集上训练了我们的SSD300和SSD500模型。由于COCO中的物体偏小，我们在所有层都使用了更小的默认框。我们按照章节2.2中提到的策略，但现在我们最小的默认框尺度为0.1而不是0.2，在conv4_3上的默认框尺度为0.07（相当于一个$300 \times 300$图片上的21个像素）。</p><p>我们使用trainval135k来训练我们的模型。因为COCO有着更多的数据类别，梯度一开始时并不稳定。我们先用$8 \times 10^{-4}$学习率训练4k，接着用0.001训练140k，继续用0.0001训练60k，0.00001训练40k。表4显示了在test-dev2015上的结果。与我们在VOC数据集上观察到的类似，SSD300比FRCN在mAP@0.5和mAP@[0.5:0.95]都好。SSD300与Faster R-CNN有类似的mAP@[0.5:0.95]，但mAP@0.5要差。我们推测这是因为图片尺寸过小，阻止了模型准确地定位很多小物体。通过提升图片尺寸到$500 \times 500$，SSD500在两个指标上都超过了Faster R-CNN。另外，我们的SSD500模型同样好过ION$^{[21]}$，一个FRCN的多scale版本，用一个recurrent网络显式地建模上下文。在图5中，我们展示了一些COCO test-dev下的SSD500模型检测样本。</p><p><img src="/archives/./1533560323532.png" alt="表4"></p><h3 id="3-5-Preliminary-ILSVRC-results"><a href="#3-5-Preliminary-ILSVRC-results" class="headerlink" title="3.5 Preliminary ILSVRC results"></a>3.5 Preliminary ILSVRC results</h3><p>我们把COCO所用架构应用于ILSVRC DET数据集$^{[15]}$上。我们在ILSVRC 14 DET train和val1上训练了一个SSD300模型。我们首先用0.0008的学习率训练了4k，接着用0.001训练了320k，随后是0.0001，100k和0.00001，60k。我们在val2上得到了41.1的mAP。再一次地验证了SSD作为泛化的高质量实时检测系统的框架能力。</p><h3 id="3-6-Inference-time"><a href="#3-6-Inference-time" class="headerlink" title="3.6 Inference time"></a>3.6 Inference time</h3><p>考虑到我们方法生成的限位框数量很大，在推理时有效使用非极大值抑制nms很有必要。通过一个0.01的信心阈值就能滤除大部分框。接着我们靠CUDA的排序，用GPU来计算剩余框的两两交叠，并对每类应用jaccard overlap为0.45的nms，保留每张图片的top 200个检测。SSD300处理每张图片需要2.2ms（20个VOC类），很接近新增层的总消耗时间。</p><p>表5显示了SSD，Faster R-CNN和YOLO的比较。Faster R-CNN使用额外的层来预测候选区域，需要特征重采样。与其相反，我们的SSD500在速度和精度上都超越了它。值得注意的是我们的SSD300是唯一能达到70%mAP的实时系统。尽管Fast YOLO fps为155，但它mAP低了近20%。</p><p><img src="/archives/./1533603434721.png" alt="表5"></p><h2 id="4-Related-Work"><a href="#4-Related-Work" class="headerlink" title="4. Related Work"></a>4. Related Work</h2><p>在图像物体检测领域有两类稳定方法，一个基于滑动窗口另一个基于候选区域分类。在卷积网络到来前，两个方法前沿技术为：DMP和SS，有着近似的性能。但在R-CNN带来的戏剧性提升后，它将基于SS的候选区域和基于卷积网络的后分类结合起来，候选区域方法就变得流行起来。</p><p>原始R-CNN方法以被多种方式优化。首先进行的是提升后分类的质量和速度，因为它需要分类数千个图片块，是非常耗时的。SPPnet显著地加速了原始R-CNN。它提出了空间金字塔池化层，对区域大小和尺度更健壮，允许分类层重用多个图片分辨率生成的feature map。FRCN扩展了SPPnet使其能端到端地通过最小化一个信心和限位框回归合并loss来调优所有层，该方法是被MultiBox提出的。</p><p>第二部分尝试是使用更深的神经网络提升产生候选的质量。在近期研究如MultiBox$^{[7,8]}$，基于低级图片特征的SS被一个独立的深度网络替代。这进一步提升了准确率，但导致了更复杂的设置，需要训练两个网络同时他们之间还有依赖。Faster R-CNN用Region Proposal network RPN替代了SS方法，并提出了通过修改共享卷积层和预测层来将RPN集成进Faster R-CNN的方法。这一候选区域方法用于池化中级特征，最终分类步骤不那么耗时。我们的SSD非常类似Faster R-CNN中的RPN，我们也使用固定集的框来预测，类似RPN中的锚点框。但与其使用池化的特征，再用另一个分类器来评估，我们同时为每个框预测逐类得分。因此我们的方法避免了将RPN合入FRCN的复杂，更容易训练，更快，也更容易集成进其它任务中。</p><p>另一系列方法，与我们的方法直接相关，跳过了候选步骤，直接预测限位框及其逐类信心。OverFeat，一个滑动窗口的深度版本，在知道物体类别信心后，直接从最顶级feature map的每个位置预测一个限位框。YOLO使用最顶级的feature map来预测限位框和多类信心。我们的SSD方法在这类识别，因为我们没有proposal步骤，只有默认框。但我们的方法比现有的要灵活。如果我们只在最顶级feature map的每个位置使用一个默认框，我们的SSD方法就和OverFeat架构类似。如果我们使用整个最顶级feature map并增加一个全连接层来预测而不是用我们的卷积预测器，并不显式考虑多个长宽比，我们能近似地得到YOLO。</p><h2 id="5-Conclusions"><a href="#5-Conclusions" class="headerlink" title="5 Conclusions"></a>5 Conclusions</h2><p>本论文提出了SSD，一个多类快速单步物体检测器。我们模型的关键特征是使用附着于网络顶的多个feature map的多尺度卷积限位框输出。这一表达允许我们有效地为框可能的形状建模。我们实验性地验证了合适的训练策略，大量精心挑选的默认框能提升性能。我们的SSD方法比现有方法限位框预测位置、尺度和长宽比多至少一个数量级。</p><p>我们证明了使用同样的VGG-16架构，SSD比前沿的检测器速度和精度都高。我们的SSD500模型更是超越了Faster R-CNN的精度，同时还快上3倍。我们的实时SSD300模型fps为58，比当前的实时模型YOLO要快，同时检测精度更高。</p><p><img src="/archives/./1533607827717.png" alt="图5"></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Uijlings, J.R., van de Sande, K.E., Gevers, T., Smeulders, A.W.: Selective search for object<br>recognition. IJCV (2013)</li><li>Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object detection<br>with region proposal networks. In: NIPS. (2015)</li><li>He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR.<br>(2016)</li><li>Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., LeCun, Y.: Overfeat: Integrated<br>recognition, localization and detection using convolutional networks. In: ICLR. (2014)</li><li>Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Unified, real-time<br>object detection. In: CVPR. (2016)</li><li>Girshick, R.: Fast R-CNN. In: ICCV. (2015)</li><li>Erhan, D., Szegedy, C., Toshev, A., Anguelov, D.: Scalable object detection using deep<br>neural networks. In: CVPR. (2014)</li><li>Szegedy, C., Reed, S., Erhan, D., Anguelov, D.: Scalable, high-quality object detection.<br>arXiv preprint arXiv:1412.1441 v3 (2015)</li><li>He, K., Zhang, X., Ren, S., Sun, J.: Spatial pyramid pooling in deep convolutional networks<br>for visual recognition. In: ECCV. (2014)</li><li>Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation.<br>In: CVPR. (2015)</li><li>Hariharan, B., Arbelaez, P., Girshick, R., Malik, J.: Hypercolumns for object segmentation ´<br>and fine-grained localization. In: CVPR. (2015)</li><li>Liu, W., Rabinovich, A., Berg, A.C.: ParseNet: Looking wider to see better. In: ILCR. (2016)</li><li>Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.: Object detectors emerge in deep<br>scene cnns. In: ICLR. (2015)</li><li>Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition.<br>In: NIPS. (2015)</li><li>Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy,<br>A., Khosla, A., Bernstein, M., Berg, A.C., Li, F.F.: Imagenet large scale visual recognition<br>challenge. IJCV (2015)</li><li>Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Semantic image segmentation<br>with deep convolutional nets and fully connected crfs. In: ICLR. (2015)</li><li>Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S.,<br>Darrell, T.: Caffe: Convolutional architecture for fast feature embedding. In: MM, ACM<br>(2014)</li><li>Glorot, X., Bengio, Y.: Understanding the difficulty of training deep feedforward neural<br>networks. In: AISTATS. (2010)</li><li>Hoiem, D., Chodpathumwan, Y., Dai, Q.: Diagnosing error in object detectors. In: ECCV</li><li>(2012)</li><li>Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object<br>detection and semantic segmentation. In: CVPR. (2014)</li><li>Bell, S., Zitnick, C.L., Bala, K., Girshick, R.: Inside-outside net: Detecting objects in context<br>with skip pooling and recurrent neural networks. In: CVPR. (2016)</li><li>Felzenszwalb, P., McAllester, D., Ramanan, D.: A discriminatively trained, multiscale, deformable<br>part model. In: CVPR. (2008)</li></ol></div><div class="popular-posts-header">Related Posts</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/423509a9.html" rel="bookmark">YOLOv2 and YOLO9000</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/9bab3bc6.html" rel="bookmark">Fast R-CNN</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/2c51a51a.html" rel="bookmark">YOLOv3</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/562809c1.html" rel="bookmark">YOLO</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/fc798de3.html" rel="bookmark">Faster R-CNN</a></div></li></ul><div><ul class="post-copyright"><li class="post-copyright-author"><strong>Post author: </strong>慕湮</li><li class="post-copyright-link"><strong>Post link: </strong><a href="http://muyaan.com/archives/719cc2e5.html" title="SSD">http://muyaan.com/archives/719cc2e5.html</a></li><li class="post-copyright-license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noopener noreferrer" target="_blank">CC BY-NC-SA 4.0</a> unless stating additionally.</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Computer-Vision/" rel="tag"><i class="fa fa-tag"></i> Computer Vision</a> <a href="/tags/Deep-Learning/" rel="tag"><i class="fa fa-tag"></i> Deep Learning</a> <a href="/tags/Paper/" rel="tag"><i class="fa fa-tag"></i> Paper</a> <a href="/tags/Object-Detection/" rel="tag"><i class="fa fa-tag"></i> Object Detection</a> <a href="/tags/SSD/" rel="tag"><i class="fa fa-tag"></i> SSD</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/archives/8504aa6c.html" rel="next" title="RetinaNet"><i class="fa fa-chevron-left"></i> RetinaNet</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/archives/b2e32499.html" rel="prev" title="VGGNet">VGGNet <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">Table of Contents</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">Overview</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4" alt="慕湮"><p class="site-author-name" itemprop="name">慕湮</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">21</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">30</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/tycallen" target="_blank" title="GitHub" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-github"></i>GitHub</a></span><br><span class="links-of-author-item"><a href="mailto:tyc.allen@gmail.com" target="_blank" title="E-Mail" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span><br><span class="links-of-author-item"><a href="http://weibo.com/pojunallen" target="_blank" title="微博" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-weibo"></i>微博</a></span><br><span class="links-of-author-item"><a href="https://tuchong.com/1070837" target="_blank" title="图虫" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-globe"></i>图虫</a></span><br></div><div class="links-of-blogroll motion-element links-of-blogroll-block"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> 友情链接</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="https://dotrabbit.github.io/" title="dotrabbit" target="_blank" rel="external nofollow noopener noreferrer">dotrabbit</a></li></ul></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Introduction"><span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-The-Single-Shot-Detector-SSD"><span class="nav-text">2. The Single Shot Detector(SSD)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-Model"><span class="nav-text">2.1 Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Training"><span class="nav-text">2.2 Training</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Experimental-Results"><span class="nav-text">3 Experimental Results</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-PASCAL-VOC-2007"><span class="nav-text">3.1 PASCAL VOC 2007</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-Model-analysis"><span class="nav-text">3.2 Model analysis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-PASCAL-VOC2012"><span class="nav-text">3.3 PASCAL VOC2012</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-MS-COCO"><span class="nav-text">3.4 MS COCO</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-Preliminary-ILSVRC-results"><span class="nav-text">3.5 Preliminary ILSVRC results</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-6-Inference-time"><span class="nav-text">3.6 Inference time</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Related-Work"><span class="nav-text">4. Related Work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Conclusions"><span class="nav-text">5 Conclusions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reference"><span class="nav-text">Reference</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; 2015 &mdash; <span itemprop="copyrightYear">2018</span> <span class="with-love" id="animate"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">慕湮</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="Symbols count total">312k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="Reading time total">9:27</span></div><div class="powered-by">Powered by <a class="theme-link" target="_blank" rel="external nofollow noopener noreferrer" href="https://hexo.io">Hexo</a></div><span class="post-meta-divider">|</span><div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" rel="external nofollow noopener noreferrer" href="https://theme-next.org">NexT.Muse</a></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="//cdn.staticfile.org/jquery/2.1.3/jquery.min.js"></script><script type="text/javascript" src="//cdn.staticfile.org/velocity/1.2.1/velocity.min.js"></script><script type="text/javascript" src="//cdn.staticfile.org/velocity/1.2.1/velocity.ui.min.js"></script><script type="text/javascript" src="/lib/reading_progress/reading_progress.js"></script><script type="text/javascript" src="/js/src/utils.js?v=6.3.0"></script><script type="text/javascript" src="/js/src/motion.js?v=6.3.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=6.3.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=6.3.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=6.3.0"></script><script src="https://cdn.staticfile.org/pace/1.0.2/pace.min.js"></script><link href="https://cdn.staticfile.org/pace/1.0.2/themes/blue/pace-theme-big-counter.min.css" rel="stylesheet"><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script><script>AV.initialize("UWGXBMGbguCDhSCbC3NrUQtL-gzGzoHsz","ke1jrA5b6VyR89Kqqqwf2kPP")</script><script>function showTime(e){var t=new AV.Query(e),c=[],u=$(".leancloud_visitors");u.each(function(){c.push($(this).attr("id").trim())}),t.containedIn("url",c),t.find().done(function(e){var t=".leancloud-visitors-count";if(0!==e.length){for(var n=0;n<e.length;n++){var o=e[n],i=o.get("url"),s=o.get("time"),r=document.getElementById(i);$(r).find(t).text(s)}for(n=0;n<c.length;n++){i=c[n],r=document.getElementById(i);var l=$(r).find(t);""==l.text()&&l.text(0)}}else u.find(t).text(0)}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(i){var e=$(".leancloud_visitors"),s=e.attr("id").trim(),r=e.attr("data-flag-title").trim(),t=new AV.Query(i);t.equalTo("url",s),t.find({success:function(e){if(0<e.length){var t=e[0];t.fetchWhenSave(!0),t.increment("time"),$(document.getElementById(s)).find(".leancloud-visitors-count").text(t.get("time")),t.save(null,{success:function(e){},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var n=new i,o=new AV.ACL;o.setPublicReadAccess(!0),o.setPublicWriteAccess(!0),n.setACL(o),n.set("title",r),n.set("url",s),n.set("time",1),n.save(null,{success:function(e){$(document.getElementById(s)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):1<$(".post-title-link").length&&showTime(e)})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script type="text/javascript" color="99,184,255" opacity="0.5" zindex="-2" count="99" src="//cdn.staticfile.org/canvas-nest.js/1.0.0/canvas-nest.js"></script></body></html>