<!DOCTYPE html><html class="theme-next muse use-motion" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="//cdn.staticfile.org/font-awesome/4.6.2/css/font-awesome.min.css" rel="stylesheet" type="text/css"><link href="/css/main.css?v=6.3.0" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.3.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.3.0"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.3.0"><link rel="mask-icon" href="/images/logo.svg?v=6.3.0" color="#222"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Muse",version:"6.3.0",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!1,fastclick:!1,lazyload:!1,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="description" content="RetinaNet论文翻译，业界常用的单步物体检测模型。其创新点是提出了focal loss函数，用于解决前景背景样本数量失衡的问题。Retina Net : Focal Loss for Dense Object DetectionTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollar 2017Facebook AI"><meta name="keywords" content="Computer Vision,Deep Learning,Paper,Object Detection,RetinaNet"><meta property="og:type" content="article"><meta property="og:title" content="RetinaNet"><meta property="og:url" content="http://muyaan.com/archives/8504aa6c.html"><meta property="og:site_name" content="慕湮"><meta property="og:description" content="RetinaNet论文翻译，业界常用的单步物体检测模型。其创新点是提出了focal loss函数，用于解决前景背景样本数量失衡的问题。Retina Net : Focal Loss for Dense Object DetectionTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollar 2017Facebook AI"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://muyaan.com/archives/1533289842501.png"><meta property="og:image" content="http://muyaan.com/archives/1533290094445.png"><meta property="og:image" content="http://muyaan.com/archives/1533391367480.png"><meta property="og:image" content="http://muyaan.com/archives/1533398942105.png"><meta property="og:image" content="http://muyaan.com/archives/1533400333105.png"><meta property="og:image" content="http://muyaan.com/archives/1533439202787.png"><meta property="og:image" content="http://muyaan.com/archives/1533453175507.png"><meta property="og:image" content="http://muyaan.com/archives/1533453187778.png"><meta property="og:image" content="http://muyaan.com/archives/1533453223131.png"><meta property="og:image" content="http://muyaan.com/archives/1533453206877.png"><meta property="og:updated_time" content="2018-09-12T03:22:25.031Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="RetinaNet"><meta name="twitter:description" content="RetinaNet论文翻译，业界常用的单步物体检测模型。其创新点是提出了focal loss函数，用于解决前景背景样本数量失衡的问题。Retina Net : Focal Loss for Dense Object DetectionTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollar 2017Facebook AI"><meta name="twitter:image" content="http://muyaan.com/archives/1533289842501.png"><link rel="canonical" href="http://muyaan.com/archives/8504aa6c.html"><script type="text/javascript" id="page.configurations">CONFIG.page={sidebar:""}</script><meta name="google-site-verification" content="Ojd_HrL_PelaXvKK5IkbhLbjZ_sHt6IxRzP-XPaaTw4"><meta name="baidu-site-verification" content="skdT7mIEAb"><title>RetinaNet | 慕湮</title><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?ca5844321cfb80fdf6f12b4dcc326991";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style type="text/css">.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.logo-line-after i{right:initial}</style></noscript></head><body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">慕湮</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">白日放歌须纵酒</h1></div><div class="site-nav-toggle"><button aria-label="Toggle navigation bar"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-archives menu-item-active"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><div class="reading-progress-bar"></div><article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://muyaan.com/archives/8504aa6c.html"><span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="慕湮"><meta itemprop="description" content=""><meta itemprop="image" content="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4"></span><span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="慕湮"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">RetinaNet</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2018-08-06 09:54:08" itemprop="dateCreated datePublished" datetime="2018-08-06T09:54:08+08:00">2018-08-06</time> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">Edited on</span> <time title="Modified: 2018-09-12 11:22:25" itemprop="dateModified" datetime="2018-09-12T11:22:25+08:00">2018-09-12</time> </span><span id="/archives/8504aa6c.html" class="leancloud_visitors" data-flag-title="RetinaNet"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Views: </span><span class="leancloud-visitors-count"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数: </span><span title="Symbols count in article">19k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="Reading time">34 mins.</span></div></div></header><div class="post-body" itemprop="articleBody"><p>RetinaNet论文翻译，业界常用的单步物体检测模型。其创新点是提出了focal loss函数，用于解决前景背景样本数量失衡的问题。</p><blockquote><h1 id="Retina-Net-Focal-Loss-for-Dense-Object-Detection"><a href="#Retina-Net-Focal-Loss-for-Dense-Object-Detection" class="headerlink" title="Retina Net : Focal Loss for Dense Object Detection"></a><a href="https://arxiv.org/abs/1708.02002" rel="external nofollow noopener noreferrer" target="_blank">Retina Net : Focal Loss for Dense Object Detection</a></h1><p>Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollar 2017<br>Facebook AI Research (FAIR)</p></blockquote><a id="more"></a><hr><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>现在前沿的物体检测系统都基于两步、候选区域驱动的机制。如流行的R-CNN框架$^{[11]}$，第一步生成一个候选物体位置的稀疏集，第二步用一个卷积网络将每个候选位置分类为前景类之一或背景。经过一系列的进化$^{[10,28,20,14]}$，这一两步框架持续的刷新着COCO benchmark$^{[21]}$的最高精度。</p><p>尽管两步检测器非常成功，还是需要问的一个自然问题是：一个简单的单步检测器能否获得相似的准确度吗？一个单步检测器被应用到一个物体位置、尺度和长宽比的有规律的密集抽样集上。近期的单步检测器的研究，如YOLO$^{[26,27]}$和SSD$^{[22,9]}$，证明了结果是有希望的，不过它们的准确度对比前沿两步方法相对要低10~40%。</p><p>本论文更进一步：我们提出了一个单步检测器，首次，有着与前沿两步检测器匹配的COCO AP，如Feature Pyramid Network（FPN）$^{[20]}$或Mask R-CNN$^{[14]}$，一个Faster R-CNN$^{[28]}$的变种。为了达到这一结果，我们把训练中的类不平衡当做妨碍单步检测器获得前沿准确度的主要障碍，并提出了用于消除这一障碍的新loss函数。</p><p>在类似R-CNN的检测器中，类不平衡通过两步和启发式采样解决了。候选步骤（如SS$^{[35]}$，EdgeBoxes$^{[39]}$，DeepMask$^{[24,25]}$，RPN$^{[28]}$）反复降低候选位置数量直至小的值（如1-2k），过滤掉大部分背景样本。在第二步分类时，启发式采样，如固定的前景背景比（1：3），或online hard example mining(OHEM)$^{[31]}$，用于维持前景和背景间的平衡可控。</p><p>与其想法，单步检测器需要处理多得多的从一个图片中规律采样的候选物体位置集。在实际中这常常是近100k个位置，密集地覆盖了空间位置、尺度和长宽比。尽管也能使用类似的启发式采样，但因为训练过程始终被容易分类的背景样本统治，是没有效果的。这一无效性是物体检测的经典问题，通常通过一些技术如boostrapping$^{[33,29]}$或HEM$^{[37,8,31]}$解决。</p><p>本论文中，我们提出了一个新的loss函数，它相比之前处理类失衡的办法更有效。该loss是一个动态尺度交叉熵（dynamically scaled cross entropy）loss，其尺度因子随着对正确类的自信增长而衰减至0，见图1。凭直觉讲，这一尺度因子能自动的为训练中的容易样本的贡献降低权重，并迅速将模型焦点放到困难样本上。实验显示我们提出的<em>Focal Loss</em>允许我们训练高精度、单步检测器，显著地超过了使用启发式采样或HEM的系统。最后，我们注意到焦点loss的形式不是关键，而且我们用其它例子证明也能获得相似结果。</p><p><img src="/archives/./1533289842501.png" alt="图1"></p><p>为了证明我们提出的焦点loss的有效性，我们设计了一个简单的单步检测器，叫RetinaNet，因其对输入图片可能的物体位置密集采样而得名。其设计特色是高效的网络内特征金字塔和使用锚点框。它从[22,6,28,20]中吸取了多种近期概念。RetinaNet高效而准确，我们的最好的模型，基于ResNet-101-FPN，在COCO test-dev上的AP为39.1%，同时有5fps的速度，超越了之前发表的包括单、双步算法的所有单模型结果，见图2。</p><p><img src="/archives/./1533290094445.png" alt="图2"></p><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p><strong>Classic Object Detectors:</strong> 滑动窗口，也就是把分类器应用到密集图片格上的方法，有着悠久而丰富的历史。最早的成功案例之一是Lecun等人将卷积神经网络应用于手写数字识别的经典工作（LeNet）$^{[19,36]}$。Viola和Jones$^{[37]}$使用boosted object detector进行人脸检测，引领了这一类模型的广泛使用。HOG$^{[4]}$的提出和integral channel features$^{[5]}$引出了行人检测的有效方法。DPMs$^{[8]}$帮助密集检测器扩展到更泛化的物体领域，并在PASCAL上领先多年。尽管滑动窗口方法是经典计算机视觉里的领导检测方法，随着深度学习的复苏$^{[18]}$，下文描述的两步检测器迅速统治了物体检测领域。</p><p><strong>Two-stage Detectors:</strong> 现代物体检测的通知级方法是基于两步的。按照先驱SS的方法，第一步生成一个稀疏候选集，应该包含了所有物体同时又过滤了大部分负样本位置。第二步是把候选区域前景类/背景中。R-CNN$^{[11]}$把第二步的分类器升级为一个卷积网络，大幅提升了准确率。多年来R-CNN的速度不断被提升。Region Proposal Network RPN把生成候选区域与第二步的分类集成为单个卷积网络，形成了Faster R-CNN框架$^{[28]}$。很多这一框架的扩展都被提出$^{[20,31,32,16,14]}$。</p><p><strong>One-stage Detectors:</strong> OverFeat$^{[30]}$是现代首个基于深度网络的单步物体检测器之一。更近年的SSD$^{22,9}$和YOLO$^{[26,27]}$再次激起了对单步方法的兴趣。这些检测器为速度而调优但其准确度落后两步方法。SSD的AP低约10-20%，而YOLO专注于更极致的速度/精度权衡。见图2。近期研究发现两步检测器可以简单地降低输入图片的分辨率和候选数量来加速，但单步方法就算有更多计算力预算，精度仍较低$^{[17]}$。本论文的目标是弄懂是否单步检测器能达到甚至超越两步检测器的精度，同时有着相似甚至更快的速度。</p><p>RetinaNet检测器的设计与许多之前的密集检测器有相似点，特别是RPN提出的锚点概念$^{[28]}$和SSD$^{[22]}$与FPN$^{[20]}$使用的特征金字塔。需要强调的一点是，我们的简单模型取得最高结果的原因不是网络设计的革新，而是我们提出的最新loss函数。</p><p><strong>Class Imbalance: </strong>所有单步物体检测方法，经典的如boosted detector和DPM，现代的如SSD，都面临训练时类失衡的问题。这些检测器为每个图片评估出$10^4-10^5$个候选位置，但包含的物体仅仅是少数。这一失衡导致了两个问题：（1）由于大部分位置都是容易的负样本，它们不能贡献有用的学习信号，训练非常低效；（2）大量简单负样本可以压倒训练并导致模型退化。一个常见的解决方案是如hard negative mining$^{[33,37,8,31,22]}$的形式，在训练中采样难的负样本，或更复杂的采样/重加权的方法。与其相反，我们证明了我们提出的focal loss自然地处理了单步检测器面对的类失衡问题，允许我们高效地训练：训练所有样本而不需要采样，不用担心简单负样本压倒loss和梯度。</p><p><strong>Robust Estimation:</strong> 学界曾经对于设计通过对有着更大误差的样本（hard样本）的降权来减少outliers异常值的贡献的robust loss函数（如Huber Loss$^{[13]}$）感兴趣。与之相反，我们的focal loss设计于通过对inliers(简单样本)的降权，尽管其数量大，但使其对总loss贡献小来解决类失衡的问题。换句话说，focal loss 扮演了与robust loss 相反的角色：它专注于在困难样本的稀疏集上训练。</p><h2 id="3-Focal-Loss"><a href="#3-Focal-Loss" class="headerlink" title="3. Focal Loss"></a>3. Focal Loss</h2><p>Focal loss被设计以解决单步物体检测器面对的训练时前景背景类的极度失衡问题（如 1:1000）。我们从二分类的交叉熵(CE)loss开始介绍：</p><script type="math/tex;mode=display">CE (p,y) = \begin{cases}
 -\log(p) & \mbox{if } \  \mbox{y =1 }
 \\-\log(1-p) & otherwise.
 \end{cases} \qquad (1)</script><p>上方的公式中$y in \{ \pm1 \}$指gt类而$p \in [0,1]$是模型为标签为y = 1的类估计的概率。为了简便，我们定义$p_t$为：</p><script type="math/tex;mode=display">p_t = \begin{cases}
p & if \  y = 1 \\
1 - p & otherwise,
\end{cases} \qquad (2)</script><p>并将CE重写为$CE(p,y) = CE(p_t) = -\log (p_t)$</p><p>CE loss可以看做图1中的蓝色曲线。这一loss的重要特点是，从曲线很容易能看出，就算样本是很容易分类的$(p_t \gg .5 )$，其loss也有一个不可忽视的大小。当把大量简单样本相加，这些小的loss值能压倒稀有的类。</p><h3 id="3-1-Balanced-Cross-Entropy"><a href="#3-1-Balanced-Cross-Entropy" class="headerlink" title="3.1 Balanced Cross Entropy"></a>3.1 Balanced Cross Entropy</h3><p>解决类失衡的常见方法是引入权重因子$\alpha \in [0,1]$，为类1。$1 - \alpha$为-1类。在实际中$\alpha$可能被设置为类频率的逆，或当做通过交叉验证设置的超参数。为了简便，我们按定义$p_t$那样定义$\alpha_t$：</p><script type="math/tex;mode=display">CE(p_t) = -\alpha_t \log(p_t) \qquad (3)</script><p>这个loss只是对CE的简单扩展，我们将其当做实验性的基线。</p><h3 id="3-2-Focal-Loss-Definition"><a href="#3-2-Focal-Loss-Definition" class="headerlink" title="3.2 Focal Loss Definition"></a>3.2 Focal Loss Definition</h3><p>如我们的实验将显示的那样，密集检测器训练时遭遇的类失衡压过了交叉熵loss。容易分类的负样本占据了loss的大部分，统治了梯度。尽管$\alpha$平衡了正负样本的重要性，但并没有区分easy/hard样本。我们提出了将loss函数重构为为容易样本降权，专注于hard negative训练。</p><p>更正式的，我们提出为交叉熵loss增加一个调制因子modulating factor $(1 - p_t)^{\gamma}$，带有可调优的<em>focusing</em>参数$\gamma \ge 0$，我们定义Focal loss 为：</p><script type="math/tex;mode=display">FL(p_t) = -(1-p_t)^{\gamma} \log (p_t) \qquad (4)</script><p>focal loss 可被看做图1中多个$\gamma \in [0,5]$的值。我们注意到focal loss 的两个特点</p><ol><li>当一个样本分类错误且$p_t$较小时，调制因子接近1，从而不影响loss的值。随着$p_t \rightarrow 1$，因子趋近于0，这样很容易分类的样本就被降权。</li><li>focusing参数$\gamma$平滑地调整了容易样本被降权的速率。当$\gamma = 0$，FL = CE，随着$\gamma$增长，调制因子的作用也开始增长（实验发现$\gamma = 2$效果最好）。</li></ol><p>从直觉上讲，调制因子降低了容易样本对loss的贡献，并扩展了收到低loss的样本范围。例如，当$\gamma = 2$时，一个分类为$p_t = 0.9$的样本会比CE的loss低100倍，而当$p_t \approx 0.968$时，会低1000倍。反过来这也增加了纠正错分类样本的重要性（当$p_t \le .5$时loss小了近4倍）。</p><p>在实际运用中我们会使用focal loss的一个$\alpha-balanced$变体：</p><script type="math/tex;mode=display">FL(p_t) = -\alpha_t(1-p_t)^{\gamma} \log (p_t) \qquad (4)</script><p>我们在实验中采用了这一形式，相比$non-\alpha-balanced$形式它略微提升了准确度。最后，我们注意到把loss层和计算loss中的p的sigmoid操作结合能有更好的数值稳定性。</p><p>尽管我们的主要实验使用了上面的focal loss，但其精确形式并不关键。在附录中我们尝试了其它focal loss变体，证明了它们同样有效。</p><h3 id="3-3-Class-Imbalance-and-Model-Initialization"><a href="#3-3-Class-Imbalance-and-Model-Initialization" class="headerlink" title="3.3 Class Imbalance and Model Initialization"></a>3.3 Class Imbalance and Model Initialization</h3><p>二分类模型默认初始化为输出相等的概率y = -1或1。在这样的初始化下，这个loss函数会被高频类统治，导致训练初期的不稳定性。为了解决这一问题，我们引入了“优先”概念，在训练初期，模型为稀少的类（前景）的p估计一个优先值。我们将其记做$\pi$，将其设置为让模型预测的稀少类概率低，如0.01。需要说明这是模型初始化的改变，而不是loss函数。我们发现在类极失衡时，这能提升训练稳定性，不管是CE还是FL loss。</p><h3 id="3-4-Class-Imbalance-and-Two-Stage-Detectors"><a href="#3-4-Class-Imbalance-and-Two-Stage-Detectors" class="headerlink" title="3.4 Class Imbalance and Two-Stage Detectors"></a>3.4 Class Imbalance and Two-Stage Detectors</h3><p>两步检测器经常使用CE loss训练而不使用$\alpha-balancing$或我们提出的loss。它们通常使用两个机制解决类失衡：（1）两步层叠和（2）有偏向的mini-batch采样。第一步是一个候选区域机制$^{[35,24,28]}$将近乎无限的可能物体位置集降低到1~2k个。更重要的是，物体位置不是随机采样而是极有可能是真实物体位置，这也移除了大部分容易负样本。在第二步进行训练时，常常采用偏见采样来构建mini-batch，如1：3的正负样本比例。这个比值有点隐式的$\alpha-balance$因子的感觉。我们提出的focal loss是为单步检测系统来解决类失衡的问题。</p><h2 id="4-RetinaNet-Detector"><a href="#4-RetinaNet-Detector" class="headerlink" title="4. RetinaNet Detector"></a>4. RetinaNet Detector</h2><p>RetinaNet是单个、统一的网络，它由一个backbone主干网和两个任务相关的子网组成。主干网负责对输入图片计算卷积Feature map，是一个非定制的（off-the-self）卷积网络。第一个子网利用主干网的输出进行物体分类；第二个子网进行卷积限位框回归。我们特地为单步、密集检测提出了两个子网这一简单、有特色的设计，见图3。尽管这些组件的细节设计有很多可能的选择，实验会显示大部分设计参数对具体值并不特别敏感。接下来我们将描述RetinaNet中的每个组件</p><p><img src="/archives/./1533391367480.png" alt="图3"></p><p><strong>Feature Pyramid Network Backbone:</strong> 我们采用[20]中的特征金字塔网络FPN作为RetinaNet的骨干网。简单地说，FPN用一个自顶向下的通路和侧（lateral）连接增广一个标准卷积网络，让网络能高效地从单分辨率输入图片构建一个富的、多尺度的特征金字塔，见图3 (a)-(b)。金字塔每一级都能用于在不同尺度检测物体。相比FCN$^{[23]}$，FPN提升了多尺度预测能力。</p><p>我们在RetinaNet架构顶部建立了FPN。我们构建了从$P_3$到$P_7$级的金字塔，其中l指金字塔等级（$P_l$比输入图片分辨率低$2^l$）。如[20]中一样，所有金字塔等级都有$C \equiv 256$个通道。细节与[20]大体一致，只有适度修改。尽管许多设计选择不关键，但需要强调FPN主干网的使用是关键的。预实验中只使用RetinaNet层的特征得到的AP较低。</p><p><strong>Anchors:</strong> 我们使用的平移不变性锚点框类似[20]中的那些RPN变体。在金字塔等级$P_3 $到$ P_7$，锚点框面积从$32^2$到$512^2$。每个金字塔等级我们都使用三种长宽比{1:2,1:1,2:1}。为了覆盖更密集的尺度，我们在每一级原生的三个长宽比锚点集上增加了$\{ 2^0,2^{1/3},2^{2/3}\}$大小的锚点。这提升了AP。每个等级共$A = 9$个锚点，所有等级加起来覆盖了32-813个像素的尺度范围。</p><p>每个锚点框都赋予一个长度为K的分类目标独热编码向量，K是物体类数量，和一个长度为4的限位框回归目标向量。我们使用了RPN$^{[28]}$的赋值规则但为多类检测进行了修改并调整了阈值。赋值为gt物体框的IoU阈值为0.5，背景的IoU为[0,0.4)。因为每个锚点最多分配一个物体框，我们将对应的独热编码设为1，其余为0。如果锚点框未分配，如IoU在[0.4,0.5)，它将在训练中被忽略。限位框回归计算锚点框和分配的物体框的偏移，如未分配则忽略。</p><p><strong>Classification Subnet: </strong>分类子网K的每个类预测物体出现在A的每个锚点的概率。这个子网是附着于每个FPN等级的小FCN；这个子网在各级的参数是共享的。其设计很简单。对于给定金字塔等级，接收一个有C个通道的Feature map作为输入，子网进行4个 3 * 3的卷积层，每个都有C个filter且后跟一个ReLU激活，接着是一个3*3的有着KA个filter的卷积层。接着一个sigmoid激活函数，为每个空间位置输出KA二分预测，见图3(c)。在大部分实验中我们使用C = 256，A = 9。</p><p>与RPN$^{[28]}$相反，我们的物体分类子网更深，只用了3*3卷积，且没有与限位框回归子网共享参数。我们发现更抽象的设计决策比超参数的具体值更重要。</p><p><strong>Box Regression Subnet:</strong> 与物体分类子网并行，我们为每个金字塔等级添加了另一个小FCN来为每个锚点框与其临近的gt对象偏移回归。对于每个空间位置中A个锚点，为每个生成的4个输出预测了锚点框和gt框的相对偏移（使用了R-CNN的标准限位框参数化）。我们注意到一点，与近期的研究不同的是，我们使用了一个有着更少参数的类无关限位框回归器，但同样有效。尽管两个子网结构相同，但参数无关。</p><h3 id="4-1-Inference-and-Training"><a href="#4-1-Inference-and-Training" class="headerlink" title="4.1 Inference and Training"></a>4.1 Inference and Training</h3><p><strong>Inference:</strong> RetinaNet 由一个ResNet-FPN主干网，一个分类子网和一个限位框回归子网构成，见图3。所以推理只需简单的将一张图片使用网络前向计算即可。为了提升预测速度，我们只处理每层的1k平方最高的限位框预测，after thresholding detector confidence at 0.05。各层的最佳预测进行合并，外加一个阈值为0.5的非极大值抑制，就得到了最终结果。</p><p><strong>Focal Loss:</strong> 我们使用本论文提出的focal loss作为分类子网输出的loss。如我们会在章节5中展示，我们发现实践中$\gamma = 2$表现不错，而RetinaNet在$\gamma \in [0.5,5]$都相对健壮。需要强调的是训练RetinaNet时，focal loss为每个样本图片的所有~100k锚点都应用。这与常见实践如使用启发式采样（RPN）和hard example mining（OHEM，SSD）相反，它们为每个mini-batch选出一个小的锚点框集合（如256）。一个图片的总focal loss被计算为所有~100k锚点的focal loss的和，用被分配gt框的锚点数量归一化。选择用有gt框的数量而不是所有锚点框数量归一化，因为大部分锚点框都是简单负样本，在focal loss下贡献的loss微不足道。最后我们注意到$\alpha$，赋予稀有类的权重，同样有一个很大的稳定范围，但由于存在与$\gamma$的相互作用，很有必要同时选择这俩的值（见表1a和1b）。通常来说随着$\gamma$增长$\alpha$需要降低（$\gamma =2$时，$\alpha = 0.25$最佳）。</p><p><strong>Initialization:</strong> 我们用了ResNet-50-FPN和Res-101-FPN主干网$^{[20]}$。所用的两个模型都在ImageNet1k上预训练；我们使用[16]公开的模型。为FPN加入的新层像[20]一样初始化。除了子网最后一层之外，RetinaNet的所有卷积层都用一个$\sigma = 0.01$的高斯分布初始化权重，bias为0。对于分类子网的最后一个卷积层，我们把bias初始化为$b = -\log ((1-\pi)/\pi)$，$\pi$指在训练开始时，每个锚点框都被标记为有~$\pi$的confidence是前景。我们在所有实验中使用$\pi = .01$，不过结果对于具体值是健壮的。如章节3.3中解释过的，这一初始化防止了大量背景锚点在训练初期产生一个大的不稳定loss值。</p><p><strong>Optimization:</strong> RetinaNet使用SGD训练。我们使用同步SGD在8个GPU上，每个mini-batch共16张图片（每个GPU两张）。除非特别说明，所有的模型都用0.01的学习率训练前90k，接下来60k除以10，最后80k再除以10。我们使用水平翻转作为唯一的数据增广手段。decay为0.0001，momentum为0.9。训练loss是focal loss的和，限位框回归使用了标准平滑$L_1$loss$^{[10]}$。表格1e中的模型训练时间范围为10~35个小时。</p><p><img src="/archives/./1533398942105.png" alt="表1"></p><h2 id="5-Experiments"><a href="#5-Experiments" class="headerlink" title="5. Experiments"></a>5. Experiments</h2><p>我们呈现了在COCO上的结果。训练时，我们按照常见方法使用COCO trainval135k划分（80k来自训练集和来自40k val的随机35k子集）。我们通过在minival 划分（val中剩余5k）测试进行了损害和敏感性测试。我们将test-dev作为报告主要结果，该集没有公开标签需要提交服务器评估。</p><h3 id="5-1-Training-Dense-Detection"><a href="#5-1-Training-Dense-Detection" class="headerlink" title="5.1 Training Dense Detection"></a>5.1 Training Dense Detection</h3><p>我们进行了大量实验来分析用于密集检测的loss函数在许多优化策略下的表现。所有实验我们都使用添加了一个FPN的50层或101层的ResNet。所有消融实验的训练和测试图片尺度都为600像素。</p><p><strong>Network Initialization:</strong> 我们首先尝试了使用原始初始化和训练策略的标准交叉熵函数。由于网络在训练中偏离，很快就失败了。不过，仅仅将模型最后一层初始化为检测到一个物体的先验概率为$\pi = .01$（见章节4.1）就能有效训练了。用这样设定训练的ResNet-50的RetinaNet就已经在COCO上得到了30.2的AP。结果对于$\pi$的值不敏感，因此所有实验都使用$\pi = .01$。</p><p><strong>Balanced Cross Entropy:</strong> 我们进一步尝试了使用章节3.1中描述的$\alpha-balanced$ CE loss来提升训练。不同取值的$\alpha$的结果见表1a。设置$\alpha = .75$得到了0.9%的AP提升。</p><p><strong>Focal Loss:</strong> 使用我们提出的focal loss的结果在表1b中显示。它引入了新的超参数，focusing参数$\gamma$，控制了调制因子的强度。当$\gamma = 0$，即是CE loss。随着$\gamma$增大，loss的形式使容易样本的loss更小，更易忽略，见图1。随着$\gamma$增大，FL相对CE有大幅提升。当$\gamma =2$，FL相对$\alpha-balanced$ CE loss 提升了2.9%的AP。</p><p>表1b中我们为每个$\gamma$找到了最合适的$\alpha$。我们观察到为较大的$\gamma$挑选的$\alpha$的值较低（因为容易样本已被降权，正样本的强调就不需那么多）。总的来说，调整$\gamma$的提升要大得多，而实际$\alpha$的最佳区间是[.25,.75]（我们测试了$\alpha \in [.01,.999]$）。我们所有实验都采用$\gamma =2.0,\alpha = .25$，但$\alpha = .5$同样不错（AP低0.4%）。</p><p><strong>Analysis of the Focal Loss:</strong> 为了更好地理解focal loss，我们分析了loss对收敛模型的经验分布。我们把默认的ResNet-101 600像素模型用$\gamma = 2$训练（有36%的AP）。我们将模型应用于大量随机图片，并采样约$10^7$负窗口和$10^5$正窗口的预测概率。接下来我们分别为正负样本计算FL，并将其归一化为和为1。有了归一化后的loss，我们能将其从低到高排序，并为正负样本及其不同的$\gamma$设定（尽管模型使用$\gamma = 2$训练）画出其累积分布（cumulative distribution function CDF）。</p><p><img src="/archives/./1533400333105.png" alt="图4"></p><p>CDF见图4。如果我们观察正样本的图像，我们会发现不同的$\gamma$值下CDF看起来非常像。比如，约20%的最难样本占据了近半数正loss，随着$\gamma$增大，更多loss集中到前20%样本中，但效果较小。</p><p>$\gamma$对负样本的作用有着戏剧性地不同。对于$\gamma=0$，正负样本的CDF非常相似。但随着$\gamma$增大，更多权重集中到困难负样本上。实际上，当$\gamma=2$时（我们的默认设定），绝大部分loss来自很小部分的样本。很容易能看出，FL能有效忽略简单负样本的影响，将注意力集中到困难负样本。</p><p><strong>Online Hard Example Mining (OHEM):</strong> [31]提出了通过用high-loss样本构建mini-batch来提升两步检测器的训练。在OHEM中，每个样本都被其loss评分，接着使用非极大值抑制nms，mini-batch就使用最高loss的样本构成。nms阈值和mini-batch size都是可调优的参数。类似focal loss，OHEM将重心放在误分类的样本上，但不同的是，OHEM完全抛弃了容易样本。我们也实现了SSD$^{[22]}$中使用的一个OHEM变体：在对所有样本应用nms后，强制用1:3的正负样本比例来构建mini-batch，来保证每个mini-batch中都有足够的正样本。</p><p>我们在有着更大的类失衡的单步检测器中测试了OHEM的变体。不同batch size和nms阈值下的原版OHEM策略和OHEM 1:3的结果见表1d。结果使用了ResNet-101，我们用FL训练的有着36%的AP的基线。而OHEM的最佳设定（没有1:3比例，batch size 128，nms为.5）的AP为32.8%。3.2%的AP差距，说明FL比OHEM训练密集检测器更有效。我们尝试过OHEM的其它参数设定和变体，但没有得到更好的结果。</p><p><strong>Hinge Loss:</strong> 早期实验中，我们尝试过使用$p_t$的hinge loss训练，在特定的$p_t$值上，loss为0。但是这并不稳定，我们没能获得有意义的结果。附录中有不同loss函数的结果。</p><h3 id="5-2-Model-Architecture-Design"><a href="#5-2-Model-Architecture-Design" class="headerlink" title="5.2 Model Architecture Design"></a>5.2 Model Architecture Design</h3><p><strong>Anchor Density:</strong> 一个单步检测器的最重要的设计因素就是它用多密集的框来覆盖图片空间。两步检测器能通过一个区域池化操作$^{[10]}$来在任意位置、尺度和长宽比对框分类。与其相反，单步检测器使用固定的抽样格，一个获得高覆盖率的方法是在每个空间位置使用多个”锚点”来覆盖不同尺度和长宽比的框。</p><p>我们考虑过每个FPN级和每个空间位置的锚点尺度和长宽比数量。我们考虑过每个位置从单个矩形锚点到12个锚点，有4个尺度（$2^{k/4},for k \le 3$）和3个长宽比[0.5,1,2]。使用ResNet-50的结果见表1c。仅使用一个矩形锚点框就有很好的AP（30.3）。不过当每个位置使用12个锚点框时，AP能提升近4个点。我们在本论文的其它实验都使用这一设定。</p><p>最终，我们注意到再增加6-9个锚点框不能有更多提升。尽管两步检测系统能分类一个图片中的任意数量框，密度的性能饱和意味着更密的两步检测器不能得到提升。</p><p><strong>Speed versus Accuracy:</strong> 更大的骨干网会有更高的准确度，但推理也会更慢。像输入图片尺度（用图片短边定义）一样。我们在表1e中显示了两个因子的影响。在图2中我们画出了RetinaNet的速度/精确度权衡曲线，并将其与近期其余方法在COCO test-dev上进行了比较。图形显示RetinaNet得益于我们的focal loss，尽管使用了低精度的体制，仍对其余现存方法都形成了上包围。使用ResNet-101-FPN和600像素尺度的RetinaNet（简记做RetinaNet-101-600）接近了近期发表的ResNet-101-FPN Faster R-CNN$^{[20]}$的精确度，运行速度也较快（每张图片122ms vs. 172ms ，都使用NVIDIA M40测试）。使用更大的尺度让RetinaNet超越所有两步方法的精确度，同时仍较快。如果需要更快的运行时间，只有一个操作点（500像素输入），使用ResNet-50-FPN。更高的帧率需要特别的网络设计，像[27]那样。我们注意到论文发表后，Faster R-CNN的变体$^{[12]}$的结果更快更精确。</p><h3 id="5-3-Comparison-to-State-of-the-Art"><a href="#5-3-Comparison-to-State-of-the-Art" class="headerlink" title="5.3 Comparison to State of the Art"></a>5.3 Comparison to State of the Art</h3><p>我们把RetinaNet在COCO test-dev上与其余前沿方法（包括单步和两步）进行了比较。表2是我们的RetinaNet-101-800模型的结果，它使用了抖动scale训练，比表1e的模型训练时间长了1.5倍（得到了1.3的AP提升）。比起现存的单步方法，我们的方法领先了最接近的竞争者DSSD近5.9的AP，同时还更快，见表2。与前沿两步方法相比，RetinaNet领先最佳的基于Inception-ResNet-v2-TDM$^{[32]}$的Faster R-CNN2.3的AP。将ResNeXt-32x8d-101-FPN$^{[38]}$作为骨干网再次提升了1.7的AP，在COCO上超过了40。</p><p><img src="/archives/./1533439202787.png" alt="表2"></p><h2 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6. Conclusion"></a>6. Conclusion</h2><p>在本论文中，我们将类失衡确定为单步检测器无法达到两步检测器一样高性能的主要障碍。为了解决这一问题，我们提出了focal loss，为CE loss添加了一个调制因子，将训练重点放在困难负样本上。我们的方法简洁而高效。我们通过设计一个全卷积单步检测器来演示其效果，并报告了广泛的实验证明其获得的前沿速度和准确度。代码已<a href="https://github.com/facebookresearch/Detectron" rel="external nofollow noopener noreferrer" target="_blank">开源</a>。</p><h2 id="Appendix-A-Focal-Loss"><a href="#Appendix-A-Focal-Loss" class="headerlink" title="Appendix A: Focal Loss*"></a>Appendix A: Focal Loss*</h2><p>Focal loss的具体形式并不关键。我们现在用focal loss 进行交替演示，有着类似的属性会产生比得上的结果。接下来的分析也能更加洞察focal loss的属性。</p><p>我们以考虑CE和FL与正文略不同的形式开始。我们定义数量$x_t$如下：</p><script type="math/tex;mode=display">x_t = yx, \qquad (6)</script><p>其中$y \in \{ \pm 1 \}$如之前一样，指gt的类。接下来我们可以写作$p_t = \sigma(x_t)$（这于等式2的$p_t$定义可以兼容）。当$x_t \gt 0$时样本被正确分类，这时$p_t \gt .5$。</p><p>我们现在能用$x_t$定义另一版本的focal loss，定义$p_t^<em>$和$FL^</em>$如下：</p><script type="math/tex;mode=display">p_t^* = \sigma(\gamma x_t + \beta),\qquad \quad (7)</script><script type="math/tex;mode=display">FL^* = -\log (p_t^*)/\gamma. \qquad (8)</script><p>$FL^*$有两个参数，$\gamma$和$\beta$，控制了曲线的陡峭度和位移。图5中我们绘制了两个参数设置的$FL^*$，与CE和FL一起。可以从图中看出$FL,FL^*$消除了赋予分类得很好的样本的loss。</p><p>我们用完全一样的设定训练了RetinaNet-50-600，但我们用设定的参数的$FL^*$替换了FL。这些模型获得了与原版近似的结果，见表3。换句话说，$FL^*$是对FL的合理修改，实际应用中有不错表现。</p><p>我们发现许多$\gamma$和$\beta$设定都有不错的结果。图7中我们展示了使用$FL^*$的一系列参数的RetinaNet-50-600的结果。曲线颜色编码为有效设置（模型收敛且AP大于33.5）的曲线显示为蓝色。出于简洁，我们在所有实验中使用$\alpha = .25$。能看出降低分类得很好的样本loss很有效。</p><p>更泛化地讲，我们认为任何有着与$FL$或$FL^*$相同属性的loss都同样有效。</p><p><img src="/archives/./1533453175507.png" alt="图5"></p><p><img src="/archives/./1533453187778.png" alt="表3"></p><p><img src="/archives/./1533453223131.png" alt="图7"></p><h2 id="Appendix-B-Derivatives"><a href="#Appendix-B-Derivatives" class="headerlink" title="Appendix B: Derivatives"></a>Appendix B: Derivatives</h2><p>$CE,FL$和$FL^*$对x的偏导如下：</p><script type="math/tex;mode=display">\begin{align}
\frac {d CE}  {d x} = y(p_t -1 ) \tag{9}  \\
\frac {d FL} {d x}  = y(1-p_t)^{\gamma} (\gamma p_t \log (p_t) + p_t -1) \tag{10} \\
\frac {d FL^*} {d x} = y (p_t^* -1) \tag{11} \\
\end{align}</script><p>所选择的设定的曲线见图6。所有loss函数偏导对于高信心的预测偏导都趋向于-1或0。不像CE，FL和FL*的有效设定下，当$x_t \gt 0$，偏导很快变小。</p><p><img src="/archives/./1533453206877.png" alt="图6"></p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>References<br>[1] S. Bell, C. L. Zitnick, K. Bala, and R. Girshick. Insideoutside<br>net: Detecting objects in context with skip pooling<br>and recurrent neural networks. In CVPR, 2016. 6<br>[2] S. R. Bulo, G. Neuhold, and P. Kontschieder. Loss maxpooling<br>for semantic image segmentation. In CVPR, 2017.<br>3<br>[3] J. Dai, Y. Li, K. He, and J. Sun. R-FCN: Object detection via<br>region-based fully convolutional networks. In NIPS, 2016. 1<br>[4] N. Dalal and B. Triggs. Histograms of oriented gradients for<br>human detection. In CVPR, 2005. 2<br>[5] P. Dollar, Z. Tu, P. Perona, and S. Belongie. Integral channel ´<br>features. In BMVC, 2009. 2, 3<br>[6] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable<br>object detection using deep neural networks. In CVPR, 2014.<br>2<br>[7] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and<br>A. Zisserman. The PASCAL Visual Object Classes (VOC)<br>Challenge. IJCV, 2010. 2<br>[8] P. F. Felzenszwalb, R. B. Girshick, and D. McAllester. Cascade<br>object detection with deformable part models. In CVPR,2010. 2, 3<br>[9] C.-Y. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg. DSSD:<br>Deconvolutional single shot detector. arXiv:1701.06659,2016. 1, 2, 8<br>[10] R. Girshick. Fast R-CNN. In ICCV, 2015. 1, 2, 4, 6, 8<br>[11] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature<br>hierarchies for accurate object detection and semantic<br>segmentation. In CVPR, 2014. 1, 2, 5<br>[12] R. Girshick, I. Radosavovic, G. Gkioxari, P. Dollar, ´<br>and K. He. Detectron. <a href="https://github.com/" rel="external nofollow noopener noreferrer" target="_blank">https://github.com/</a><br>facebookresearch/detectron, 2018. 8<br>[13] T. Hastie, R. Tibshirani, and J. Friedman. The elements of<br>statistical learning. Springer series in statistics Springer,<br>Berlin, 2008. 3, 7<br>[14] K. He, G. Gkioxari, P. Dollar, and R. Girshick. Mask R- ´<br>CNN. In ICCV, 2017. 1, 2, 4<br>[15] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling<br>in deep convolutional networks for visual recognition. In<br>ECCV. 2014. 2<br>[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning<br>for image recognition. In CVPR, 2016. 2, 4, 5, 6, 8<br>[17] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara,<br>A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, and<br>K. Murphy. Speed/accuracy trade-offs for modern convolutional<br>object detectors. In CVPR, 2017. 2, 8<br>[18] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet classification<br>with deep convolutional neural networks. In NIPS,2012. 2<br>[19] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.<br>Howard, W. Hubbard, and L. D. Jackel. Backpropagation<br>applied to handwritten zip code recognition. Neural computation,1989. 2<br>[20] T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, and ´<br>S. Belongie. Feature pyramid networks for object detection.<br>In CVPR, 2017. 1, 2, 4, 5, 6, 8<br>[21] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,<br>P. Dollar, and C. L. Zitnick. Microsoft COCO: Com- ´<br>mon objects in context. In ECCV, 2014. 1, 6<br>[22] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, and S. Reed.<br>SSD: Single shot multibox detector. In ECCV, 2016. 1, 2, 3,<br>6, 7, 8<br>[23] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional<br>networks for semantic segmentation. In CVPR, 2015. 4<br>[24] P. O. Pinheiro, R. Collobert, and P. Dollar. Learning to segment<br>object candidates. In NIPS, 2015. 2, 4<br>[25] P. O. Pinheiro, T.-Y. Lin, R. Collobert, and P. Dollar. Learn- ´<br>ing to refine object segments. In ECCV, 2016. 2<br>[26] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You<br>only look once: Unified, real-time object detection. In<br>CVPR, 2016. 1, 2<br>[27] J. Redmon and A. Farhadi. YOLO9000: Better, faster,<br>stronger. In CVPR, 2017. 1, 2, 8<br>[28] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards<br>real-time object detection with region proposal networks.<br>In NIPS, 2015. 1, 2, 4, 5, 8<br>[29] H. Rowley, S. Baluja, and T. Kanade. Human face detection<br>in visual scenes. Technical Report CMU-CS-95-158R,<br>Carnegie Mellon University, 1995. 2<br>[30] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,<br>and Y. LeCun. Overfeat: Integrated recognition, localization<br>and detection using convolutional networks. In ICLR, 2014.<br>2<br>[31] A. Shrivastava, A. Gupta, and R. Girshick. Training regionbased<br>object detectors with online hard example mining. In<br>CVPR, 2016. 2, 3, 6, 7<br>[32] A. Shrivastava, R. Sukthankar, J. Malik, and A. Gupta. Beyond<br>skip connections: Top-down modulation for object detection.<br>arXiv:1612.06851, 2016. 2, 8<br>[33] K.-K. Sung and T. Poggio. Learning and Example Selection<br>for Object and Pattern Detection. In MIT A.I. Memo No.<br>1521, 1994. 2, 3<br>[34] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi.<br>Inception-v4, inception-resnet and the impact of residual<br>connections on learning. In AAAI Conference on Artificial<br>Intelligence, 2017. 8<br>[35] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W.<br>Smeulders. Selective search for object recognition. IJCV,2013. 2, 4<br>[36] R. Vaillant, C. Monrocq, and Y. LeCun. Original approach<br>for the localisation of objects in images. IEE Proc. on Vision,<br>Image, and Signal Processing, 1994. 2<br>[37] P. Viola and M. Jones. Rapid object detection using a boosted<br>cascade of simple features. In CVPR, 2001. 2, 3<br>[38] S. Xie, R. Girshick, P. Dollar, Z. Tu, and K. He. Aggregated ´<br>residual transformations for deep neural networks. In CVPR,2017. 8<br>[39] C. L. Zitnick and P. Dollar. Edge boxes: Locating object ´<br>proposals from edges. In ECCV, 2014. 2</p></div><div class="popular-posts-header">Related Posts</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/562809c1.html" rel="bookmark">YOLO</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/5beb21d0.html" rel="bookmark">R-CNN</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/fc798de3.html" rel="bookmark">Faster R-CNN</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/423509a9.html" rel="bookmark">YOLOv2 and YOLO9000</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/719cc2e5.html" rel="bookmark">SSD</a></div></li></ul><div><ul class="post-copyright"><li class="post-copyright-author"><strong>Post author: </strong>慕湮</li><li class="post-copyright-link"><strong>Post link: </strong><a href="http://muyaan.com/archives/8504aa6c.html" title="RetinaNet">http://muyaan.com/archives/8504aa6c.html</a></li><li class="post-copyright-license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noopener noreferrer" target="_blank">CC BY-NC-SA 4.0</a> unless stating additionally.</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Computer-Vision/" rel="tag"><i class="fa fa-tag"></i> Computer Vision</a> <a href="/tags/Deep-Learning/" rel="tag"><i class="fa fa-tag"></i> Deep Learning</a> <a href="/tags/Paper/" rel="tag"><i class="fa fa-tag"></i> Paper</a> <a href="/tags/Object-Detection/" rel="tag"><i class="fa fa-tag"></i> Object Detection</a> <a href="/tags/RetinaNet/" rel="tag"><i class="fa fa-tag"></i> RetinaNet</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/archives/562809c1.html" rel="next" title="YOLO"><i class="fa fa-chevron-left"></i> YOLO</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/archives/719cc2e5.html" rel="prev" title="SSD">SSD <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">Table of Contents</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">Overview</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4" alt="慕湮"><p class="site-author-name" itemprop="name">慕湮</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">21</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">30</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/tycallen" target="_blank" title="GitHub" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-github"></i>GitHub</a></span><br><span class="links-of-author-item"><a href="mailto:tyc.allen@gmail.com" target="_blank" title="E-Mail" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span><br><span class="links-of-author-item"><a href="http://weibo.com/pojunallen" target="_blank" title="微博" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-weibo"></i>微博</a></span><br><span class="links-of-author-item"><a href="https://tuchong.com/1070837" target="_blank" title="图虫" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-globe"></i>图虫</a></span><br></div><div class="links-of-blogroll motion-element links-of-blogroll-block"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> 友情链接</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="https://dotrabbit.github.io/" title="dotrabbit" target="_blank" rel="external nofollow noopener noreferrer">dotrabbit</a></li></ul></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Retina-Net-Focal-Loss-for-Dense-Object-Detection"><span class="nav-text">Retina Net : Focal Loss for Dense Object Detection</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Introduction"><span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Related-Work"><span class="nav-text">2. Related Work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Focal-Loss"><span class="nav-text">3. Focal Loss</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Balanced-Cross-Entropy"><span class="nav-text">3.1 Balanced Cross Entropy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-Focal-Loss-Definition"><span class="nav-text">3.2 Focal Loss Definition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-Class-Imbalance-and-Model-Initialization"><span class="nav-text">3.3 Class Imbalance and Model Initialization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-Class-Imbalance-and-Two-Stage-Detectors"><span class="nav-text">3.4 Class Imbalance and Two-Stage Detectors</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-RetinaNet-Detector"><span class="nav-text">4. RetinaNet Detector</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-Inference-and-Training"><span class="nav-text">4.1 Inference and Training</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Experiments"><span class="nav-text">5. Experiments</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-Training-Dense-Detection"><span class="nav-text">5.1 Training Dense Detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-Model-Architecture-Design"><span class="nav-text">5.2 Model Architecture Design</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-Comparison-to-State-of-the-Art"><span class="nav-text">5.3 Comparison to State of the Art</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-Conclusion"><span class="nav-text">6. Conclusion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Appendix-A-Focal-Loss"><span class="nav-text">Appendix A: Focal Loss*</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Appendix-B-Derivatives"><span class="nav-text">Appendix B: Derivatives</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-text">References</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; 2015 &mdash; <span itemprop="copyrightYear">2018</span> <span class="with-love" id="animate"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">慕湮</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="Symbols count total">312k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="Reading time total">9:27</span></div><div class="powered-by">Powered by <a class="theme-link" target="_blank" rel="external nofollow noopener noreferrer" href="https://hexo.io">Hexo</a></div><span class="post-meta-divider">|</span><div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" rel="external nofollow noopener noreferrer" href="https://theme-next.org">NexT.Muse</a></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="//cdn.staticfile.org/jquery/2.1.3/jquery.min.js"></script><script type="text/javascript" src="//cdn.staticfile.org/velocity/1.2.1/velocity.min.js"></script><script type="text/javascript" src="//cdn.staticfile.org/velocity/1.2.1/velocity.ui.min.js"></script><script type="text/javascript" src="/lib/reading_progress/reading_progress.js"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=6.3.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=6.3.0"></script><script defer type="text/javascript" src="/js/src/onload.js?v=6.3.0"></script><script src="https://cdn.staticfile.org/pace/1.0.2/pace.min.js"></script><link href="https://cdn.staticfile.org/pace/1.0.2/themes/blue/pace-theme-big-counter.min.css" rel="stylesheet"><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script><script>AV.initialize("UWGXBMGbguCDhSCbC3NrUQtL-gzGzoHsz","ke1jrA5b6VyR89Kqqqwf2kPP")</script><script>function showTime(e){var t=new AV.Query(e),c=[],u=$(".leancloud_visitors");u.each(function(){c.push($(this).attr("id").trim())}),t.containedIn("url",c),t.find().done(function(e){var t=".leancloud-visitors-count";if(0!==e.length){for(var n=0;n<e.length;n++){var o=e[n],i=o.get("url"),s=o.get("time"),r=document.getElementById(i);$(r).find(t).text(s)}for(n=0;n<c.length;n++){i=c[n],r=document.getElementById(i);var l=$(r).find(t);""==l.text()&&l.text(0)}}else u.find(t).text(0)}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(i){var e=$(".leancloud_visitors"),s=e.attr("id").trim(),r=e.attr("data-flag-title").trim(),t=new AV.Query(i);t.equalTo("url",s),t.find({success:function(e){if(0<e.length){var t=e[0];t.fetchWhenSave(!0),t.increment("time"),$(document.getElementById(s)).find(".leancloud-visitors-count").text(t.get("time")),t.save(null,{success:function(e){},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var n=new i,o=new AV.ACL;o.setPublicReadAccess(!0),o.setPublicWriteAccess(!0),n.setACL(o),n.set("title",r),n.set("url",s),n.set("time",1),n.save(null,{success:function(e){$(document.getElementById(s)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):1<$(".post-title-link").length&&showTime(e)})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script async type="text/javascript" color="99,184,255" opacity="0.5" zindex="-2" count="99" src="//cdn.staticfile.org/canvas-nest.js/1.0.0/canvas-nest.js"></script></body></html>