<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">






  
  
    
      
    
    
      
    
  <script src="https://cdn.staticfile.org/pace/1.0.2/pace.min.js"></script>
  <link href="https://cdn.staticfile.org/pace/1.0.2/themes/blue/pace-theme-big-counter.min.css" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">






















  

<link href="//cdn.staticfile.org/font-awesome/4.6.2/css/font-awesome.min.css" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.4.1" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.4.1">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.4.1">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.4.1">


  <link rel="mask-icon" href="/images/logo.svg?v=6.4.1" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.4.1',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Face++ 17年的一篇论文，Aligned ReID，公司有同事正在使用这篇论文。入坑两个月，18篇博文，感觉还行。不过相关领域经典论文越看越少，今后涉及前沿论文时，不能从海量论文中筛选出有价值的，我就凉了。通过abstract和Introduction快速了解一篇论文，需要好好练习。要看的太多了，数学的微积分、线代、概率统计，CV的传统特征、传统算法也知之甚少，深度学习的训练也做的不多，只能">
<meta name="keywords" content="Computer Vision,Deep Learning,Convolutional Network,Paper,re-ID">
<meta property="og:type" content="article">
<meta property="og:title" content="Aligned ReID">
<meta property="og:url" content="http://muyaan.com/archives/ae74f67c.html">
<meta property="og:site_name" content="慕湮">
<meta property="og:description" content="Face++ 17年的一篇论文，Aligned ReID，公司有同事正在使用这篇论文。入坑两个月，18篇博文，感觉还行。不过相关领域经典论文越看越少，今后涉及前沿论文时，不能从海量论文中筛选出有价值的，我就凉了。通过abstract和Introduction快速了解一篇论文，需要好好练习。要看的太多了，数学的微积分、线代、概率统计，CV的传统特征、传统算法也知之甚少，深度学习的训练也做的不多，只能">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://muyaan.com/archives/1536136805860.png">
<meta property="og:image" content="http://muyaan.com/archives/1536148224803.png">
<meta property="og:image" content="http://muyaan.com/archives/1536148246710.png">
<meta property="og:image" content="http://muyaan.com/archives/1536197175462.png">
<meta property="og:image" content="http://muyaan.com/archives/1536200938543.png">
<meta property="og:image" content="http://muyaan.com/archives/1536201678819.png">
<meta property="og:image" content="http://muyaan.com/archives/1536201710175.png">
<meta property="og:image" content="http://muyaan.com/archives/1536202637951.png">
<meta property="og:image" content="http://muyaan.com/archives/1536202664437.png">
<meta property="og:image" content="http://muyaan.com/archives/1536202680372.png">
<meta property="og:image" content="http://muyaan.com/archives/1536202619620.png">
<meta property="og:image" content="http://muyaan.com/archives/1536216010913.png">
<meta property="og:image" content="http://muyaan.com/archives/1536216190641.png">
<meta property="og:image" content="http://muyaan.com/archives/1536216209637.png">
<meta property="og:updated_time" content="2018-09-12T03:22:25.017Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Aligned ReID">
<meta name="twitter:description" content="Face++ 17年的一篇论文，Aligned ReID，公司有同事正在使用这篇论文。入坑两个月，18篇博文，感觉还行。不过相关领域经典论文越看越少，今后涉及前沿论文时，不能从海量论文中筛选出有价值的，我就凉了。通过abstract和Introduction快速了解一篇论文，需要好好练习。要看的太多了，数学的微积分、线代、概率统计，CV的传统特征、传统算法也知之甚少，深度学习的训练也做的不多，只能">
<meta name="twitter:image" content="http://muyaan.com/archives/1536136805860.png">






  <link rel="canonical" href="http://muyaan.com/archives/ae74f67c.html">



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Aligned ReID | 慕湮</title>
  






  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ca5844321cfb80fdf6f12b4dcc326991";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">慕湮</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <h1 class="site-subtitle" itemprop="description">白日放歌须纵酒</h1>
      
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives menu-item-active">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>
  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  
    <div class="reading-progress-bar"></div>
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://muyaan.com/archives/ae74f67c.html">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="慕湮">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="慕湮">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">Aligned ReID
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-09-06 15:16:03" itemprop="dateCreated datePublished" datetime="2018-09-06T15:16:03+08:00">2018-09-06</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-09-12 11:22:25" itemprop="dateModified" datetime="2018-09-12T11:22:25+08:00">2018-09-12</time>
              
            
          </span>

          

          
            
          

          
          
             <span id="/archives/ae74f67c.html" class="leancloud_visitors" data-flag-title="Aligned ReID">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数：</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">本文字数：</span>
                
                <span title="本文字数">17k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">31 分钟</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Face++ 17年的一篇论文，Aligned ReID，公司有同事正在使用这篇论文。<br>入坑两个月，18篇博文，感觉还行。不过相关领域经典论文越看越少，今后涉及前沿论文时，不能从海量论文中筛选出有价值的，我就凉了。通过abstract和Introduction快速了解一篇论文，需要好好练习。<br>要看的太多了，数学的微积分、线代、概率统计，CV的传统特征、传统算法也知之甚少，深度学习的训练也做的不多，只能慢慢积跬步了。</p>
<blockquote>
<p><a href="https://arxiv.org/abs/1711.08184" rel="external nofollow noopener noreferrer" target="_blank">AlignedReID: Surpassing Human-Level Performance in Person Re-Identification</a><br>Xuan Zhang, Hao Luo, Xing Fan, Weilai Xiang, Yixiao Sun, Qiqi Xiao, Wei Jiang, Chi Zhang, Jian Sun<br>Megvii, Inc.(Face++), Institute of Cyber-Systems and Control, Zhejiang University, 2017</p>
</blockquote>
<h2 id=""><a href="#" class="headerlink" title=""></a><a id="more"></a></h2><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>Person Re-ID，在另一个时间或地点认出一个人，是一个有挑战性的计算机视觉任务。其应用广泛，从在不同摄像头追踪一个人到从庞大图片集搜索人，从为图集分组到零售店里的访客分析。如许多视觉识别问题一样，姿势的变化，视角光照，遮挡都让这一问题很困难。</p>
<p>传统方法重点在低级特征，如颜色、形状和局部描述符$^{[9, 11]}$。随着深度学习的复活，CNN以端到端的形式，通过多种loss如contrasive loss$^{[32]}$，triplet loss$^{[18]}$，improved triplet loss$^{[6]}$，quadruplet loss$^{[3]}$和triplet hard loss$^{[13]}$统治了这一领域$^{[24, 32, 6, 54, 16, 24]}$。</p>
<p><img src="/archives/ae74f67c/1536136805860.png" alt="图1"></p>
<p>许多基于CNN的方法学习全局特征，忽略了人的空间结构，缺点如下：1) 不准确的人检测框会影响特征学习，如图1 (a-b)；2) 姿势的变化或不严格的身体变形使训练很困难，如图1(c-d)；3) 人体被遮挡部分会为习得特征引入不相关的上下文，如图1(e-f)；4) 在全局特征中强调局部的不同非常困难，特别是我们需要区别两个外观非常近似的人，如图1(g-h)。为了克服这些缺点，近期研究开始关注基于部分的、局部特征训练。一些研究$^{[33, 38, 43]}$把整个身体划分为几个固定部分，不考虑它们之间的对齐。但它仍受制于缺点1、2、3。一些其他研究将姿势估计结果用于对齐$^{[52, 37, 50]}$，这需要额外的监督和一个姿势预估步骤（通常有错误倾向）。</p>
<p>本论文提出了一个新的方法，叫做AlignedReID。它仍学习全局特征，但在训练中进行自动部位对齐，同时不需要额外的监督或显式的姿势估计。在训练时，我们同时训练一个全局特征和局部特征。在局部分支，我们通过引入一个shortest path loss，对齐局部部分。在推理时，我们抛弃局部分支，仅提取全局特征。我们发现仅使用全局特征就几乎与同时使用两个的性能近似了。换句话说，在我们的联合训练框架中，全局特征它自己在局部特征训练的帮助下，能非常好的解决我们之前提到的缺点。不仅如此，全局特征的形式使我们的方法很适合大规模ReID系统的部署，不需要花费很高的局部特征匹配。</p>
<p>我们在训练目标设置中还采用了一个互助mutual训练方法$^{[49]}$，允许两个模型互相习得更佳的表达。将AlignedReID与互助学习结合起来，我们的系统在Market1501，CUHK03和CUHK-SYSU上领先前沿一大截。为了理解人类在ReID任务中表现如何，我们测量了10个职业标注人。我们发现带re-ranking的系统超越了人类的准确率。据我们所知，这是首个Re-ID任务中机器超越人类的报告。</p>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p><strong>Metric Learning.</strong> 深度度量训练方法将原始图片转换为特征，计算特征距离作为相似度。通常一个人的两张图片定义为positive对。Triplet loss被正负对之间的空白所驱动。通过HEM选择合适样本是有效的。将softmax和度量loss结合起来加速收敛也是流行的方法。</p>
<p><strong>Feature Alignment.</strong> 近期通过姿势估计对齐局部特征很流行。如用pose invariant embedding PIE将行人对齐为标注姿势来减少姿势不同的影响。Global-Local-Alignment Descriptor(GLAD)不直接对齐行人，而是检测关键姿势点，并从对应区域提取局部特征。SpindleNet$^{[50]}$使用RPN生成多个身体区域，在不同阶段逐步合并相邻身体区域的response map。这些方法都需要额外姿势标注，并处理姿势估计引入的误差。</p>
<p><strong>Mutual Learning.</strong> [49]提出了一个深度互助训练策略，在训练过程中全体学生合作学习并互相传授。DarkRank$^{[4]}$提出了一种新的用于模型压缩和加速的knowledge-cross样本相似度，取得了前沿性能。这些方法在分类中使用了互助训练。我们在本论文中学习了这一方法。</p>
<p><strong>Re-ranking.</strong> 在获得图片特征后，大部分当前方法都使用L2欧氏距离来为ranking或检索计算相似得分。[35, 57, 1]使用了额外的Re-ranking来提升Re-ID准确率。特别的，[57]提出了使用k-reciprocal编码的Re-ranking方法，结合了原始距离和Jaccard 距离。</p>
<h2 id="3-Our-Approach"><a href="#3-Our-Approach" class="headerlink" title="3. Our Approach"></a>3. Our Approach</h2><h3 id="3-1-AlignedReID"><a href="#3-1-AlignedReID" class="headerlink" title="3.1 AlignedReID"></a>3.1 AlignedReID</h3><p>我们为输入图片生成单个全局特征，作为最终输出，并使用L2距离作为相似性度量。但全局特征是在训练阶段与局部特征联合习得的。</p>
<p>对每个图片，我们使用一个CNN（如ResNet-50）来提取特征图，即最后一个卷积层的输出（$C \times H \times W$，图2中是$2048\times 7\times7$）。在特征图上直接使用全局池化就得到了全局特征。局部特征使用水平池化，一个在水平方向的全局池化，在每一行提取一个局部特征，再用一个$1\times1$卷积来将通道数从C降低到c。这一每个局部特征(c维向量)表达了一个人的图片的一个水平部分。因此一个人的图片被表达为一个全局特征和H个局部特征。</p>
<p>两个人的图片的距离是它们全局和局部距离的和。全局距离是全局特征的L2距离。对于局部距离，我们从上至下动态的匹配局部部分，找到有最小棕距离的局部特征alignment。这基于一个简单假设，对于同一人的两张图片，第一张图片的一个身体部分的局部特征与另一张图的对应身体部分最相似。</p>
<p><img src="/archives/ae74f67c/1536148224803.png" alt="图2"></p>
<p>给定两个图片的局部特征，$F=\{f_1,…,f_H\}$和$G=\{g_1,…,g_H\}$，我们先通过一个 逐元素变化归一化距离：</p>
<script type="math/tex; mode=display">d_{i,j} = \frac {e^{\left || f_i-g_j \right ||_2} - 1} {e^{\left || f_i-g_j \right ||_2} + 1} \ \ \   i,j \in 1, 2, 3 ..., H, \tag 1</script><p>其中$d_{i,j}$是第一张图片的第i个部分和第二张图片的第j部分的距离。基于这些距离构成了距离矩阵D，第(i,j)个元素就是$d_{i,j}$。我们将两张图片的局部距离定义为矩阵D中从(1,1)到(H,H)的最短路径。距离可以如下动态规划求解：</p>
<script type="math/tex; mode=display">S_{i,j} =
\begin{cases} 
d_{i,j}  & \mbox{}i=1,\mbox{}  j=1\\
S_{i-1,\ j} +d_{i,j}  & \mbox{}i\neq1,\mbox{}  j=1\\
S_{i,\ j-1} +d_{i,j}  & \mbox{}i = 1,\mbox{}  j \neq 1\\
\min (S_{i-1,\ j} , S_{i,\ j-1} )+d_{i,j}  & \mbox{}i \neq 1,\mbox{}  j \neq 1\\
\end{cases} \ \tag 2</script><p>其中$S_{i,j}$是在距离矩阵D中从(1,1)到(i,j)总距离最短的路径距离。</p>
<p><img src="/archives/ae74f67c/1536148246710.png" alt="图3"></p>
<p>如图3所示，图片AB都是同一个人的样本。对应身体部分的对齐，如A中的P1和B中的P4，都包含在最短路径中。同时，不对应的部分的对齐也包含在了最短路径中，如AB中各自的P1。这些无关对齐对于保持vertical alignment的顺序和让相关对齐可能是必须的。无关的对齐有更大的L2距离，其等式1中的梯度接近0。因此它的贡献很小。最短路径的总距离，即两种图片的局部距离，主要由相关对齐决定。<br>训练阶段，全局和局部距离一起定义了两种图片的相似度，我们使用了[13]提出的Trihard Loss。对于每个样本按照全局距离，找出同一身份最不相似的和不同身份最相似的样本，组成triplet。使用全局距离进行HEM的原因有二。首先全局距离计算快很多。其次我们观察到使用两种距离进行HEM没有显著区别。</p>
<p>推理阶段仅用全局特征计算相似度，因为接近两个特征一起用的性能。这一反直觉的现象可能有两个原因：1) 联合训练的特征图比单独训练全局特征好，因为我们训练阶段开发了人物图片的结构；2) 在局部特征匹配的帮助下，全局特征能将注意力集中于人体，而不是过拟合背景。</p>
<h3 id="3-2-Mutual-Learning-for-Metric-Learning"><a href="#3-2-Mutual-Learning-for-Metric-Learning" class="headerlink" title="3.2 Mutual Learning for Metric Learning"></a>3.2 Mutual Learning for Metric Learning</h3><p>我们训练AlignedReID模型时采用了互助训练。一个基于distillation（精华/蒸馏）的模型从一个预训练的大教师网向一个小的学生网络转移知识，如[4]。本论文中，我们同时训练学生模型集，互相之间转移知识，如[49]。[49]在分类概率仅采用了Kullback-Leibler(KL)距离，我们提出了一个新的互助学习loss。</p>
<p><img src="/archives/ae74f67c/1536197175462.png" alt="图4"></p>
<p>训练方法如图4。整体loss函数包含了metric loss, metric mutual loss, classification loss和classification mutual loss。metric loss由全局和局部距离决定，而metric mutual loss仅由全局距离决定。classification mutual loss如[49]一样，是KL divergence。</p>
<p>给定N张图的batch，每个网络提取它们的全局特征，计算全局距离，得到$N\times N$批量距离矩阵，其中$M_{ij}^{\theta_1}$和$M_{ij}^{\theta_2}$标记了各自矩阵的第(i,j)个元素。互助训练loss如下：</p>
<script type="math/tex; mode=display">\begin{align}
L_M  = \frac 1 {N^2} \sum _i^N \sum _j ^N &( [ZG(M_{ij}^{\theta_1}) -M_{ij}^{\theta_2}]^2 \notag \\
& + [M_{ij}^{\theta_1} - ZG( M_{ij}^{\theta_2})]^2 )  \notag  \\
\end{align} \tag 3</script><p>其中$ZG(\cdot)$代表zero gradient函数，在计算梯度时将变量档次常量，在训练阶段阻止反向传播。</p>
<p>通过应用0梯度函数，二阶梯度为：</p>
<script type="math/tex; mode=display">\frac {\partial ^2 L_M} {\partial M_{ij}^{\theta_1} \partial M_{ij}^{\theta ^2}} = 0 \tag 4</script><p>我们发现这能加速收敛并提升准确率。</p>
<h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h2><h3 id="4-1-Datasets"><a href="#4-1-Datasets" class="headerlink" title="4.1 Datasets"></a>4.1 Datasets</h3><h3 id="4-2-Implementation-Details"><a href="#4-2-Implementation-Details" class="headerlink" title="4.2 Implementation Details"></a>4.2 Implementation Details</h3><p>我们使用在ImageNet上预训练的ResNet-50和ResNet50-Xception（ResNet-X）作为基础模型。ResNet-X将$3\times3$卷积核替换为Xception cell$^{[7]}$，含有一个$3\times3$逐通道的卷积层和一个$1\times1$空间卷积层。每张图缩放到$224\times224$像素。数据增广包括随机水平翻转和裁切。全局和局部距离的TriHard loss的margin为0.5，batch-size为160，每个身份有4张图片。每个epoch有2000个mini-batch。我们使用初始学习率为0.001的Adam optimizer，在80和160epoch缩小10倍，直到收敛。</p>
<p>关于互助训练，Classification mutual loss（KL）的权重为0.01，metric mutual loss的权重为0.001。使用初始学习率为$3\times10^{-4}$的Adam optimizer，在60和120epoch缩小$10^{-4}$和$10^{-5}$。</p>
<p>Re-ranking是提升Re-ID性能的有效技术。我们采用了[57]的方法和细节。</p>
<h3 id="4-3-Advantage-of-AlignedReID"><a href="#4-3-Advantage-of-AlignedReID" class="headerlink" title="4.3 Advantage of AlignedReID"></a>4.3 Advantage of AlignedReID</h3><p>图5是一些典型对齐结果。图5(a)中，右边人的检测框不准确，导致了头部严重未对准。AlignedReID将左边的第一块和右边的前3块匹配到了最短路径中。图5(b)是另一种困难的情况，人体结构不完全。左图不包含膝盖以下的部分。在对齐中，左右图的裙子部分关联了起来，而右图的腿部为最短路径贡献很小。图5(c)是一个遮挡的例子，人的下半部不可见。对齐显示被遮挡部分为最短路径贡献很小，故训练时重点在其它部分。图5(d)是两个相似的人。他们T恤logo不同，导致两个图片的最短路径总距离较大。</p>
<p><img src="/archives/ae74f67c/1536200938543.png" alt="图5"></p>
<p>我们接着比较了两个类似的网络：Baseline不含局部特征分支，GL-Baseline有不对齐的局部特征分支。GL-Baseline中局部loss是空间相关的局部特征距离和。所有结果都通过同样的网络和同样的训练设定得到。结果见表1。与Baseline相比，GL-Baseline通常准确率更低。因此不对齐的局部分支没有帮助。同时AlignedReID在所有数据集提升了3.1%~7.9%的rank-1准确率和3.6%~10.1%的mAP。带对齐的局部特征分支有助于让网络关注有用的图片区，分辨出只有微小不同的相似图片。</p>
<p><img src="/archives/ae74f67c/1536201678819.png" alt="表1"></p>
<p>我们发现如果我们在推理阶段还使用局部距离，rank-1准确率提升约0.3%~0.5%。但耗时太大。</p>
<h3 id="4-4-Analysis-of-Mutual-Learning"><a href="#4-4-Analysis-of-Mutual-Learning" class="headerlink" title="4.4 Analysis of Mutual Learning"></a>4.4 Analysis of Mutual Learning</h3><p>结果见表2。</p>
<p><img src="/archives/ae74f67c/1536201710175.png" alt="表2"></p>
<h3 id="4-5-Comparison-with-Other-Methods"><a href="#4-5-Comparison-with-Other-Methods" class="headerlink" title="4.5 Comparison with Other Methods"></a>4.5 Comparison with Other Methods</h3><p>我们比较了各前沿方法，见表3~5。AlignedReID代表我们的方法，使用了互助训练。AlignedReID（RK）是我们的方法，带互助训练和带k-reciprocal编码的Re-ranking。</p>
<p><img src="/archives/ae74f67c/1536202637951.png" alt="表3"></p>
<p><img src="/archives/ae74f67c/1536202664437.png" alt="表4"></p>
<p><img src="/archives/ae74f67c/1536202680372.png" alt="表5"></p>
<h2 id="5-Human-Performance-in-Person-ReID"><a href="#5-Human-Performance-in-Person-ReID" class="headerlink" title="5. Human Performance in Person ReID"></a>5. Human Performance in Person ReID</h2><p>为了让研究可行，每个查询图片，人类仅需从小得多的图集中辨认。人的推理所用图集见图6。结果见表6。</p>
<p><img src="/archives/ae74f67c/1536202619620.png" alt="图6"></p>
<p><img src="/archives/ae74f67c/1536216010913.png" alt="表6"></p>
<p>图7显示了一些样本，标注者选择错误，而我们的top-1正确。</p>
<p><img src="/archives/ae74f67c/1536216190641.png" alt="图7"></p>
<h2 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6. Conclusion"></a>6. Conclusion</h2><p>本论文证明了隐式的局部特征对齐能大幅提升全局特征训练。这一结果给与了我们重要的insight：有结构的先验端到端训练比盲目的要强力得多。</p>
<p>我们的方法仍未完全超越人类，图8是一些很少困惑到人类的大错误。</p>
<p><img src="/archives/ae74f67c/1536216209637.png" alt="图8"></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] S. Bai, X. Bai, and Q. Tian. Scalable person reidentification on supervised smoothed manifold. arXiv preprint arXiv:1703.08359, 2017.<br>[2] I. B. Barbosa, M. Cristani, B. Caputo, A. Rognhaugen, and T. Theoharis. Looking beyond appearances: Synthetic training data for deep cnns in re-identification. arXiv preprint arXiv:1701.03153, 2017.<br>[3] W. Chen, X. Chen, J. Zhang, and K. Huang. Beyond triplet loss: a deep quadruplet network for person re-identification. arXiv preprint arXiv:1704.01719, 2017.<br>[4] Y. Chen, N. Wang, and Z. Zhang. Darkrank: Accelerating deep metric learning via cross sample similarities transfer. arXiv preprint arXiv:1707.01220, 2017.<br>[5] Y.-C. Chen, X. Zhu, W.-S. Zheng, and J.-H. Lai. Person reidentification by camera correlation aware feature augmentation. IEEE Transactions on Pattern Analysis and MachineIntelligence, 2017.<br>[6] D. Cheng, Y. Gong, S. Zhou, J. Wang, and N. Zheng. Person re-identification by multi-channel parts-based cnn with improved triplet loss function. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1335–1344, 2016.<br>[7] F. Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016.<br>[8] H. Fan, L. Zheng, and Y. Yang. Unsupervised person reidentification: Clustering and fine-tuning. arXiv preprint arXiv:1705.10444, 2017.<br>[9] M. Farenzena, L. Bazzani, A. Perina, V. Murino, and M. Cristani. Person re-identification by symmetry-driven accumulation of local features. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 2360–2367. IEEE, 2010.<br>[10] M. Geng, Y. Wang, T. Xiang, and Y. Tian. Deep transfer learning for person re-identification. arXiv preprint arXiv:1611.05244, 2016.<br>[11] O. Hamdoun, F. Moutarde, B. Stanciulescu, and B. Steux. Person re-identification in multi-camera system by signature based on interest point descriptors collected on short video sequences. In Distributed Smart Cameras, 2008. ICDSC 2008. Second ACM/IEEE International Conference on, pages 1–6. IEEE, 2008.<br>[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.<br>[13] A. Hermans, L. Beyer, and B. Leibe. In defense of the triplet loss for person re-identification. arXiv preprint arXiv:1703.07737, 2017.<br>[14] W. Li, R. Zhao, T. Xiao, and X. Wang. Deepreid: Deep filter pairing neural network for person re-identification. pages 152–159, 2014.<br>[15] S. Liao, Y. Hu, X. Zhu, and S. Z. Li. Person re-identification by local maximal occurrence representation and metric learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2197–2206, 2015.<br>[16] Y. Lin, L. Zheng, Z. Zheng, Y. Wu, and Y. Yang. Improving person re-identification by attribute and identity learning. arXiv preprint arXiv:1703.07220, 2017.<br>[17] H. Liu, J. Feng, Z. Jie, K. Jayashree, B. Zhao, M. Qi, J. Jiang, and S. Yan. Neural person search machines. In The IEEE International Conference on Computer Vision (ICCV), Oct 2017.<br>[18] H. Liu, J. Feng, M. Qi, J. Jiang, and S. Yan. End-to-end comparative attention networks for person re-identification. IEEE Transactions on Image Processing, 2017.<br>[19] H. Liu, Z. Jie, K. Jayashree, M. Qi, J. Jiang, S. Yan, and J. Feng. Video-based person re-identification with accumulative motion context. arXiv preprint arXiv:1701.00193, 2017.<br>[20] X. Liu, H. Zhao, M. Tian, L. Sheng, J. Shao, S. Yi, J. Yan, and X. Wang. Hydraplus-net: Attentive deep features for pedestrian analysis. 2017.<br>[21] Y. Liu, J. Yan, and W. Ouyang. Quality aware network for set to set recognition. arXiv preprint arXiv:1704.03373, 2017.<br>[22] X. Ma, X. Zhu, S. Gong, X. Xie, J. Hu, K.-M. Lam, and Y. Zhong. Person re-identification by unsupervised video matching. Pattern Recognition, 65:197–210, 2017.<br>[23] N. Martinel, A. Das, C. Micheloni, and A. K. RoyChowdhury. Temporal model adaptation for person reidentification. In European Conference on Computer Vision, pages 858–877. Springer, 2016.<br>[24] T. Matsukawa and E. Suzuki. Person re-identification using cnn features learned from combination of attributes. In Pattern Recognition (ICPR), 2016 23rd International Conference on, pages 2428–2433. IEEE, 2016.<br>[25] N. McLaughlin, J. Martinez del Rincon, and P. Miller. Recurrent convolutional network for video-based person reidentification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1325– 1334, 2016.<br>[26] P. Peng, T. Xiang, Y. Wang, M. Pontil, S. Gong, T. Huang, and Y. Tian. Unsupervised cross-dataset transfer learning for person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1306–1315, 2016.<br>[27] F. Radenovic, G. Tolias, and O. Chum. Cnn image retrieval ´ learns from bow: Unsupervised fine-tuning with hard examples. In European Conference on Computer Vision, pages 3–20. Springer, 2016.<br>[28] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211–252, 2015.<br>[29] A. Schumann, S. Gong, and T. Schuchert. Deep learning prototype domains for person re-identification. arXiv preprint arXiv:1610.05047, 2016.<br>[30] Springer. MARS: A Video Benchmark for Large-Scale Person Re-identification, 2016.<br>[31] Y. T. Tesfaye, E. Zemene, A. Prati, M. Pelillo, and M. Shah. Multi-target tracking in multiple non-overlapping cameras using constrained dominant sets. arXiv preprint arXiv:1706.06196, 2017.<br>[32] R. R. Varior, M. Haloi, and G. Wang. Gated siamese convolutional neural network architecture for human reidentification. In European Conference on Computer Vision, pages 791–808. Springer, 2016.<br>[33] R. R. Varior, B. Shuai, J. Lu, D. Xu, and G. Wang. A siamese long short-term memory architecture for human reidentification. In European Conference on Computer Vision, pages 135–153. Springer, 2016.<br>[34] R. R. Varior, B. Shuai, J. Lu, D. Xu, and G. Wang. A siamese long short-term memory architecture for human reidentification. In European Conference on Computer Vision, pages 135–153, 2016.<br>[35] J. Wang, S. Zhou, J. Wang, and Q. Hou. Deep ranking model by large adaptive margin learning for person reidentification. arXiv preprint arXiv:1707.00409, 2017.<br>[36] T. Wang, S. Gong, X. Zhu, and S. Wang. Person reidentification by discriminative selection in video ranking. IEEE transactions on pattern analysis and machine intelligence, 38(12):2501–2514, 2016.[37] L. Wei, S. Zhang, H. Yao, W. Gao, and Q. Tian. Glad: Global-local-alignment descriptor for pedestrian retrieval. arXiv preprint arXiv:1709.04329, 2017.<br>[38] Q. Xiao, K. Cao, H. Chen, F. Peng, and C. Zhang. Cross domain knowledge transfer for person re-identification. arXiv preprint arXiv:1611.06026, 2016.<br>[39] Q. Xiao, H. Luo, and C. Zhang. Margin sample mining loss: A deep learning based method for person re-identification. arXiv preprint arXiv:1710.00478, 2017.<br>[40] T. Xiao, H. Li, W. Ouyang, and X. Wang. Learning deep feature representations with domain guided dropout for person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1249– 1258, 2016.<br>[41] T. Xiao, S. Li, B. Wang, L. Lin, and X. Wang. Endto-end deep learning for person search. arXiv preprint arXiv:1604.01850, 2016.<br>[42] T. Xiao, S. Li, B. Wang, L. Lin, and X. Wang. Joint detection and identification feature learning for person search. In Proc. CVPR, 2017.<br>[43] H. Yao, S. Zhang, Y. Zhang, J. Li, and Q. Tian. Deep representation learning with part loss for person re-identification. arXiv preprint arXiv:1707.00798, 2017.<br>[44] J. You, A. Wu, X. Li, and W.-S. Zheng. Top-push videobased person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1345–1353, 2016.<br>[45] D. Zhang, W. Wu, H. Cheng, R. Zhang, Z. Dong, and Z. Cai. Image-to-video person re-identification with temporally memorized similarity learning. IEEE Transactions on Circuits and Systems for Video Technology, 2017.<br>[46] L. Zhang, T. Xiang, and S. Gong. Learning a discriminative null space for person re-identification. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.<br>[47] L. Zhang, T. Xiang, and S. Gong. Learning a discriminative null space for person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1239–1248, 2016.<br>[48] W. Zhang, S. Hu, and K. Liu. Learning compact appearance representation for video-based person re-identification. arXiv preprint arXiv:1702.06294, 2017.<br>[49] Y. Zhang, T. Xiang, T. M. Hospedales, and H. Lu. Deep mutual learning. arXiv preprint arXiv:1706.00384, 2017.<br>[50] H. Zhao, M. Tian, S. Sun, J. Shao, J. Yan, S. Yi, X. Wang, and X. Tang. Spindle net: Person re-identification with human body region guided feature decomposition and fusion. CVPR, 2017.<br>[51] R. Zhao, W. Oyang, and X. Wang. Person re-identification by saliency learning. IEEE transactions on pattern analysis and machine intelligence, 39(2):356–370, 2017.<br>[52] L. Zheng, Y. Huang, H. Lu, and Y. Yang. Pose invariant embedding for deep person re-identification. arXiv preprint arXiv:1701.07732, 2017.<br>[53] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian. Scalable person re-identification: A benchmark. In Computer Vision, IEEE International Conference, 2015.<br>[54] L. Zheng, Y. Yang, and A. G. Hauptmann. Person reidentification: Past, present and future. arXiv preprint arXiv:1610.02984, 2016.<br>[55] Z. Zheng, L. Zheng, and Y. Yang. A discriminatively learned cnn embedding for person re-identification. arXiv preprint arXiv:1611.05666, 2016.<br>[56] Z. Zheng, L. Zheng, and Y. Yang. Unlabeled samples generated by gan improve the person re-identification baseline in vitro. In The IEEE International Conference on Computer Vision (ICCV), Oct 2017.<br>[57] Z. Zhong, L. Zheng, D. Cao, and S. Li. Re-ranking person re-identification with k-reciprocal encoding. arXiv preprint arXiv:1701.08398, 2017.<br>[58] Z. Zhou, Y. Huang, W. Wang, L. Wang, and T. Tan. See the forest for the trees: Joint spatial and temporal recurrent neural networks for video-based person re-identification.</p>

      
    </div>

    
      

  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="/archives/ed3a751d.html" rel="bookmark">ResNet</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="/archives/623fcf69.html" rel="bookmark">LeNet</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="/archives/423509a9.html" rel="bookmark">YOLOv2 and YOLO9000</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="/archives/36059c58.html" rel="bookmark">Person Re-identification: Past, Present and Future</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="/archives/b2e32499.html" rel="bookmark">VGGNet</a></div>
      
    </li>
  
  </ul>


    

    
    
    

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>慕湮</li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://muyaan.com/archives/ae74f67c.html" title="Aligned ReID">http://muyaan.com/archives/ae74f67c.html</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noopener noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！</li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Computer-Vision/" rel="tag"># Computer Vision</a>
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
            <a href="/tags/Convolutional-Network/" rel="tag"># Convolutional Network</a>
          
            <a href="/tags/Paper/" rel="tag"># Paper</a>
          
            <a href="/tags/re-ID/" rel="tag"># re-ID</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/archives/a2cff214.html" rel="next" title="GAN 生成式对抗网络">
                <i class="fa fa-chevron-left"></i> GAN 生成式对抗网络
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/archives/380c6fd1.html" rel="prev" title="Improving Person Re-identification by Attribute and Identity Learning">
                Improving Person Re-identification by Attribute and Identity Learning <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4" alt="慕湮">
            
              <p class="site-author-name" itemprop="name">慕湮</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">21</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">30</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/tycallen" target="_blank" title="GitHub" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:tyc.allen@gmail.com" target="_blank" title="E-Mail" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="http://weibo.com/pojunallen" target="_blank" title="微博" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-weibo"></i>微博</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://tuchong.com/1070837" target="_blank" title="图虫" rel="external nofollow noopener noreferrer"><i class="fa fa-fw fa-globe"></i>图虫</a>
                  
                </span>
              
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友情链接
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://dotrabbit.github.io/" title="dotrabbit" target="_blank" rel="external nofollow noopener noreferrer">dotrabbit</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#"><span class="nav-text"></span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Introduction"><span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Related-Work"><span class="nav-text">2. Related Work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Our-Approach"><span class="nav-text">3. Our Approach</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-AlignedReID"><span class="nav-text">3.1 AlignedReID</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-Mutual-Learning-for-Metric-Learning"><span class="nav-text">3.2 Mutual Learning for Metric Learning</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Experiments"><span class="nav-text">4. Experiments</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-Datasets"><span class="nav-text">4.1 Datasets</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-Implementation-Details"><span class="nav-text">4.2 Implementation Details</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-Advantage-of-AlignedReID"><span class="nav-text">4.3 Advantage of AlignedReID</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-Analysis-of-Mutual-Learning"><span class="nav-text">4.4 Analysis of Mutual Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-5-Comparison-with-Other-Methods"><span class="nav-text">4.5 Comparison with Other Methods</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Human-Performance-in-Person-ReID"><span class="nav-text">5. Human Performance in Person ReID</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-Conclusion"><span class="nav-text">6. Conclusion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-text">References</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2015 – <span itemprop="copyrightYear">2018</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">慕湮</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
    <span title="站点总字数">313k</span>
  

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    
    <span title="站点阅读时长">9:30</span>
  
</div>


  



  <div class="powered-by">由 <a class="theme-link" target="_blank" rel="external nofollow noopener noreferrer" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a class="theme-link" target="_blank" rel="external nofollow noopener noreferrer" href="https://theme-next.org">NexT.Muse</a></div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>














  











  



  
  
    <script type="text/javascript" src="//cdn.staticfile.org/jquery/2.1.3/jquery.min.js"></script>
  

  
  
    <script type="text/javascript" src="//cdn.staticfile.org/velocity/1.2.1/velocity.min.js"></script>
  

  
  
    <script type="text/javascript" src="//cdn.staticfile.org/velocity/1.2.1/velocity.ui.min.js"></script>
  

  
  
    <script type="text/javascript" src="//cdn.staticfile.org/canvas-nest.js/1.0.0/canvas-nest.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/reading_progress/reading_progress.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.4.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.4.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.4.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.4.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.1"></script>



  



  










  





  

  
  <script>
    
    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();

      Counter('get', '/classes/Counter', { where: JSON.stringify({ url }) })
        .done(function ({ results }) {
          if (results.length > 0) {
            var counter = results[0];
            
              var $element = $(document.getElementById(url));
              $element.find('.leancloud-visitors-count').text(counter.time + 1);
            
            Counter('put', `/classes/Counter/${counter.objectId}`, JSON.stringify({ time: { "__op":"Increment", "amount":1 } }))
            
            .fail(function ({ responseJSON }) {
                console.log('Failed to save Visitor num, with error message: ' + responseJSON.error);
            })
          } else {
            
              Counter('post', '/classes/Counter', JSON.stringify({ title: title, url: url, time: 1}))
                .done(function () {
                  var $element = $(document.getElementById(url));
                  $element.find('.leancloud-visitors-count').text(1);
                })
                .fail(function () {
                  console.log('Failed to create');
                });
            
          }
        })
      .fail(function ({ responseJSON }) {
        console.log('LeanCloud Counter Error:' + responseJSON.code + " " + responseJSON.error);
      });
    }
    

    $(function() {
      $.get('https://app-router.leancloud.cn/2/route?appId=' + "UWGXBMGbguCDhSCbC3NrUQtL-gzGzoHsz")
        .done(function ({ api_server }) {
          var Counter = function (method, url, data) {
            return $.ajax({
              method: method,
              url: `https://${api_server}/1.1${url}`,
              headers: {
                'X-LC-Id': "UWGXBMGbguCDhSCbC3NrUQtL-gzGzoHsz",
                'X-LC-Key': "ke1jrA5b6VyR89Kqqqwf2kPP",
                'Content-Type': 'application/json',
              },
              data: data,
            });
          };
          
          addCount(Counter);
          
        })
    });
  </script>



  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  
  

  

  

  

  

  

</body>
</html>
