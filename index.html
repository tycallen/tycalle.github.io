<!DOCTYPE html><html class="theme-next muse use-motion" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-big-counter.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="//www.googlefonts.cn//css?family=Lato:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=6.3.0" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.3.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.3.0"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.3.0"><link rel="mask-icon" href="/images/logo.svg?v=6.3.0" color="#222"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Muse",version:"6.3.0",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!1,fastclick:!1,lazyload:!1,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="keywords" content="DL,CV"><meta property="og:type" content="website"><meta property="og:title" content="慕湮"><meta property="og:url" content="http://www.muyaan.com/index.html"><meta property="og:site_name" content="慕湮"><meta property="og:locale" content="zh-Hans"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="慕湮"><link rel="canonical" href="http://www.muyaan.com/"><script type="text/javascript" id="page.configurations">CONFIG.page={sidebar:""}</script><title>慕湮 – 白日放歌须纵酒</title><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?ca5844321cfb80fdf6f12b4dcc326991";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style type="text/css">.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-home"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">慕湮</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">白日放歌须纵酒</p></div><div class="site-nav-toggle"><button aria-label="Toggle navigation bar"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home menu-item-active"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li></ul></nav><script type="text/javascript" color="99,184,255" opacity="0.5" zindex="-2" count="99" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><section id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.muyaan.com/2018/09/03/MTCNN/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="慕湮"><meta itemprop="description" content=""><meta itemprop="image" content="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="慕湮"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/09/03/MTCNN/" itemprop="url">MTCNN</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2018-09-03 16:59:11 / Modified: 17:06:40" itemprop="dateCreated datePublished" datetime="2018-09-03T16:59:11+08:00">2018-09-03</time> </span><span id="/2018/09/03/MTCNN/" class="leancloud_visitors" data-flag-title="MTCNN"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Views: </span><span class="leancloud-visitors-count"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数: </span><span title="Symbols count in article">9.4k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="Reading time">17 mins.</span></div></div></header><div class="post-body" itemprop="articleBody"><p>公司其它项目组在复现的一篇论文，经典人脸检测与对齐。自上一篇论文翻吐了之后，打算总结性的记录论文了。这一篇没有全文翻译，看得非常轻松，不过跟论文本身高度总结，不吹闲B有关系吧。喜欢这样的paper！</p><blockquote><h1 id="MTCNN-Joint-Face-Detection-and-Alignment-using-Multi-task-Cascaded-Convolutional-Networks"><a href="#MTCNN-Joint-Face-Detection-and-Alignment-using-Multi-task-Cascaded-Convolutional-Networks" class="headerlink" title="MTCNN: Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks"></a><a href="https://kpzhang93.github.io/MTCNN_face_detection_alignment/paper/spl.pdf" target="_blank" rel="noopener">MTCNN: Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks</a></h1><p>Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, Yu Qiao<br>IEEE 2016</p></blockquote><hr><h2 id="I-Introduction"><a href="#I-Introduction" class="headerlink" title="I. Introduction"></a>I. Introduction</h2><p>人脸检测和对齐是很多人脸应用的基础，如人脸识别和表情分析facial expression analysis。但脸部视觉变化过大，如遮挡，姿势和光照都为实际应用提出了挑战。</p><p>Viola和Jones提出的层叠人脸检测$^{[2]}$使用了Haar-Like特征和AdaBoost取得了不错的性能，也有实时效率。但在实际应用效果不佳。也有使用Deformable part models DPM的$^{[5,6,7]}$。Yang等人$^{[11]}$使用了卷积神经网络进行人脸属性识别，来进一步产生人脸窗口候选。Li等人$^{[19]}$使用了层叠CNN，但需要限位框校准，且忽略了人脸关键点位置的相关性和限位框回归。</p><p>人脸对齐可以分为两类，基于回归的方法$^{[12,13,16]}$和模板匹配template fitting方法$^{[14, 15, 7]}$。近期Zhang等人$^{[22]}$提出了人脸属性识别作为额外的任务，以加强人脸对齐性能。</p><p>但之前所有人脸检测和对齐方法都忽略了它们之间的相关性。有的研究尝试了同时解决它们，但仍有限制。Chen等人$^{[18]}$使用不同像素值的随机森林同时进行对齐和检测，但性能受限于手工特征。Zhang等人$^{[20]}$使用多任务CNN来提升多view人脸检测，但检测召回率受限于弱人脸检测器产生的初始检测窗口。</p><p>传统Hard sample mining使用离线形式，极大增加了手工操作。需要在线的才能自动适应当前训练状态。</p><p>本论文提出了一个新的框架，用多任务训练的层叠CNN集成了这两个任务。该CNN包含3个阶段。第一阶段通过浅CNN快速产生候选窗口。接着通过一个更复杂的CNN，拒绝大量无脸窗口。最终使用一个更强力的CNN来再次精炼结果，输出5个人脸关键点坐标。</p><h2 id="II-Approach"><a href="#II-Approach" class="headerlink" title="II. Approach"></a>II. Approach</h2><h3 id="A-Overall-Framework"><a href="#A-Overall-Framework" class="headerlink" title="A. Overall Framework"></a>A. Overall Framework</h3><p>我们方法流程见图1。<br><img src="/2018/09/03/MTCNN/./1535953283802.png" alt="图1"></p><p>首先缩放到不同尺度，构建图片金字塔。</p><p><strong>Stage 1：</strong>我们提出了一个FCN，叫P-Net（proposal net），来获取候选脸部窗口和它们的限位框回归向量。接着根据回归向量校正候选。最后使用非极大值抑制NMS来合并高度重合的候选。</p><p><strong>Stage 2：</strong>所有候选输入到另一个CNN，叫Refine CNN（R-Net），它拒绝大量错误候选，使用限位框回归进行校正，使用了NMS。</p><p><strong>Stage 3：</strong>类似阶段2，但本阶段更偏向有监督的人类区域识别。网络会输出5个人类关键点。</p><h3 id="B-CNN-Architecture"><a href="#B-CNN-Architecture" class="headerlink" title="B. CNN Architecture"></a>B. CNN Architecture</h3><p>现有CNN方式有以下方面的限制：（1）一些卷积层的卷积核缺少多样性，也许会影响其辨识力。（2）与多类物体检测和分类任务相比，人脸检测是一个挑战性的二分类任务，也许每层只需更少的卷结核。因此我们降低了核数量，将$5\times5$改为$3\times3$卷积核来降低计算量，增大深度以获得更好性能。我们因此用更少的运行时间获得了更高性能。（训练阶段结果见表1）。CNN架构见图2，我们使用了PReLU$^{[30]}$作为非线性激活。</p><p><img src="/2018/09/03/MTCNN/./1535955102668.png" alt="表1"></p><p><img src="/2018/09/03/MTCNN/./1535957304770.png" alt="图2"></p><h3 id="C-Training"><a href="#C-Training" class="headerlink" title="C. Training"></a>C. Training</h3><p>我们用了3个任务训练CNN检测器：人脸/无人脸分类，限位框回归和人脸关键点定位。</p><p><em>1) Face classification:</em> 训练目标表达为二分类问题，每个样本$x_i$我们使用交叉熵loss：</p><script type="math/tex;mode=display">L_i ^{det} = - (y_i^{det} \log (p_i) + (1 - y_i ^{det} ) (1 - \log (p_i))  )  \ \  \  \tag {1}</script><p>其中$p_i$是网络输出的样本$x_i$是一张脸的概率。$y_i^{det} \in \{ 0, 1\}$标记了gt标签</p><p><em>2) Bounding box regression:</em> 对于每个候选窗口，我们预测它和最近gt的偏移。训练目标表达为回归问题，我们为每个样本使用了欧几里得loss：</p><script type="math/tex;mode=display">L_i^{box} = \begin {Vmatrix} \hat y_i^{box} - y _i ^{box} \end {Vmatrix} \tag 2</script><p>其中$\hat y_i^{box} $是来自网络的回归目标，$ y _i ^{box} $是gt坐标。共四个坐标，包括左上，宽和高，故$ y _i ^{box} \in \mathbb R^4 $</p><p><em>3) Facial landmark localization:</em> 类似限位框，人脸关键点检测也化为回归问题，我们最小化欧几里得loss：</p><script type="math/tex;mode=display">L_i^{landmark} =  \begin {Vmatrix} \hat y_i^{landmark} - y _i ^{landmark} \end {Vmatrix} \tag 3</script><p>共5个关键点，包括左右眼，鼻子和左右嘴角。</p><p><em>4) Multi-source training:</em> 因为每个CNN有多个任务，因此训练过程会有多种训练图片，比如脸，没有脸和部分对齐的脸。有些loss函数（公式1-3）不会用到。例如对于背景样本，我们只会计算$L_i^{det}$，另两个loss设为0。这可以通过样本类型指示器实现。总体训练目标为：</p><script type="math/tex;mode=display">\min \sum_{i=1} ^N \sum _{j\in \{ det, box, landmark\}} \alpha _j \beta _i ^j L_i ^j \tag 4</script><p>其中N是训练样本数量，$\alpha_j$标识任务重要性。在P-Net和R-Net中，$\alpha _{det} = 1 , \alpha _{box} = 0.5 , \alpha _{landmark} = 0.5$，而在O-Net中为$\alpha _{det} = 1 , \alpha _{box} = 0.5 , \alpha _{landmark} = 1$。$\beta_i ^j \in \{ 0 ,1\}$是样本类型指示器。使用了SGD训练这些CNN。</p><p><em>5) Online Hard sample mining:</em> 与传统HEM在原始分类器训练完成后使用不同，我们进行在线HEM，自动使用训练过程。</p><p>特别的，在每个mini-batch，我们为前向传播计算的所有样本loss排序，选择前70%作为困难样本用于反向传播。抛弃了剩余简单样本。效果见章节III。</p><h2 id="III-Experiments"><a href="#III-Experiments" class="headerlink" title="III. Experiments"></a>III. Experiments</h2><p>本节我们评估了HEM的有效性，用我们的人脸检测和对齐方法在FDDB，WIDER FACE和Wild（AFLW）上与前沿方法进行了比较。</p><h3 id="A-Training-Data"><a href="#A-Training-Data" class="headerlink" title="A. Training Data"></a>A. Training Data</h3><p>因为我们同时进行人脸检测和对齐，在训练中我们使用了四中数据标注：(i) Negative: 与任意gt脸IoU都低于0.3的区域； (2) Positive: 与一个gt脸IoU大于0.65；(3) Part faces: 与一个gt脸$IoU \in [0.4, 0.65]；(4)关键点脸: 有5个关键点的脸。部分脸和负样本之间有模糊地带，不同数据集标注中它们都不同。正负样本用于分类任务，正样本和部分脸用于限位框回归，关键点脸用于关键点定位。总体训练数据组成为3:1:1:2(Negative/ positive/ part face/ landmark face)。每个网络训练集如下：</p><p>1) P-Net: 随机从WIDER FACE取几小块，获得正、负和部分脸样本，从CelebA随机取关键点脸</p><p>2) R-Net: 使用第一阶段的框架在WIDER FACE中检测脸，获得获得正、负和部分脸样本，从CelebA中检测关键点脸</p><p>3) O-Net: 类似R-Net，但我们使用框架前两步进行检测以获得样本。</p><h3 id="B-The-effectiveness-of-online-hard-sample-mining"><a href="#B-The-effectiveness-of-online-hard-sample-mining" class="headerlink" title="B. The effectiveness of online hard sample mining"></a>B. The effectiveness of online hard sample mining</h3><p>我们训练了两个P-Net（是否使用OHEM）并在FDDB上比较性能（图3.a ）。OHEM提升了1.5%的整体性能。</p><p><img src="/2018/09/03/MTCNN/./1535963902762.png" alt="图3"></p><h3 id="C-The-effectiveness-of-joint-detection-and-alignment"><a href="#C-The-effectiveness-of-joint-detection-and-alignment" class="headerlink" title="C. The effectiveness of joint detection and alignment"></a>C. The effectiveness of joint detection and alignment</h3><p>我们在FDDB上（用同样的P-Net和R-Net）评估两个O-Net（是否联合训练关键点回归）。我们也比较了两个O-Net的限位框回归性能（图3.b）。</p><h3 id="D-Evaluation-on-face-detection"><a href="#D-Evaluation-on-face-detection" class="headerlink" title="D. Evaluation on face detection"></a>D. Evaluation on face detection</h3><p>我们在FDDB上与前沿方法[1, 5, 6, 11, 18, 19, 26, 27, 28, 29]进行了比较。图4 a-d显示了我们领先一大截。<br><img src="/2018/09/03/MTCNN/./1535964541018.png" alt="图4"></p><h3 id="E-Evaluation-on-face-alignment"><a href="#E-Evaluation-on-face-alignment" class="headerlink" title="E. Evaluation on face alignment"></a>E. Evaluation on face alignment</h3><p>我们与以下人脸对齐方法比较了性能：RCPR[12], TSPM[7], Luxand face SDK[17], ESR[13], CDM[15], SDM[21], TCDCN[22]。平均误差通过预估关键点与gt距离计算，并用眼间距离interocular归一化。图5说明我们领先很多。也显示了我们的方法在嘴角定位不那么优秀，也许是因为我们训练集的表情变化小，这对嘴角位置影响很大。</p><p><img src="/2018/09/03/MTCNN/./1535964779720.png" alt="图5"></p><h3 id="F-Runtime-efficiency"><a href="#F-Runtime-efficiency" class="headerlink" title="F. Runtime efficiency"></a>F. Runtime efficiency</h3><p>见表2。我们代码是未优化的Matlab 代码。</p><p><img src="/2018/09/03/MTCNN/./1535964796705.png" alt="表2"></p><h2 id="IV-Conclusion"><a href="#IV-Conclusion" class="headerlink" title="IV. Conclusion"></a>IV. Conclusion</h2><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] B. Yang, J. Yan, Z. Lei, and S. Z. Li, “Aggregate channel features for multi-view face detection,” in IEEE International Joint Conference on Biometrics, 2014, pp. 1-8.<br>[2] P. Viola and M. J. Jones, “Robust real-time face detection. International journal of computer vision,” vol. 57, no. 2, pp. 137-154, 2004<br>[3] M. T. Pham, Y. Gao, V. D. D. Hoang, and T. J. Cham, “Fast polygonal integration and its application in extending haar-like features to improve object detection,” in IEEE Conference on Computer Vision and Pattern Recognition, 2010, pp. 942-949.<br>[4] Q. Zhu, M. C. Yeh, K. T. Cheng, and S. Avidan, “Fast human detection using a cascade of histograms of oriented gradients,” in IEEE Computer Conference on Computer Vision and Pattern Recognition, 2006, pp. 1491-1498.<br>[5] M. Mathias, R. Benenson, M. Pedersoli, and L. Van Gool, “Face detection without bells and whistles,” in European Conference on Computer Vision, 2014, pp. 720-735.<br>[6] J. Yan, Z. Lei, L. Wen, and S. Li, “The fastest deformable part model for object detection,” in IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 2497-2504.<br>[7] X. Zhu, and D. Ramanan, “Face detection, pose estimation, and landmark localization in the wild,” in IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 2879-2886.<br>[8] M. Köstinger, P. Wohlhart, P. M. Roth, and H. Bischof, “Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization,” in IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2011, pp. 2144-2151.<br>[9] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with deep convolutional neural networks,” in Advances in neural information processing systems, 2012, pp. 1097-1105.<br>[10] Y. Sun, Y. Chen, X. Wang, and X. Tang, “Deep learning face representation by joint identification-verification,” in Advances in Neural Information Processing Systems, 2014, pp. 1988-1996.<br>[11] S. Yang, P. Luo, C. C. Loy, and X. Tang, “From facial parts responses to face detection: A deep learning approach,” in IEEE International Conference on Computer Vision, 2015, pp. 3676-3684.<br>[12] X. P. Burgos-Artizzu, P. Perona, and P. Dollar, “Robust face landmark estimation under occlusion,” in IEEE International Conference on Computer Vision, 2013, pp. 1513-1520.<br>[13] X. Cao, Y. Wei, F. Wen, and J. Sun, “Face alignment by explicit shape regression,” International Journal of Computer Vision, vol 107, no. 2, pp. 177-190, 2012.<br>[14] T. F. Cootes, G. J. Edwards, and C. J. Taylor, “Active appearance models,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 23, no. 6, pp. 681-685, 2001.<br>[15] X. Yu, J. Huang, S. Zhang, W. Yan, and D. Metaxas, “Pose-free facial landmark fitting via optimized part mixtures and cascaded deformable shape model,” in IEEE International Conference on Computer Vision, 2013, pp. 1944-1951.<br>[16] J. Zhang, S. Shan, M. Kan, and X. Chen, “Coarse-to-fine auto-encoder networks (CFAN) for real-time face alignment,” in European Conference on Computer Vision, 2014, pp. 1-16.<br>[17] Luxand Incorporated: Luxand face SDK, <a href="http://www.luxand.com/" target="_blank" rel="noopener">http://www.luxand.com/</a><br>[18] D. Chen, S. Ren, Y. Wei, X. Cao, and J. Sun, “Joint cascade face detection and alignment,” in European Conference on Computer Vision, 2014, pp. 109-122.<br>[19] H. Li, Z. Lin, X. Shen, J. Brandt, and G. Hua, “A convolutional neural network cascade for face detection,” in IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 5325-5334.<br>[20] C. Zhang, and Z. Zhang, “Improving multiview face detection with multi-task deep convolutional neural networks,” IEEE Winter Conference on Applications of Computer Vision, 2014, pp. 1036-1041.<br>[21] X. Xiong, and F. Torre, “Supervised descent method and its applications to face alignment,” in IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 532-539.<br>[22] Z. Zhang, P. Luo, C. C. Loy, and X. Tang, “Facial landmark detection by deep multi-task learning,” in European Conference on Computer Vision, 2014, pp. 94-108.<br>[23] Z. Liu, P. Luo, X. Wang, and X. Tang, “Deep learning face attributes in the wild,” in IEEE International Conference on Computer Vision, 2015, pp. 3730-3738.<br>[24] S. Yang, P. Luo, C. C. Loy, and X. Tang, “WIDER FACE: A Face Detection Benchmark”. arXiv preprint arXiv:1511.06523.<br>[25] V. Jain, and E. G. Learned-Miller, “FDDB: A benchmark for face detection in unconstrained settings,” Technical Report UMCS-2010-009, University of Massachusetts, Amherst, 2010.<br>[26] B. Yang, J. Yan, Z. Lei, and S. Z. Li, “Convolutional channel features,” in IEEE International Conference on Computer Vision, 2015, pp. 82-90.<br>[27] R. Ranjan, V. M. Patel, and R. Chellappa, “A deep pyramid deformable part model for face detection,” in IEEE International Conference on Biometrics Theory, Applications and Systems, 2015, pp. 1-8.<br>[28] G. Ghiasi, and C. C. Fowlkes, “Occlusion Coherence: Detecting and Localizing Occluded Faces,” arXiv preprint arXiv:1506.08347.<br>[29] S. S. Farfade, M. J. Saberian, and L. J. Li, “Multi-view face detection using deep convolutional neural networks,” in ACM on International Conference on Multimedia Retrieval, 2015, pp. 643-650.<br>[30] K. He, X. Zhang, S. Ren, J. Sun, “Delving deep into rectifiers: Surpassing human-level performance on imagenet classification,” in IEEE International Conference on Computer Vision, 2015, pp. 1026-1034.</p></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.muyaan.com/2018/09/02/Cross-Entropy-Loss-——-交叉熵Loss/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="慕湮"><meta itemprop="description" content=""><meta itemprop="image" content="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="慕湮"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/09/02/Cross-Entropy-Loss-——-交叉熵Loss/" itemprop="url">Cross Entropy Loss —— 交叉熵Loss</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2018-09-02 17:46:26 / Modified: 17:57:42" itemprop="dateCreated datePublished" datetime="2018-09-02T17:46:26+08:00">2018-09-02</time> </span><span id="/2018/09/02/Cross-Entropy-Loss-——-交叉熵Loss/" class="leancloud_visitors" data-flag-title="Cross Entropy Loss —— 交叉熵Loss"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Views: </span><span class="leancloud-visitors-count"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数: </span><span title="Symbols count in article">2.5k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="Reading time">4 mins.</span></div></div></header><div class="post-body" itemprop="articleBody"><p>交叉熵loss是常见的loss函数</p><hr><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p><a href="https://en.m.wikipedia.org/wiki/Information_theory" target="_blank" rel="noopener">信息论</a>中，cross entropy是两个<a href="https://en.m.wikipedia.org/wiki/Probability_distribution" target="_blank" rel="noopener">概率分布</a>p和q在同一事件集上度量辨认一个来自该集的事件所需平均位数，判断其编码是优化自反常概率分布q，还是真分布p。</p><p>交叉熵定义如下：</p><script type="math/tex;mode=display">H(p, q) = E_p [- \log q] = H(p) + D_{KL} (p || q)</script><p>其中$H(p)$是p的<a href="https://en.m.wikipedia.org/wiki/Information_entropy" target="_blank" rel="noopener">熵</a>，$D_{KL} (p || q)$是q自p的 <a href="https://en.m.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" target="_blank" rel="noopener">Kullback–Leibler divergence</a>（也被称为p对q的相对熵——注意这个逆转）</p><p>对于离散的p和q，这意味着：</p><script type="math/tex;mode=display">H(p,q) = - \sum_x p(x) \log q(x)</script><p>连续分布的情况是类似的。我们必须假定p和q对某些<a href="https://en.m.wikipedia.org/wiki/Measure_%28mathematics%29" target="_blank" rel="noopener">测量</a>r（通常r是一个<a href="https://en.m.wikipedia.org/wiki/Lebesgue_measure" target="_blank" rel="noopener">Lebesgue measure</a> on a <a href="https://en.m.wikipedia.org/wiki/Borel_set" target="_blank" rel="noopener">Borel</a> <a href="https://en.m.wikipedia.org/wiki/Sigma-algebra" target="_blank" rel="noopener">σ-algebra</a>）是<a href="https://en.m.wikipedia.org/wiki/Absolutely_continuous" target="_blank" rel="noopener">绝对连续</a>的。设P和Q是p和q对r的概率密度函数，那么</p><script type="math/tex;mode=display">-\int _X P(x) \log Q(x) dr(x) = E_p[-\log Q]</script><p>注意：符号$H(p,q)$还有另一个概念，p和q的<a href="https://en.m.wikipedia.org/wiki/Joint_entropy" target="_blank" rel="noopener">联合熵</a>。</p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>在信息论中，<a href="https://en.m.wikipedia.org/wiki/Kraft%E2%80%93McMillan_inequality" target="_blank" rel="noopener">Kraft-McMillan理论</a>确定了任何用于确定概率集X中一个值$x_i$的消息的直接可解码编码方案能被看做表达一个X的隐式概率分布$q(x_i) = 2^{-l_i}$，其中$l_i$是$x_i$编码位长度。因此交叉熵可以被译为当数据实际是P分布时，被错误假定为分布Q的每个已知数datum预期的消息长度。这也是为何预期是对概率分布P而不是Q。</p><script type="math/tex;mode=display">H(p,q) = E_p[l_i] = E_p \begin{bmatrix}  \end {bmatrix}</script><script type="math/tex;mode=display">H(p,q) = \sum _{x_i} p(x_i) \log \frac 1 {q(x_i)}</script><script type="math/tex;mode=display">H(p,q) = - \sum_x p(x) \log q(x)</script><h2 id="Estimation"><a href="#Estimation" class="headerlink" title="Estimation"></a>Estimation</h2><p>经常会有需要在分布p未知的情况下测量交叉熵。一个例子就是<a href="https://en.m.wikipedia.org/wiki/Language_model" target="_blank" rel="noopener">语言建模</a>，模型基于训练集T创建，接着在测试集上测量其交叉熵，以获得模型预测测试集的准确率。在该例中，p是任何语料库corpus的词的真分布，q是模型预测的词分布。因为真分布是未知的，交叉熵无法直接计算，故用如下方程估计交叉熵：</p><script type="math/tex;mode=display">H(T,q) = -\sum _{i=1}^N \frac 1 N \log _2 q(x_i)</script><p>其中N是测试集大小，q(x)是从训练集中估计的事件x的概率。这是对真交叉熵的蒙特卡洛Monte Carlo估计，把训练集当做来自p(x)的样本。</p><h2 id="Cross-entropy-minimization"><a href="#Cross-entropy-minimization" class="headerlink" title="Cross-entropy minimization"></a>Cross-entropy minimization</h2><p>交叉熵的最小化常用于优化和稀有概率估计，见<a href="https://en.m.wikipedia.org/wiki/Cross-entropy_method" target="_blank" rel="noopener">交叉熵方法</a>。</p><p>当把分布q与固定参照分布p相比时，交叉熵和<a href="https://en.m.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" target="_blank" rel="noopener">KL divergence</a>都由一个加法常量决定（因为p是固定的）：当p=q时都为其最小值，KL divergence是0，交叉熵是H(p)。在工程领域最小化KL divergence的原则（Kullback的“<a href="https://en.m.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Principle_of_minimum_discrimination_information" target="_blank" rel="noopener">Principle of Minimum Discrimination Information</a>”）通常称作Principle of Minimum Cross-Entropy(MCE)或Minxent。</p><p>但是如文章<a href="https://en.m.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" target="_blank" rel="noopener">Kullback-Leibler divergence</a>所讨论那样，有时q是固定的优先引用分布，p被优化未尽量接近q。在这一情形下两个最简化minimisation不相等。这导致了文献中的一些含糊，有的作者尝试通过重定义交叉熵为$D_{KL}(p||q)$而不是H(p,q)来解决这一不一致。</p><h2 id="Cross-Entropy-error-function-and-logistic-regression"><a href="#Cross-Entropy-error-function-and-logistic-regression" class="headerlink" title="Cross-Entropy error function and logistic regression"></a>Cross-Entropy error function and logistic regression</h2><p>交叉熵能用于机器学习和优化中定义一个loss函数。真概率$p_i$是真标签，给定分布$q_i$是当前模型的预测值。</p><p>更具体的，让我们考虑<a href="https://en.m.wikipedia.org/wiki/Logistic_regression" target="_blank" rel="noopener">逻辑回归</a>，是处理给定数据集将它们分类到标记为0和1的两个可能类的最基础形式。给定输入向量X，逻辑回归模型预测一个输出$y \in \{0,1\}$。建模使用的概率是<a href="https://en.m.wikipedia.org/wiki/Logistic_function" target="_blank" rel="noopener">逻辑函数</a>$g(z) = \frac 1 {1+e^{-z}}$。也就是输出y = 1的概率由如下给出</p><script type="math/tex;mode=display">q_{y=1} = \hat y \equiv  g( w \cdot x ) = 1 / (1 + e ^{-w \cdot x})</script><p>其中权重向量w通过一些合适的算法如梯度下降来优化。类似的，找到输出y = 0的概率为</p><script type="math/tex;mode=display">q_{y=0} = 1 - \hat y</script><p>真概率可以类似表达为$p_{y=1} = y$和$p_{y=0} = 1 -y$。</p><p>设定好符号后，$p \in \{y, 1-y\}$和$q\in\{ \hat y, 1- \hat y\}$，我们能用交叉熵来度量p和q之间的不同：</p><script type="math/tex;mode=display">H(p,q) = - \sum _i p_i \log q_i = -y \log \hat y - (1-y) \log (1-\hat y)</script><p>逻辑回归通常使用的典型loss函数是对样本中的所有交叉熵取平均值。假如我们有N个样本，它们序号为$n = 1, … , N$。loss函数就为：</p><script type="math/tex;mode=display">J(w) = \frac 1 N \sum _{n =1} ^N H(p_n,q_n) = \frac 1 N \sum _{n=1} ^N \begin {bmatrix} y_n \log \hat y_n  + (1-y_n) \log (1 - \hat y_n)\end {bmatrix}</script><p>其中$\hat y_n \equiv g(w \cdot x_n) = 1 / ( 1 + e ^{- w \cdot x_n})$，g(z)是之前提到的逻辑函数。</p><p>逻辑loss有时也被称为交叉熵loss，有时也称为log loss（这种情况二分类标签经常标记为(-1, +1)）。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://en.m.wikipedia.org/wiki/Cross_entropy" target="_blank" rel="noopener">Cross entropy - Wikipedia</a></p><p><a href="https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy" target="_blank" rel="noopener">Cross Entropy</a></p></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.muyaan.com/2018/08/29/Person-Re-identification-Past-Present-and-Future/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="慕湮"><meta itemprop="description" content=""><meta itemprop="image" content="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="慕湮"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/08/29/Person-Re-identification-Past-Present-and-Future/" itemprop="url">Person Re-identification: Past, Present and Future</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2018-08-29 20:36:11 / Modified: 20:40:22" itemprop="dateCreated datePublished" datetime="2018-08-29T20:36:11+08:00">2018-08-29</time> </span><span id="/2018/08/29/Person-Re-identification-Past-Present-and-Future/" class="leancloud_visitors" data-flag-title="Person Re-identification: Past, Present and Future"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Views: </span><span class="leancloud-visitors-count"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数: </span><span title="Symbols count in article">62k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="Reading time">1:52</span></div></div></header><div class="post-body" itemprop="articleBody"><p>一篇Person re-ID的调研型论文。由于对re-ID一无所知，就选择了它作为切入点。不得不说翻译本论文实在不是个好主意，20页真的有点多。前期也翻译了不少其它论文，翻译相比直接阅读有利有弊吧，不过以后会有选择性的翻译，注重总结了。</p><blockquote><h1 id="Person-Re-identification-Past-Present-and-Future"><a href="#Person-Re-identification-Past-Present-and-Future" class="headerlink" title="Person Re-identification: Past, Present and Future"></a><a href="https://arxiv.org/abs/1610.02984" target="_blank" rel="noopener">Person Re-identification: Past, Present and Future</a></h1><p>Liang Zheng, Yi Yang, Alexander G. Hauptmann, 2016.10</p><p>University of Technology at Sydney, Carnegie Mellon University</p></blockquote><h2><a href="#" class="headerlink"></a></h2><div class="post-button text-center"><a class="btn" href="/2018/08/29/Person-Re-identification-Past-Present-and-Future/#more" rel="contents">Read more &raquo;</a></div></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.muyaan.com/2018/08/21/Batch-Normalization/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="慕湮"><meta itemprop="description" content=""><meta itemprop="image" content="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="慕湮"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/08/21/Batch-Normalization/" itemprop="url">Batch Normalization</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2018-08-21 15:32:47 / Modified: 18:32:27" itemprop="dateCreated datePublished" datetime="2018-08-21T15:32:47+08:00">2018-08-21</time> </span><span id="/2018/08/21/Batch-Normalization/" class="leancloud_visitors" data-flag-title="Batch Normalization"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Views: </span><span class="leancloud-visitors-count"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数: </span><span title="Symbols count in article">17k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="Reading time">30 mins.</span></div></div></header><div class="post-body" itemprop="articleBody"><p>赫赫有名的BN论文，相较各个网络的论文要硬核得多，尚未理解的部分也较多。深度学习开始入门到现在已有50天，工作时划水外加业余啃完了13篇论文，涵盖了经典的分类和检测大部分论文，目前应该只剩余GoogLeNet的3个变种，MobileNet，还有RNN那一大堆没看了。公司业务一大部分在于行人检测与重识别，后续会开始看person re-identification相关的论文，剩余的几个网络啦，ReLU啦，后面合适的时候再说吧~</p><blockquote><p><a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a><br>Sergey Ioffe, Christian Szegedy 2015.02</p><p>Google Inc.</p></blockquote><h2><a href="#" class="headerlink"></a></h2><div class="post-button text-center"><a class="btn" href="/2018/08/21/Batch-Normalization/#more" rel="contents">Read more &raquo;</a></div></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.muyaan.com/2018/08/16/GoogLeNet-v1/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="慕湮"><meta itemprop="description" content=""><meta itemprop="image" content="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="慕湮"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/08/16/GoogLeNet-v1/" itemprop="url">GoogLeNet v1</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2018-08-16 17:31:18 / Modified: 19:28:25" itemprop="dateCreated datePublished" datetime="2018-08-16T17:31:18+08:00">2018-08-16</time> </span><span id="/2018/08/16/GoogLeNet-v1/" class="leancloud_visitors" data-flag-title="GoogLeNet v1"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Views: </span><span class="leancloud-visitors-count"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数: </span><span title="Symbols count in article">12k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="Reading time">23 mins.</span></div></div></header><div class="post-body" itemprop="articleBody"><p>GoogLeNet v1论文翻译。一个经典图片分类卷积网络架构。主要思想是通过现有密集架构，组合出一个稀疏空间架构，也就是Inception模块。</p><blockquote><h1 id="GoogleNet-Going-Deeper-with-Convolutions"><a href="#GoogleNet-Going-Deeper-with-Convolutions" class="headerlink" title="GoogleNet: Going Deeper with Convolutions"></a><a href="https://arxiv.org/abs/1409.4842" target="_blank" rel="noopener">GoogleNet: Going Deeper with Convolutions</a></h1><p>Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, 2014.11</p><p>Google Inc. University of North Carolina. Chapel Hill. University of Michigan. Ann Arbor Magic Leap Inc.</p></blockquote><h2><a href="#" class="headerlink"></a></h2><div class="post-button text-center"><a class="btn" href="/2018/08/16/GoogLeNet-v1/#more" rel="contents">Read more &raquo;</a></div></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.muyaan.com/2018/08/13/YOLOv3/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="慕湮"><meta itemprop="description" content=""><meta itemprop="image" content="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="慕湮"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/08/13/YOLOv3/" itemprop="url">YOLOv3</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2018-08-13 19:38:32 / Modified: 19:47:03" itemprop="dateCreated datePublished" datetime="2018-08-13T19:38:32+08:00">2018-08-13</time> </span><span id="/2018/08/13/YOLOv3/" class="leancloud_visitors" data-flag-title="YOLOv3"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Views: </span><span class="leancloud-visitors-count"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数: </span><span title="Symbols count in article">7.6k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="Reading time">14 mins.</span></div></div></header><div class="post-body" itemprop="articleBody"><p>YOLOv3论文翻译，一个YOLOv2的升级版。作者还表达了对计算机视觉技术应用到不好的地方的担忧，并吐槽了论文评审和mAP评价方法。感觉是个超酷的人~</p><blockquote><p><a href="https://arxiv.org/abs/1804.02767" target="_blank" rel="noopener">YOLOv3: An Incremental Improvement</a><br>Joseph Redmon, Ali Farhadi, 2018.4<br>University of Washington</p></blockquote><h2><a href="#" class="headerlink"></a></h2><div class="post-button text-center"><a class="btn" href="/2018/08/13/YOLOv3/#more" rel="contents">Read more &raquo;</a></div></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.muyaan.com/2018/08/12/YOLOv2-and-YOLO9000/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="慕湮"><meta itemprop="description" content=""><meta itemprop="image" content="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="慕湮"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/08/12/YOLOv2-and-YOLO9000/" itemprop="url">YOLOv2 and YOLO9000</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2018-08-12 21:08:19" itemprop="dateCreated datePublished" datetime="2018-08-12T21:08:19+08:00">2018-08-12</time> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">Edited on</span> <time title="Modified: 2018-08-13 19:47:10" itemprop="dateModified" datetime="2018-08-13T19:47:10+08:00">2018-08-13</time> </span><span id="/2018/08/12/YOLOv2-and-YOLO9000/" class="leancloud_visitors" data-flag-title="YOLOv2 and YOLO9000"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Views: </span><span class="leancloud-visitors-count"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数: </span><span title="Symbols count in article">13k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="Reading time">23 mins.</span></div></div></header><div class="post-body" itemprop="articleBody"><p>YOLOv2论文翻译。它提出了一个升级版的YOLO，以及利用分类数据集提升检测模型检测类别范围的方法，和一个使用该方法训练出的模型：YOLO9000。</p><blockquote><p><a href="https://arxiv.org/abs/1612.08242" target="_blank" rel="noopener">YOLO9000: Better, Faster, Stronger</a><br>Joseph Redmon, Ali Farhadi, 2016<br>University of Washington, Allen Institute for AI</p></blockquote><h2><a href="#" class="headerlink"></a></h2><div class="post-button text-center"><a class="btn" href="/2018/08/12/YOLOv2-and-YOLO9000/#more" rel="contents">Read more &raquo;</a></div></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.muyaan.com/2018/08/09/VGGNet/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="慕湮"><meta itemprop="description" content=""><meta itemprop="image" content="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="慕湮"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/08/09/VGGNet/" itemprop="url">VGGNet</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2018-08-09 15:33:48 / Modified: 15:47:01" itemprop="dateCreated datePublished" datetime="2018-08-09T15:33:48+08:00">2018-08-09</time> </span><span id="/2018/08/09/VGGNet/" class="leancloud_visitors" data-flag-title="VGGNet"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Views: </span><span class="leancloud-visitors-count"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数: </span><span title="Symbols count in article">11k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="Reading time">20 mins.</span></div></div></header><div class="post-body" itemprop="articleBody"><p>VGGNet论文翻译，小卷积核、更深网络的引领者</p><blockquote><p><a href="https://arxiv.org/abs/1409.1556" target="_blank" rel="noopener">VGGNet: Very Deep Convolutional Networks for Large-Scale Image Recognition</a><br>Karen Simonyan, Andrew Zisserman, 2014<br>Visual Geometry Group, Department of Engineering Science, University of Oxford</p></blockquote><div class="post-button text-center"><a class="btn" href="/2018/08/09/VGGNet/#more" rel="contents">Read more &raquo;</a></div></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.muyaan.com/2018/08/07/SSD/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="慕湮"><meta itemprop="description" content=""><meta itemprop="image" content="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="慕湮"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/08/07/SSD/" itemprop="url">SSD</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2018-08-07 10:11:28 / Modified: 14:31:16" itemprop="dateCreated datePublished" datetime="2018-08-07T10:11:28+08:00">2018-08-07</time> </span><span id="/2018/08/07/SSD/" class="leancloud_visitors" data-flag-title="SSD"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Views: </span><span class="leancloud-visitors-count"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数: </span><span title="Symbols count in article">13k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="Reading time">23 mins.</span></div></div></header><div class="post-body" itemprop="articleBody"><p>SSD论文翻译</p><blockquote><p><a href="https://arxiv.org/abs/1512.02325v2" target="_blank" rel="noopener">SSD: Single Shot MultiBox Detector</a><br>Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, ChengYang Fu, Alexander C. Berg 2015<br>UNC Chapel Hill, Zoox Inc. Google Inc. University of Michigan, Ann-Arbor</p></blockquote><div class="post-button text-center"><a class="btn" href="/2018/08/07/SSD/#more" rel="contents">Read more &raquo;</a></div></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.muyaan.com/2018/08/06/RetinaNet/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="慕湮"><meta itemprop="description" content=""><meta itemprop="image" content="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="慕湮"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/08/06/RetinaNet/" itemprop="url">RetinaNet</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2018-08-06 09:54:08 / Modified: 10:37:03" itemprop="dateCreated datePublished" datetime="2018-08-06T09:54:08+08:00">2018-08-06</time> </span><span id="/2018/08/06/RetinaNet/" class="leancloud_visitors" data-flag-title="RetinaNet"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Views: </span><span class="leancloud-visitors-count"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数: </span><span title="Symbols count in article">19k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="Reading time">34 mins.</span></div></div></header><div class="post-body" itemprop="articleBody"><p>RetinaNet论文翻译，业界常用的单步物体检测模型。其创新点是提出了focal loss函数，用于解决前景背景样本数量失衡的问题。</p><blockquote><h1 id="Retina-Net-Focal-Loss-for-Dense-Object-Detection"><a href="#Retina-Net-Focal-Loss-for-Dense-Object-Detection" class="headerlink" title="Retina Net : Focal Loss for Dense Object Detection"></a><a href="https://arxiv.org/abs/1708.02002" target="_blank" rel="noopener">Retina Net : Focal Loss for Dense Object Detection</a></h1><p>Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollar 2017<br>Facebook AI Research (FAIR)</p></blockquote><div class="post-button text-center"><a class="btn" href="/2018/08/06/RetinaNet/#more" rel="contents">Read more &raquo;</a></div></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article></section><nav class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a></nav></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview-wrap sidebar-panel sidebar-panel-active"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="https://avatars0.githubusercontent.com/u/3022000?s=460&v=4" alt="慕湮"><p class="site-author-name" itemprop="name">慕湮</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">17</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">26</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/tycallen" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a></span><br><span class="links-of-author-item"><a href="mailto:tyc.allen@gmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span><br><span class="links-of-author-item"><a href="http://weibo.com/pojunallen" target="_blank" title="微博"><i class="fa fa-fw fa-weibo"></i>微博</a></span><br></div><div class="links-of-blogroll motion-element links-of-blogroll-block"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> 友情链接</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="https://dotrabbit.github.io/" title="dotrabbit" target="_blank">dotrabbit</a></li></ul></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love" id="animate"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">慕湮</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="Symbols count total">267k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="Reading time total">8:05</span></div><div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div><span class="post-meta-divider">|</span><div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://theme-next.org">NexT.Muse</a></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=6.3.0"></script><script type="text/javascript" src="/js/src/motion.js?v=6.3.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=6.3.0"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script><script>AV.initialize("UWGXBMGbguCDhSCbC3NrUQtL-gzGzoHsz","ke1jrA5b6VyR89Kqqqwf2kPP")</script><script>function showTime(e){var t=new AV.Query(e),c=[],u=$(".leancloud_visitors");u.each(function(){c.push($(this).attr("id").trim())}),t.containedIn("url",c),t.find().done(function(e){var t=".leancloud-visitors-count";if(0!==e.length){for(var n=0;n<e.length;n++){var o=e[n],i=o.get("url"),s=o.get("time"),r=document.getElementById(i);$(r).find(t).text(s)}for(n=0;n<c.length;n++){i=c[n],r=document.getElementById(i);var l=$(r).find(t);""==l.text()&&l.text(0)}}else u.find(t).text(0)}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(i){var e=$(".leancloud_visitors"),s=e.attr("id").trim(),r=e.attr("data-flag-title").trim(),t=new AV.Query(i);t.equalTo("url",s),t.find({success:function(e){if(0<e.length){var t=e[0];t.fetchWhenSave(!0),t.increment("time"),$(document.getElementById(s)).find(".leancloud-visitors-count").text(t.get("time")),t.save(null,{success:function(e){},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var n=new i,o=new AV.ACL;o.setPublicReadAccess(!0),o.setPublicWriteAccess(!0),n.setACL(o),n.set("title",r),n.set("url",s),n.set("time",1),n.save(null,{success:function(e){$(document.getElementById(s)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):1<$(".post-title-link").length&&showTime(e)})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>